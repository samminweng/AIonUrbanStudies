DocId,Cited by,Year,Document Type,Title,Abstract,Author Keywords,Authors,DOI
8,,2022,Article,Automatic Target Detection from Satellite Imagery Using Machine Learning,"Object detection is a vital step in satellite imagery-based computer vision applications such as precision agriculture, urban planning and defense applications. In satellite imagery, object detection is a very complicated task due to various reasons including low pixel resolution of objects and detection of small objects in the large scale (a single satellite image taken by Digital Globe com-prises over 240 million pixels) satellite images. Object detection in satellite images has many challenges such as class variations, multiple objects pose, high variance in object size, illumination and a dense background. This study aims to compare the performance of existing deep learning algorithms for object detection in satellite imagery. We created the dataset of satellite imagery to perform object detection using convolutional neural network-based frameworks such as faster RCNN (faster region-based convolutional neural network), YOLO (you only look once), SSD (single-shot detector) and SIMRDWN (satellite imagery multiscale rapid detection with windowed networks). In addition to that, we also performed an analysis of these approaches in terms of accuracy and speed using the developed dataset of satellite imagery. The results showed that SIMRDWN has an accuracy of 97% on high-resolution images, while Faster RCNN has an accuracy of 95.31% on the standard resolution (1000 × 600). YOLOv3 has an accuracy of 94.20% on standard resolution (416 416) while on the other hand SSD has an accuracy of 84.61% on standard resolution (300 × 300). When it comes to speed and efficiency, YOLO is the obvious leader. In real-time surveillance, SIMRDWN fails. When YOLO takes 170 to 190 milliseconds to perform a task, SIMRDWN takes 5 to 103 milliseconds. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",Deep learning; Faster RCNN; Satellite images; SIMRDWN; SSD; YOLO,"Tahir A., Munawar H.S., Akram J., Adil M., Ali S., Kouzani A.Z., Pervez Mahmud M.A.",10.3390/s22031147
74,,2021,Article,Cloud detection using an ensemble of pixel-based machine learning models incorporating unsupervised classification,"Remote sensing imagery, such as that provided by the United States Geological Survey (USGS) Landsat satellites, has been widely used to study environmental protection, hazard analysis, and urban planning for decades. Clouds are a constant challenge for such imagery and, if not handled correctly, can cause a variety of issues for a wide range of remote sensing analyses. Typically, cloud mask algorithms use the entire image; in this study we present an ensemble of different pixel-based approaches to cloud pixel modeling. Based on four training subsets with a selection of different input features, 12 machine learning models were created. We evaluated these models using the cropped LC8-Biome cloud validation dataset. As a comparison, Fmask was also applied to the cropped scene Biome dataset. One goal of this research is to explore a machine learning modeling approach that uses as small a training data sample as possible but still provides an accurate model. Overall, the model trained on the sample subset (1.3% of the total training samples) that includes unsupervised Self-Organizing Map classification results as an input feature has the best performance. The approach achieves 98.57% overall accuracy, 1.18% cloud omission error, and 0.93% cloud commission error on the 88 cropped test images. By comparison to Fmask 4.0, this model improves the accuracy by 10.12% and reduces the cloud omission error by 6.39%. Furthermore, using an additional eight independent validation images that were not sampled in model training, the model trained on the second largest subset with an additional five features has the highest overall accuracy at 86.35%, with 12.48% cloud omission error and 7.96% cloud commission error. This model’s overall correctness increased by 3.26%, and the cloud omission error decreased by 1.28% compared to Fmask 4.0. The machine learning cloud classification models discussed in this paper could achieve very good performance utilizing only a small portion of the total training pixels available. We showed that a pixel-based cloud classification model, and that as each scene obviously has unique spectral characteristics, and having a small portion of example pixels from each of the sub-regions in a scene can improve the model accuracy significantly. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Cloud detection; Ensemble approaches; HOT; Landsat 8; Machine learning; NDSI; NDVI; Self organizing maps (SOM); Whitness,"Yu X., Lary D.J.",10.3390/rs13163289
92,,2021,Conference Paper,"Assessment of combining convolutional neural networks and object based image analysis to land cover classification using sentinel 2 satellite imagery (Tenes Region, Algeria)","Land cover maps can provide valuable information for various applications, such as territorial monitoring, environmental protection, urban planning and climate change prevention. In this purpose, remote sensing based on image classification approaches undergoing a high revolution can be dedicated to land cover mapping tasks. Similarly, deep learning models are considerably applied in remote sensing applications; which can automatically learn features from large amounts of data. Prevalently, the Convolutional Neural Network (CNN), have been increasingly performed in image classification. The aim of this study is to apply a new approach to analyse land cover, and extract its features. Experiments carried out on a coastal town located in north-western Algeria (Ténès region). The study area is chosen because of its importance as a part of the national strategy to combat natural hazards, specifically floods. As well as, a simple CNN model with two hidden layers was constructed, combined with an Object-Based Image Analysis (OBIA). In this regard, a Sentinel-2 image was used, to perform the classification, using spectral index combinations. Furthermore, to compare the performance of the proposed approach, an OBIA based on machines learning algorithms mainly Random Forest (RF) and Support Vector Machine (SVM), was provided. Results of accuracy assessment of classification showed good values in terms of Overall accuracy and Kappa Index, which reach to 93.1% and 0.91, respectively. As a comparison, CNN-OBIA approach outperformed OBIA based on RF algorithm. Therefore, Final land cover maps can be used as a support tool in regional and national decisions. © 2021 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives. All rights reserved.",Convolutional neural networks (CNN); Land cover; Machine learning; Object based image analysis (OBIA); Sentinel-2; Ténès,"Zaabar N., Niculescu S., Mihoubi M.K.",10.5194/isprs-archives-XLIII-B3-2021-383-2021
113,1.0,2021,Article,A refined method of high-resolution remote sensing change detection based on machine learning for newly constructed building areas,"Automatic detection of newly constructed building areas (NCBAs) plays an important role in addressing issues of ecological environment monitoring, urban management, and urban planning. Compared with low-and-middle resolution remote sensing images, high-resolution remote sensing images are superior in spatial resolution and display of refined spatial details. Yet its problems of spectral heterogeneity and complexity have impeded research of change detection for high-resolution remote sensing images. As generalized machine learning (including deep learning) technologies proceed, the efficiency and accuracy of recognition for ground-object in remote sensing have been substantially improved, providing a new solution for change detection of high-resolution remote sensing images. To this end, this study proposes a refined NCBAs detection method consisting of four parts based on generalized machine learning: (1) pre-processing; (2) candidate NCBAs are obtained by means of bi-temporal building masks acquired by deep learning semantic segmentation, and then registered one by one; (3) rules and support vector machine (SVM) are jointly adopted for classification of NCBAs with high, medium and low confidence; and (4) the final vectors of NCBAs are obtained by post-processing. In addition, area-based and pixel-based methods are adopted for accuracy assessment. Firstly, the proposed method is applied to three groups of GF1 images covering the urban fringe areas of Jinan, whose experimental results are divided into three categories: high, high-medium, and high-medium-low confidence. The results show that NCBAs of high confidence share the highest F1 score and the best overall effect. Therefore, only NCBAs of high confidence are considered to be the final detection result by this method. Specifically, in NCBAs detection for three groups GF1 images in Jinan, the mean Recall of area-based and pixel-based assessment methods reach around 77% and 91%, respectively, the mean Pixel Accuracy (PA) 88% and 92%, and the mean F1 82% and 91%, confirming the effectiveness of this method on GF1. Similarly, the proposed method is applied to two groups of ZY302 images in Xi’an and Kunming. The scores of F1 for two groups of ZY302 images are also above 90% respectively, confirming the effectiveness of this method on ZY302. It can be concluded that adoption of area registration improves registration efficiency, and the joint use of prior rules and SVM classifier with probability features could avoid over and missing detection for NCBAs. In practical applications, this method is contributive to automatic NCBAs detection from high-resolution remote sensing images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Areas registration; Change detection; Deep learning; High-resolution remote sensing; Newly constructed building areas; SVM,"Wang H., Qi J., Lei Y., Wu J., Li B., Jia Y.",10.3390/rs13081507
137,2.0,2021,Article,Use of deep learning models in street-level images to classify one-story unreinforced masonry buildings based on roof diaphragms,"In this paper, we explore the potential of convolutional neural networks to classify street-level imagery of one-story unreinforced masonry buildings (MURs) according to the flexibility of the roof diaphragm (rigid or flexible). This information is critical for vulnerability studies, disaster risk assessments, disaster management strategies, etc., and is of great relevance in cities where unreinforced masonry is the most common building typology or where the majority of the population resides in such buildings. Our contribution could be useful for local governments of cities in developing countries seeking to significantly reduce the number of deaths caused by disasters. Our research results indicate that VGG19 is the convolutional neural network architecture with the best performance, with an accuracy of 0.80, a precision of 0.88, and a recall of 0.84. The results are encouraging and could be used to reduce the amount of resources (both human and economic) for the development of detailed exposure models for unreinforced masonry buildings. © 2020 Elsevier Ltd",Convolutional neural networks; Deep learning; Diaphragm; Risk assessment; Seismic risk; Unreinforced masonry; Urban planning,"Rueda-Plata D., González D., Acevedo A.B., Duque J.C., Ramos-Pollán R.",10.1016/j.buildenv.2020.107517
158,,2021,Conference Paper,Development of a Methodology for Complex Monitoring of the Development of Urban and Suburban Areas Based on the Intellectual Analysis of Earth Remote Sensing Data and Geospatial Technologies,"Over the past half century, mankind is increasingly faced with the problems of rational use of the Earth’s territories and its resources without negative impact on the environment and the person himself. The organization of human life activity requires solving the issues of urban planning and the correct distribution of zones for the construction of industrial facilities, recreation, waste disposal zones, communications, routes, etc. Balanced planning is based on monitoring the current state of infrastructure and territory. This article proposes a methodology for integrated monitoring of the development of urban and suburban areas. It is proposed to use Earth remote sensing data as a basis for the study. The issues of collection, integration and intelligent processing of satellite images are considered. The definition and segmentation of objects in images to create digital maps is performed based on machine learning algorithms. © 2021, Springer Nature Switzerland AG.",Area monitoring; Intelligent analysis; Machine learning; Remote sensing data; Urban area,"Malikov V., Sadovnikova N., Parygin D., Aleshkevich A., Savina O.",10.1007/978-3-030-87034-8_29
283,,2020,Conference Paper,"Remote sensing and GIS approach for environmental green areas planning using Landsat satellite imagery, Dubai-UAE","Over the last decade, Dubai emirate witnessed a vast, rapidly growing population, that doubled since 2008. Nowadays, Dubai considers as the most populated emirate within the United Arab Emirates (UAE). With such an increasing population and new urban developments, sustainable urban planning procedures play an essential role in Dubai's environmental quality such as air quality, and pollution. Therefore, this study will utilize the Remote Sensing and Geographic Information system (GIS) to investigate Dubai's environmental quality by addressing and locating green areas and pollution percentages within each district. The study methodology is divided into three steps. First, Landsat Satellite medium spatial resolution and multi-spectral imagery will be used as an input for segmentation and object-based analysis. Considering the spectral and spatial signatures for green areas machine learning techniques will be adopted to select the most significant features to classify and extract green areas. Second, using environmental relational indices, green areas percentages will be quantitatively compared to Sentinel air quality data, such as NO2 and SO2, as well as the population density maps. Finally, GIS techniques will be used to create Dubai Environmental Critical Map (DECM), to locate districts with limited green areas and high pollution to improve environmental standards. The study results can be used as a measure for the municipality policymakers to ensure sustainable urban development for a healthy living. © 2020 SPIE",Dubai Environmental Vulnerability Map (DEVM); Geographic Information system (GIS); Remote Sensing,"Aldogom D., Mansoori S.A., AlMaazmi A., Nazzal T.",10.1117/12.2573904
285,70.0,2020,Article,Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review,"Remote sensing (RS) systems have been collecting massive volumes of datasets for decades, managing and analyzing of which are not practical using common software packages and desktop computing resources. In this regard, Google has developed a cloud computing platform, called Google Earth Engine (GEE), to effectively address the challenges of big data analysis. In particular, this platform facilitates processing big geo data over large areas and monitoring the environment for long periods of time. Although this platform was launched in 2010 and has proved its high potential for different applications, it has not been fully investigated and utilized for RS applications until recent years. Therefore, this study aims to comprehensively explore different aspects of the GEE platform, including its datasets, functions, advantages/limitations, and various applications. For this purpose, 450 journal articles published in 150 journals between January 2010 and May 2020 were studied. It was observed that Landsat and Sentinel datasets were extensively utilized by GEE users. Moreover, supervised machine learning algorithms, such as Random Forest, were more widely applied to image classification tasks. GEE has also been employed in a broad range of applications, such as Land Cover/land Use classification, hydrology, urban planning, natural disaster, climate analyses, and image processing. It was generally observed that the number of GEE publications have significantly increased during the past few years, and it is expected that GEE will be utilized by more users from different fields to resolve their big data processing challenges. © 2008-2012 IEEE.",Big data; cloud computing; Google Earth Engine (GEE); remote sensing (RS),"Amani M., Ghorbanian A., Ahmadi S.A., Kakooei M., Moghimi A., Mirmazloumi S.M., Moghaddam S.H.A., Mahdavi S., Ghahremanloo M., Parsian S., Wu Q., Brisco B.",10.1109/JSTARS.2020.3021052
289,4.0,2020,Article,A Self-Supervised Learning Framework for Road Centerline Extraction from High-Resolution Remote Sensing Images,"Road extraction from the high-resolution remote sensing image is significant for the land planning, vehicle navigation, etc. The existing road extraction methods normally need many preprocessing and subsequent optimization steps. Therefore, an automatic road centerline extraction method based on the self-supervised learning framework for high-resolution remote sensing image is proposed. This proposed method does not need to manually select training samples and other optimization steps, such as the nonroad area removing. First, the positive sample selection method combining the spectral and shape features is proposed to extract the road sample. Then, the one-class classifier framework is introduced and the random forest positive unlabeled learning classifier is constructed to get the posterior probability of the pixel belonging to road. The shape feature and the posterior probability are combined to form the final road network in the object-oriented way. Finally, the road centerline is obtained through the tensor voting algorithm. In order to verify the effectiveness of the proposed algorithm, high-resolution remote sensing images and benchmark datasets are used to do experiments. The indexes of the completeness ratio, the correctness ratio, and the detection quality are used for the quantitative accuracy evaluation. Compared with the supervised, the unsupervised, and the one-class classification road extraction algorithms, this proposed algorithm achieves high accuracy and efficiency. For the deep learning method comparison, the deep learning method performs well in most cases especially in the complex urban area. However, the deep learning method needs a large number of samples and a long training time, and our self-supervised learning framework does not need the training samples. © 2008-2012 IEEE.",High-resolution remote sensing image; one-class classifier; road centerline; road extraction; self-supervised learning,"Guo Q., Wang Z.",10.1109/JSTARS.2020.3014242
312,11.0,2019,Conference Paper,Mapping Urban Trees Within Cadastral Parcels Using an Object-Based Convolutional Neural Network,"Urban trees offer significant benefits for improving the sustainability and liveability of cities, but its monitoring is a major challenge for urban planners. Remote-sensing based technologies can effectively detect, monitor and quantify urban tree coverage as an alternative to field-based measurements. Automatic extraction of urban land cover features with high accuracy is a challenging task and it demands artificial intelligence workflows for efficiency and thematic quality. In this context, the objective of this research is to map urban tree coverage per cadastral parcel of Sandy Bay, Hobart from very high-resolution aerial orthophoto and LiDAR data using an Object Based Convolution Neural Network (CNN) approach. Instead of manual preparation of a large number of required training samples, automatically classified Object based image analysis (OBIA) output is used as an input samples to train CNN method. Also, CNN output is further refined and segmented using OBIA to assess the accuracy. The result shows 93.2% overall accuracy for refined CNN classification. Similarly, the overlay of improved CNN output with cadastral parcel layer shows that 21.5% of the study area is covered by trees. This research demonstrates that the accuracy of image classification can be improved by using a combination of OBIA and CNN methods. Such a combined method can be used where manual preparation of training samples for CNN is not preferred. Also, our results indicate that the technique can be implemented to calculate parcel level statistics for urban tree coverage that provides meaningful metrics to guide urban planning and land management practices. © 2019 Authors.",Cadastral Parcel; Convolutional Neural Network; GEOBIA; Machine Learning; Urban Trees,"Timilsina S., Sharma S.K., Aryal J.",10.5194/isprs-annals-IV-5-W2-111-2019
378,21.0,2019,Article,Multiscale road centerlines extraction from high-resolution aerial imagery,"Accurate road extraction from high-resolution aerial imagery has many applications such as urban planning and vehicle navigation system. The common road extraction methods are based on classification algorithm, which needs to design robust handcrafted features for road. However, designing such features is difficult. For the road centerlines extraction problem, the existing algorithms have some limitations, such as spurs, time consuming. To address the above issues to some extent, we introduce the feature learning based on deep learning to extract robust features automatically, and present a method to extract road centerlines based on multiscale Gabor filters and multiple directional non-maximum suppression. The proposed algorithm consists of the following four steps. Firstly, the aerial imagery is classified by a pixel-wise classifier based on convolutional neural network (CNN). Specifically, CNN is used to learn features from raw data automatically, especially the structural features. Then, edge-preserving filtering is conducted on the resulting classification map, with the original imagery serving as the guidance image. It is exploited to preserve the edges and the details of the road. After that, we do some post-processing based on shape features to extract more reliable roads. Finally, multiscale Gabor filters and multiple directional non-maximum suppression are integrated to get a complete and accurate road network. Experimental results show that the proposed method can achieve comparable or higher quantitative results, as well as more satisfactory visual performance. © 2018 Elsevier B.V.",Centerlines extraction; Convolutional neural network (CNN); Edge-preserving filtering; Multiscale Gabor filters,"Liu R., Miao Q., Song J., Quan Y., Li Y., Xu P., Dai J.",10.1016/j.neucom.2018.10.036
437,12.0,2018,Article,Ultra-Light aircraft-based hyperspectral and colour-infrared imaging to identify deciduous tree species in an urban environment,"One may consider the application of remote sensing as a trade-off between the imaging platforms, sensors, and data gathering and processing techniques. This study addresses the potential of hyperspectral imaging using ultra-light aircraft for vegetation species mapping in an urban environment, exploring both the engineering and scientific aspects related to imaging platform design and image classification methods. An imaging system based on simultaneous use of Rikola frame format hyperspectral and Nikon D800E adopted colour infrared cameras installed onboard a Bekas X32 manned ultra-light aircraft is introduced. Two test imaging flight missions were conducted in July of 2015 and September of 2016 over a 4000 ha area in Kaunas City, Lithuania. Sixteen and 64 spectral bands in 2015 and 2016, respectively, in a spectral range of 500-900 nm were recorded with colour infrared images. Three research questions were explored assessing the identification of six deciduous tree species: (1) Pre-treatment of spectral features for classification, (2) testing five conventional machine learning classifiers, and (3) fusion of hyperspectral and colour infrared images. Classification performance was assessed by applying leave-one-out cross-validation at the individual crown level and using as a reference at least 100 field inventoried trees for each species. The best-performing classification algorithm-multilayer perceptron, using all spectral properties extracted from the hyperspectral images-resulted in a moderate classification accuracy. The overall classification accuracy was 63%, Cohen's Kappa was 0.54, and the species-specific classification accuracies were in the range of 51-72%. Hyperspectral images resulted in significantly better tree species classification ability than the colour infrared images and simultaneous use of spectral properties extracted from hyperspectral and colour infrared images improved slightly the accuracy over the 2015 image. Even though classifications using hyperspectral data cubes of 64 bands resulted in relatively larger accuracies than with 16 bands, classification error matrices were not statistically different. Alternative imaging platforms (like an unmanned aerial vehicle and a Cessna 172 aircraft) and settings of the flights were discussed using simulated imaging projects assuming the same study area and field of application. Ultra-light aircraft-based hyperspectral and colour-infrared imaging was considered to be a technically and economically sound solution for urban green space inventories to facilitate tree mapping, characterization, and monitoring. © 2018 by the authors.",Classification; Colour infrared; Hyperspectral; Ultra-light aircraft; Urban trees,"Mozgeris G., Juodkiene V., Jonikavičius D., Straigyte L., Gadal S., Ouerghemmi W.",10.3390/rs10101668
443,18.0,2018,Article,Road centerline extraction from very-high-resolution aerial image and LiDAR data based on road connectivity,"The road networks provide key information for a broad range of applications such as urban planning, urban management, and navigation. The fast-developing technology of remote sensing that acquires high-resolution observational data of the land surface offers opportunities for automatic extraction of road networks. However, the road networks extracted from remote sensing images are likely affected by shadows and trees, making the road map irregular and inaccurate. This research aims to improve the extraction of road centerlines using both very-high-resolution (VHR) aerial images and light detection and ranging (LiDAR) by accounting for road connectivity. The proposed method first applies the fractal net evolution approach (FNEA) to segment remote sensing images into image objects and then classifies image objects using the machine learning classifier, random forest. A post-processing approach based on the minimum area bounding rectangle (MABR) is proposed and a structure feature index is adopted to obtain the complete road networks. Finally, a multistep approach, that is, morphology thinning, Harris corner detection, and least square fitting (MHL) approach, is designed to accurately extract the road centerlines from the complex road networks. The proposed method is applied to three datasets, including the New York dataset obtained from the object identification dataset, the Vaihingen dataset obtained from the International Society for Photogrammetry and Remote Sensing (ISPRS) 2D semantic labelling benchmark and Guangzhou dataset. Compared with two state-of-the-art methods, the proposed method can obtain the highest completeness, correctness, and quality for the three datasets. The experiment results show that the proposed method is an efficient solution for extracting road centerlines in complex scenes from VHR aerial images and light detection and ranging (LiDAR) data. © 2018 by the authors.",LiDAR data; Object recognition; Road centerline; Road connectivity; Very-high-resolution image,"Zhang Z., Zhang X., Sun Y., Zhang P.",10.3390/rs10081284
444,7.0,2018,Article,Context-Based Filtering of Noisy Labels for Automatic Basemap Updating from UAV Data,"Unmanned aerial vehicles (UAVs) have the potential to obtain high-resolution aerial imagery at frequent intervals, making them a valuable tool for urban planners who require up-to-date basemaps. Supervised classification methods can be exploited to translate the UAV data into such basemaps. However, these methods require labeled training samples, the collection of which may be complex and time consuming. Existing spatial datasets can be exploited to provide the training labels, but these often contain errors due to differences in the date or resolution of the dataset from which these outdated labels were obtained. In this paper, we propose an approach for updating basemaps using global and local contextual cues to automatically remove unreliable samples from the training set, and thereby, improve the classification accuracy. Using UAV datasets over Kigali, Rwanda, and Dar es Salaam, Tanzania, we demonstrate how the amount of mislabeled training samples can be reduced by 44.1% and 35.5%, respectively, leading to a classification accuracy of 92.1% in Kigali and 91.3% in Dar es Salaam. To achieve the same accuracy in Dar es Salaam, between 50000 and 60000 manually labeled image segments would be needed. This demonstrates that the proposed approach of using outdated spatial data to provide labels and iteratively removing unreliable samples is a viable method for obtaining high classification accuracies while reducing the costly step of acquiring labeled training samples. © 2008-2012 IEEE.",Basemap updating; image classification; informal settlements; label noise; random forests; unmanned aerial vehicles (UAVs); urban planning,"Gevaert C.M., Persello C., Elberink S.O., Vosselman G., Sliuzas R.",10.1109/JSTARS.2017.2762905
452,1.0,2018,Conference Paper,Application of machine learning in urban greenery land cover extraction,"Urban greenery is a critical part of the modern city and the greenery coverage information is essential for land resource management, environmental monitoring and urban planning. It is a challenging work to extract the urban greenery information from remote sensing image as the trees and grassland are mixed with city built-ups. In this paper, we propose a new automatic pixel-based greenery extraction method using multispectral remote sensing images. The method includes three main steps. First, a small part of the images is manually interpreted to provide prior knowledge. Secondly, a five-layer neural network is trained and optimised with the manual extraction results, which are divided to serve as training samples, verification samples and testing samples. Lastly, the well-trained neural network will be applied to the unlabelled data to perform the greenery extraction. The GF-2 and GJ-1 high resolution multispectral remote sensing images were used to extract greenery coverage information in the built-up areas of city X. It shows a favourable performance in the 619 square kilometers areas. Also, when comparing with the traditional NDVI method, the proposed method gives a more accurate delineation of the greenery region. Due to the advantage of low computational load and high accuracy, it has a great potential for large area greenery auto extraction, which saves a lot of manpower and resources. © Authors 2018. CC BY 4.0 License.",Auto extraction; Greenery land cover; Machine learning; Multispectral image; Neural network,"Qiao X., Li L.L., Li D., Gan Y.L., Hou A.Y.",10.5194/isprs-archives-XLII-3-1409-2018
476,61.0,2018,Article,Exploring the optimal integration levels between SAR and optical data for better urban land cover mapping in the Pearl River Delta,"Integrating synthetic aperture radar (SAR) and optical data to improve urban land cover classification has been identified as a promising approach. However, which integration level is the most suitable remains unclear but important to many researchers and engineers. This study aimed to compare different integration levels for providing a scientific reference for a wide range of studies using optical and SAR data. SAR data from TerraSAR-X and ENVISAT ASAR in both WSM and IMP modes were used to be combined with optical data at pixel level, feature level and decision levels using four typical machine learning methods. The experimental results indicated that: 1) feature level that used both the original images and extracted features achieved a significant improvement of up to 10% compared to that using optical data alone; 2) different levels of fusion required different suitable methods depending on the data distribution and data resolution. For instance, support vector machine was the most stable at both the feature and decision levels, while random forest was suitable at the pixel level but not suitable at the decision level. 3) By examining the distribution of SAR features, some features (e.g., homogeneity) exhibited a close-to-normal distribution, explaining the improvement from the maximum likelihood method at the feature and decision levels. This indicated the benefits of using texture features from SAR data when being combined with optical data for land cover classification. Additionally, the research also shown that combining optical and SAR data does not guarantee improvement compared with using single data source for urban land cover classification, depending on the selection of appropriate fusion levels and fusion methods. © 2017 Elsevier B.V.",Fusion level; Fusion strategies; Optical and SAR fusion; Urban land cover,"Zhang H., Xu R.",10.1016/j.jag.2017.08.013
531,29.0,2016,Article,Land Classification Using Remotely Sensed Data: Going Multilabel,"Obtaining an up-to-date high-resolution description of land cover is a challenging task due to the high cost and labor-intensive process of human annotation through field studies. This work introduces a radically novel approach for achieving this goal by exploiting the proliferation of remote sensing satellite imagery, allowing for the up-to-date generation of global-scale land cover maps. We propose the application of multilabel classification, a powerful framework in machine learning, for inferring the complex relationships between the acquired satellite images and the spectral profiles of different types of surface materials. Introducing a drastically different approach compared to unsupervised spectral unmixing, we employ contemporary ground-collected data from the European Environment Agency to generate the label set and multispectral images from the MODIS sensor to generate the spectral features, under a supervised classification framework. To validate the merits of our approach, we present results using several state-of-the-art multilabel learning classifiers and evaluate their predictive performance with respect to the number of annotated training examples, as well as their capability to exploit examples from neighboring regions or different time instances. We also demonstrate the application of our method on hyperspectral data from the Hyperion sensor for the urban land cover estimation of New York City. Experimental results suggest that the proposed framework can achieve excellent prediction accuracy, even from a limited number of diverse training examples, surpassing state-of-the-art spectral unmixing methods. © 2016 IEEE.",CORINE; data processing; land cover; MODIS; pattern classification; remote sensing; satellite applications; time series; unmixing,"Karalas K., Tsagkatakis G., Zervakis M., Tsakalides P.",10.1109/TGRS.2016.2520203
