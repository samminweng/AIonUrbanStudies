DocId,Cited by,Year,Document Type,Title,Abstract,Author Keywords,Authors,DOI
8,,2022,Article,Automatic Target Detection from Satellite Imagery Using Machine Learning,"Object detection is a vital step in satellite imagery-based computer vision applications such as precision agriculture, urban planning and defense applications. In satellite imagery, object detection is a very complicated task due to various reasons including low pixel resolution of objects and detection of small objects in the large scale (a single satellite image taken by Digital Globe com-prises over 240 million pixels) satellite images. Object detection in satellite images has many challenges such as class variations, multiple objects pose, high variance in object size, illumination and a dense background. This study aims to compare the performance of existing deep learning algorithms for object detection in satellite imagery. We created the dataset of satellite imagery to perform object detection using convolutional neural network-based frameworks such as faster RCNN (faster region-based convolutional neural network), YOLO (you only look once), SSD (single-shot detector) and SIMRDWN (satellite imagery multiscale rapid detection with windowed networks). In addition to that, we also performed an analysis of these approaches in terms of accuracy and speed using the developed dataset of satellite imagery. The results showed that SIMRDWN has an accuracy of 97% on high-resolution images, while Faster RCNN has an accuracy of 95.31% on the standard resolution (1000 × 600). YOLOv3 has an accuracy of 94.20% on standard resolution (416 416) while on the other hand SSD has an accuracy of 84.61% on standard resolution (300 × 300). When it comes to speed and efficiency, YOLO is the obvious leader. In real-time surveillance, SIMRDWN fails. When YOLO takes 170 to 190 milliseconds to perform a task, SIMRDWN takes 5 to 103 milliseconds. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",Deep learning; Faster RCNN; Satellite images; SIMRDWN; SSD; YOLO,"Tahir A., Munawar H.S., Akram J., Adil M., Ali S., Kouzani A.Z., Pervez Mahmud M.A.",10.3390/s22031147
74,,2021,Article,Cloud detection using an ensemble of pixel-based machine learning models incorporating unsupervised classification,"Remote sensing imagery, such as that provided by the United States Geological Survey (USGS) Landsat satellites, has been widely used to study environmental protection, hazard analysis, and urban planning for decades. Clouds are a constant challenge for such imagery and, if not handled correctly, can cause a variety of issues for a wide range of remote sensing analyses. Typically, cloud mask algorithms use the entire image; in this study we present an ensemble of different pixel-based approaches to cloud pixel modeling. Based on four training subsets with a selection of different input features, 12 machine learning models were created. We evaluated these models using the cropped LC8-Biome cloud validation dataset. As a comparison, Fmask was also applied to the cropped scene Biome dataset. One goal of this research is to explore a machine learning modeling approach that uses as small a training data sample as possible but still provides an accurate model. Overall, the model trained on the sample subset (1.3% of the total training samples) that includes unsupervised Self-Organizing Map classification results as an input feature has the best performance. The approach achieves 98.57% overall accuracy, 1.18% cloud omission error, and 0.93% cloud commission error on the 88 cropped test images. By comparison to Fmask 4.0, this model improves the accuracy by 10.12% and reduces the cloud omission error by 6.39%. Furthermore, using an additional eight independent validation images that were not sampled in model training, the model trained on the second largest subset with an additional five features has the highest overall accuracy at 86.35%, with 12.48% cloud omission error and 7.96% cloud commission error. This model’s overall correctness increased by 3.26%, and the cloud omission error decreased by 1.28% compared to Fmask 4.0. The machine learning cloud classification models discussed in this paper could achieve very good performance utilizing only a small portion of the total training pixels available. We showed that a pixel-based cloud classification model, and that as each scene obviously has unique spectral characteristics, and having a small portion of example pixels from each of the sub-regions in a scene can improve the model accuracy significantly. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Cloud detection; Ensemble approaches; HOT; Landsat 8; Machine learning; NDSI; NDVI; Self organizing maps (SOM); Whitness,"Yu X., Lary D.J.",10.3390/rs13163289
87,,2021,Article,Comparing three machine learning techniques for building extraction from a digital surface model,"Automatic building extraction from high-resolution remotely sensed data is a major area of interest for an extensive range of fields (e.g., urban planning, environmental risk management) but challenging due to urban morphology complexity. Among the different methods proposed, the approaches based on supervised machine learning (ML) achieve the best results. This paper aims to investigate building footprint extraction using only high-resolution raster digital surface model (DSM) data by comparing the performance of three different popular supervised ML models on a benchmark dataset. The first two methods rely on a histogram of oriented gradients (HOG) feature descriptor and a classical ML (support vector machine (SVM)) or a shallow neural network (extreme learning machine (ELM)) classifier, and the third model is a fully convolutional network (FCN) based on deep learning with transfer learning. Used data were obtained from the International Society for Photogrammetry and Remote Sensing (ISPRS) and cover the urban areas of Vaihingen an der Enz, Potsdam, and Toronto. The results indicated that performances of models based on shallow ML (feature extraction and classifier training) are affected by the urban context investigated (F1 scores from 0.49 to 0.81), whereas the FCN-based model proved to be the most robust and best-performing method for building extraction from a high-resolution raster DSM (F1 scores from 0.80 to 0.86). © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Automated building extraction; Deep learning (DL); Digital surface model (DSM); Extreme learning machine (ELM); Fully convolutional network (FCN); Histogram of oriented gradients (HOG); Machine learning (ML); Support vector machine (SVM),"Notarangelo N.M., Mazzariello A., Albano R., Sole A.",10.3390/app11136072
89,1.0,2021,Article,Building Roof Superstructures Classification from Imbalanced and Low Density Airborne LiDAR Point Cloud,"Light Detection and Ranging (LiDAR), an active remote sensing technology, is becoming an essential tool for geoinformation extraction and urban planning. Airborne Laser Scanning (ALS) point clouds segmentation and accurate classification are challenging and crucial to produce different geo-information products like three-dimensional (3D) city designs. This paper introduces an effective data-driven approach to build roof superstructures classification for airborne LiDAR point clouds with very low density and imbalanced classes, covering an urban area. Notably, it focuses on building roof superstructures (especially dormers and chimneys) and mitigating nonplanar objects' problems. Also, the imbalanced class problem of LiDAR data, to the best of our knowledge, is not yet addressed in the literature; it is considered in this study. The major advantage of the proposed approach is using only raw data without assumptions on the distribution underlying data. The main methodological novelties of this work are summarized in the following key elements. (i) At first, an adapted connected component analysis for 3D points cloud is proposed. (ii) Twelve geometry-based features are extracted for each component. (iii) A Support Vector Machine (SVM)-driven procedure is applied to classify the 3D components. (iv) Furthermore, a new component size-based sampling (CSBS) method is proposed to treat the imbalanced data problem and has been compared with several existing resampling strategies. In this study, components are classified into five classes: shed and gable dormers, chimneys, ground, and others. The results of this investigation show the satisfying classification performance of the proposed approach. Results also showed that the proposed approach outperformed machine learning methods, including SVM, Random Forest, Decision Tree, and Adaboost. © 2001-2012 IEEE.",3D classification; imbalanced data; light detection and ranging (LiDAR); Low-density point cloud; roof superstructures,"Aissou B.E., Aissa A.B., Dairi A., Harrou F., Wichmann A., Kada M.",10.1109/JSEN.2021.3073535
92,,2021,Conference Paper,"Assessment of combining convolutional neural networks and object based image analysis to land cover classification using sentinel 2 satellite imagery (Tenes Region, Algeria)","Land cover maps can provide valuable information for various applications, such as territorial monitoring, environmental protection, urban planning and climate change prevention. In this purpose, remote sensing based on image classification approaches undergoing a high revolution can be dedicated to land cover mapping tasks. Similarly, deep learning models are considerably applied in remote sensing applications; which can automatically learn features from large amounts of data. Prevalently, the Convolutional Neural Network (CNN), have been increasingly performed in image classification. The aim of this study is to apply a new approach to analyse land cover, and extract its features. Experiments carried out on a coastal town located in north-western Algeria (Ténès region). The study area is chosen because of its importance as a part of the national strategy to combat natural hazards, specifically floods. As well as, a simple CNN model with two hidden layers was constructed, combined with an Object-Based Image Analysis (OBIA). In this regard, a Sentinel-2 image was used, to perform the classification, using spectral index combinations. Furthermore, to compare the performance of the proposed approach, an OBIA based on machines learning algorithms mainly Random Forest (RF) and Support Vector Machine (SVM), was provided. Results of accuracy assessment of classification showed good values in terms of Overall accuracy and Kappa Index, which reach to 93.1% and 0.91, respectively. As a comparison, CNN-OBIA approach outperformed OBIA based on RF algorithm. Therefore, Final land cover maps can be used as a support tool in regional and national decisions. © 2021 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives. All rights reserved.",Convolutional neural networks (CNN); Land cover; Machine learning; Object based image analysis (OBIA); Sentinel-2; Ténès,"Zaabar N., Niculescu S., Mihoubi M.K.",10.5194/isprs-archives-XLIII-B3-2021-383-2021
96,2.0,2021,Article,Assessment of deep learning techniques for land use land cover classification in southern new caledonia,"Land use (LU) and land cover (LC) are two complementary pieces of cartographic information used for urban planning and environmental monitoring. In the context of New Caledonia, a biodiversity hotspot, the availability of up-to-date LULC maps is essential to monitor the impact of extreme events such as cyclones and human activities on the environment. With the democratization of satellite data and the development of high-performance deep learning techniques, it is possible to create these data automatically. This work aims at determining the best current deep learning configuration (pixel-wise vs. semantic labelling architectures, data augmentation, image prepos-sessing, … ), to perform LULC mapping in a complex, subtropical environment. For this purpose, a specific data set based on SPOT6 satellite data was created and made available for the scientific community as an LULC benchmark in a tropical, complex environment using five representative areas of New Caledonia labelled by a human operator: four used as training sets, and the fifth as a test set. Several architectures were trained and the resulting classification was compared with a state-of-the-art machine learning technique: XGboost. We also assessed the relevance of popular neo-channels derived from the raw observations in the context of deep learning. The deep learning approach showed comparable results to XGboost for LC detection and over-performed it on the LU detection task (61.45% vs. 51.56% of overall accuracy). Finally, adding LC classification output of the dedicated deep learning architecture to the raw channels input significantly improved the overall accuracy of the deep learning LU classification task (63.61% of overall accuracy). All the data used in this study are available on line for the remote sensing community and for assessing other LULC detection techniques. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Deep learning: XGBoost; Land cover; Land use; Neo-channels; Neural network; New Caledonia; Remote sensing,"Rousset G., Despinoy M., Schindler K., Mangeas M.",10.3390/rs13122257
106,,2021,Article,"Machine learning approach to extract building footprint from high-resolution images: The case study of Makkah, Saudi Arabia","Extracting and identifying building boundaries from high-resolution images have been a hot topic in the field of remote sensing for years. Various methods including geometric, radiometric, object based and edge detection were previously deliberated and implemented in different studies in the context of building extraction. Nevertheless, the reliability of extraction process is mainly subject to user intervention. The current study proposes a new automatic morphology-based approach for extracting buildings using high-resolution satellite images of Al-Hudaybiyah region in the city of Makkah as a case study. The proposed technique integrates the support vector machine for extracting buildings that have bright and dark roofs. The appropriateness of this method has been examined by means of various indicators for example completeness, correctness and quality. Preliminary findings will illustrate the precision and accuracy of the used machine learning algorithm. Research results can provide a generic indicator to assist the planning authorities in achieving better urban planning processes taking into account all potential environmental, social and urban demands and requirements. © 2021 The Author(s) 2021. Published by Oxford University Press.",extract building footprint; high-resolution images; machine learning; Makkah,"Faisal K., Imam A., Majrashi A., Hegazy I.",10.1093/ijlct/ctaa099
113,1.0,2021,Article,A refined method of high-resolution remote sensing change detection based on machine learning for newly constructed building areas,"Automatic detection of newly constructed building areas (NCBAs) plays an important role in addressing issues of ecological environment monitoring, urban management, and urban planning. Compared with low-and-middle resolution remote sensing images, high-resolution remote sensing images are superior in spatial resolution and display of refined spatial details. Yet its problems of spectral heterogeneity and complexity have impeded research of change detection for high-resolution remote sensing images. As generalized machine learning (including deep learning) technologies proceed, the efficiency and accuracy of recognition for ground-object in remote sensing have been substantially improved, providing a new solution for change detection of high-resolution remote sensing images. To this end, this study proposes a refined NCBAs detection method consisting of four parts based on generalized machine learning: (1) pre-processing; (2) candidate NCBAs are obtained by means of bi-temporal building masks acquired by deep learning semantic segmentation, and then registered one by one; (3) rules and support vector machine (SVM) are jointly adopted for classification of NCBAs with high, medium and low confidence; and (4) the final vectors of NCBAs are obtained by post-processing. In addition, area-based and pixel-based methods are adopted for accuracy assessment. Firstly, the proposed method is applied to three groups of GF1 images covering the urban fringe areas of Jinan, whose experimental results are divided into three categories: high, high-medium, and high-medium-low confidence. The results show that NCBAs of high confidence share the highest F1 score and the best overall effect. Therefore, only NCBAs of high confidence are considered to be the final detection result by this method. Specifically, in NCBAs detection for three groups GF1 images in Jinan, the mean Recall of area-based and pixel-based assessment methods reach around 77% and 91%, respectively, the mean Pixel Accuracy (PA) 88% and 92%, and the mean F1 82% and 91%, confirming the effectiveness of this method on GF1. Similarly, the proposed method is applied to two groups of ZY302 images in Xi’an and Kunming. The scores of F1 for two groups of ZY302 images are also above 90% respectively, confirming the effectiveness of this method on ZY302. It can be concluded that adoption of area registration improves registration efficiency, and the joint use of prior rules and SVM classifier with probability features could avoid over and missing detection for NCBAs. In practical applications, this method is contributive to automatic NCBAs detection from high-resolution remote sensing images. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Areas registration; Change detection; Deep learning; High-resolution remote sensing; Newly constructed building areas; SVM,"Wang H., Qi J., Lei Y., Wu J., Li B., Jia Y.",10.3390/rs13081507
129,3.0,2021,Article,Fusion of airborne lidar point clouds and aerial images for heterogeneous land-use urban mapping,"The World Health Organization has reported that the number of worldwide urban residents is expected to reach 70% of the total world population by 2050. In the face of challenges brought about by the demographic transition, there is an urgent need to improve the accuracy of urban land-use mappings to more efficiently inform about urban planning processes. Decision-makers rely on accurate urban mappings to properly assess current plans and to develop new ones. This study investigates the effects of including conventional spectral signatures acquired by different sensors on the classification of airborne LiDAR (Light Detection and Ranging) point clouds using multiple feature spaces. The proposed method applied three machine learning algorithms—ML (Maximum Likelihood), SVM (Support Vector Machines), and MLP (Multilayer Perceptron Neural Network)—to classify LiDAR point clouds of a residential urban area after being geo-registered to aerial photos. The overall classification accuracy passed 97%, with height as the only geometric feature in the classifying space. Misclassifications occurred among different classes due to independent acquisition of aerial and LiDAR data as well as shadow and orthorectification problems from aerial images. Nevertheless, the outcomes are promising as they surpassed those achieved with large geometric feature spaces and are encouraging since the approach is computationally reasonable and integrates radiometric properties from affordable sensors. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",Bootstrap aggregation; K-fold cross-validation; LiDAR classification; LiDAR-aerial geo-registration; LiDAR-aerial integration; Maximum likelihood; Neural networks; Supervised machine learning; Support vector machines; Urban land-use,"Megahed Y., Shaker A., Yan W.Y.",10.3390/rs13040814
137,2.0,2021,Article,Use of deep learning models in street-level images to classify one-story unreinforced masonry buildings based on roof diaphragms,"In this paper, we explore the potential of convolutional neural networks to classify street-level imagery of one-story unreinforced masonry buildings (MURs) according to the flexibility of the roof diaphragm (rigid or flexible). This information is critical for vulnerability studies, disaster risk assessments, disaster management strategies, etc., and is of great relevance in cities where unreinforced masonry is the most common building typology or where the majority of the population resides in such buildings. Our contribution could be useful for local governments of cities in developing countries seeking to significantly reduce the number of deaths caused by disasters. Our research results indicate that VGG19 is the convolutional neural network architecture with the best performance, with an accuracy of 0.80, a precision of 0.88, and a recall of 0.84. The results are encouraging and could be used to reduce the amount of resources (both human and economic) for the development of detailed exposure models for unreinforced masonry buildings. © 2020 Elsevier Ltd",Convolutional neural networks; Deep learning; Diaphragm; Risk assessment; Seismic risk; Unreinforced masonry; Urban planning,"Rueda-Plata D., González D., Acevedo A.B., Duque J.C., Ramos-Pollán R.",10.1016/j.buildenv.2020.107517
144,,2021,Article,Transfer Learning Models for Land Cover and Land Use Classification in Remote Sensing Image,"Land Cover or Land Use (LCLU) classification is an important, challenging problem in remote sensing (RS) images. RS image classification is a recent technology used to extract hidden information from remotely sensed images in the observed earth environment. This classification is essential for sustainable development in agricultural decisions and urban planning using deep learning (DL) methods. DL gets more attention for accuracy and performance improvements in large datasets. This paper is aimed to apply one of the DL methods called transfer learning (TL). TL is the recent research problem in machine learning and DL approaches for image classification. DL consumes much time for training when starting from scratch. This problem could be overcome in the TL modeling technique, which uses pre-trained models to build deep TL models efficiently. We applied the TL model using bottleneck feature extraction from the pre-trained models: InceptionV3, Resnet50V2, and VGG19 to LCLU classification in the UC Merced dataset. With these experiments, the TL model has been built the outdate performance of 92.46, 94.38, and 99.64 in Resnet50V2, InceptionV3, and VGG19, respectively. © 2021 The Author(s). Published with license by Taylor & Francis Group, LLC.",,"Alem A., Kumar S.",10.1080/08839514.2021.2014192
153,,2021,Article,Assessment of approaches for the extraction of building footprints from pléiades images,"The Marina area represents an official new gateway of entry to Egypt and the development of infrastructure is proceeding rapidly in this region. The objective of this research is to obtain building data by means of automated extraction from Pléiades satellite images. This is due to the need for efficient mapping and updating of geodatabases for urban planning and touristic development. It compares the performance of random forest algorithm to other classifiers like maximum likelihood, support vector machines, and backpropagation neural networks over the well-organized buildings which appeared in the satellite images. Images were subsequently classified into two classes: buildings and non-buildings. In addition, basic morphological operations such as opening and closing were used to enhance the smoothness and connectedness of the classified imagery. The overall accuracy for random forest, maximum likelihood, support vector machines, and backpropagation were 97%, 95%, 93% and 92% respectively. It was found that random forest was the best option, followed by maximum likelihood, while the least effective was the backpropagation neural network. The completeness and correctness of the detected buildings were evaluated. Experiments confirmed that the four classification methods can effectively and accurately detect 100% of buildings from very high-resolution images. It is encouraged to use machine learning algorithms for object detection and extraction from very high-resolution images. © 2021 Author.",Backpropagation; Ensemble classifiers; Image classification; Machine learning; Maximum likelihood; Random forest; Support vector machines,"Taha L.G.E.-D., Ibrahim R.E.",10.7494/geom.2021.15.4.101
158,,2021,Conference Paper,Development of a Methodology for Complex Monitoring of the Development of Urban and Suburban Areas Based on the Intellectual Analysis of Earth Remote Sensing Data and Geospatial Technologies,"Over the past half century, mankind is increasingly faced with the problems of rational use of the Earth’s territories and its resources without negative impact on the environment and the person himself. The organization of human life activity requires solving the issues of urban planning and the correct distribution of zones for the construction of industrial facilities, recreation, waste disposal zones, communications, routes, etc. Balanced planning is based on monitoring the current state of infrastructure and territory. This article proposes a methodology for integrated monitoring of the development of urban and suburban areas. It is proposed to use Earth remote sensing data as a basis for the study. The issues of collection, integration and intelligent processing of satellite images are considered. The definition and segmentation of objects in images to create digital maps is performed based on machine learning algorithms. © 2021, Springer Nature Switzerland AG.",Area monitoring; Intelligent analysis; Machine learning; Remote sensing data; Urban area,"Malikov V., Sadovnikova N., Parygin D., Aleshkevich A., Savina O.",10.1007/978-3-030-87034-8_29
162,2.0,2021,Article,Urban tree species classification using UAV-based multi-sensor data fusion and machine learning,"Urban tree species classification is a challenging task due to spectral and spatial diversity within an urban environment. Unmanned aerial vehicle (UAV) platforms and small-sensor technology are rapidly evolving, presenting the opportunity for a comprehensive multi-sensor remote sensing approach for urban tree classification. The objectives of this paper were to develop a multi-sensor data fusion technique for urban tree species classification with limited training samples. To that end, UAV-based multispectral, hyperspectral, LiDAR, and thermal infrared imagery was collected over an urban study area to test the classification of 96 individual trees from seven species using a data fusion approach. Two supervised machine learning classifiers, Random Forest (RF) and Support Vector Machine (SVM), were investigated for their capacity to incorporate highly dimensional and diverse datasets from multiple sensors. When using hyperspectral-derived spectral features with RF, the fusion of all features extracted from all sensor types (spectral, LiDAR, thermal) achieved the highest overall classification accuracy (OA) of 83.3% and kappa of 0.80. Despite multispectral reflectance bands alone producing significantly lower OA of 55.2% compared to 70.2% with minimum noise fraction (MNF) transformed hyperspectral reflectance bands, the full dataset combination (spectral, LiDAR, thermal) with multispectral-derived spectral features achieved an OA of 81.3% and kappa of 0.77 using RF. Comparison of the features extracted from individual sensors for each species highlight the ability for each sensor to identify distinguishable characteristics between species to aid classification. The results demonstrate the potential for a high-resolution multi-sensor data fusion approach for classifying individual trees by species in a complex urban environment under limited sampling requirements. © 2021 Informa UK Limited, trading as Taylor & Francis Group.",Random Forest; Remote Sensing; Support Vector Machine; Tree Species Detection,"Hartling S., Sagan V., Maimaitijiang M.",10.1080/15481603.2021.1974275
186,3.0,2021,Article,Estimating Socio-Economic Parameters via Machine Learning Methods Using Luojia1-01 Nighttime Light Remotely Sensed Images at Multiple Scales of China in 2018,"Mapping socio-economic indicators with a raster format is still a great challenge. The nighttime light (NTL) datasets have been widely utilized to estimate the socio-economic parameters. However, the precision of the published datasets was too coarse to meet related issues such as flood losses assessment, urban planning, and epidemiological studies. The present study calibrated gross domestic product (GDP), population (POP), electric consumption (EC), and urban build-up area (B-A) at 100 m resolution for 45 cities of China in 2018 using Luojia1-01 NTL datasets via random forest (RF) as well as geographically weighted regression (GWR) model. The linear regression (LR), back propagation neural network (BPNN), and support vector machine (SVM) methods were selected for comparison with GWR and RF models. Besides, the Suomi National Polar-Orbiting Partnership-Visible Infrared Imaging Radiometer Suite (NPP-VIIRS) was chosen for comparison with Luojia1-01. The ten-folded cross-validation (CV) has been used for evaluating accuracy at county and city scales. Finally, the distribution maps of socio-economic parameters were illustrated and some findings were obtained. First, the validation results revealed that the calibration at the city-scale outperformed the county or district scale. Second, the precision of the Luojia1-01 NTL dataset surpassed the NPP-VIIRS NTL dataset on the same administrative scale except for some specific situations. Third, the precision of the simulation for the gross domestic product (GDP) is the highest than the others, followed by electric consumption (EC), build-up area (B-A), and population (POP). Fourth, the optimum model varied according to the socio-economic parameters. Fifth, the distribution of socio-economic parameters exhibited obvious spatial heterogeneity. This paper can supply scientific support for calibrating socio-economic parameters in other regions. © 2013 IEEE.",China; GWR; Luojia1-01; machine learning; multiple scales; NPP/VIIRS; socio-economic parameters,"Guo B., Bian Y., Zhang D., Su Y., Wang X., Zhang B., Wang Y., Chen Q., Wu Y., Luo P.",10.1109/ACCESS.2021.3059865
193,1.0,2021,Article,Synthesizing location semantics from street view images to improve urban land-use classification,"Land-use maps are instrumental to inform urban planning and environmental research. Street view images (SVIs) have shown great potential for automated land-use classification for land-use mapping. However, previous studies overlooked SVI-derived location contextual information that may help improve land-use classification. This study proposes a novel land-use classification method that synthesizes location semantics from SVIs to account for contextual information from SVIs, land parcels and roads around the SVIs. The proposed method first generates land-use scene images (LUSIs) by using an SVI-derived straightforward algorithm. The LUSIs are then relocated to land parcels by using a displacement strategy and classified into land-use types by using a deep learning network. This study determines the land-use types of land parcels with classified LUSIs. Two case studies, consisting of LUSIs for five land-use types, show that introducing location semantics of SVIs can remarkably improve the classification accuracy of land-use types. © 2020 Informa UK Limited, trading as Taylor & Francis Group.",land-use classification; Land-use map; location semantics; street view image,"Fang F., Yu Y., Li S., Zuo Z., Liu Y., Wan B., Luo Z.",10.1080/13658816.2020.1831515
212,2.0,2020,Article,Urban population distribution mapping with multisource geospatial data based on zonal strategy,"Mapping population distribution at fine resolutions with high accuracy is crucial to urban planning and management. This paper takes Guangzhou city as the study area, illustrates the gridded population distribution map by using machine learning methods based on zoning strategy with multisource geospatial data such as night light remote sensing data, point of interest data, land use data, and so on. The street-level accuracy evaluation results show that the proposed approach achieved good overall accuracy, with determinant coefficient (R2) being 0.713 and root mean square error (RMSE) being 5512.9. Meanwhile, the goodness of fit for single linear regression (LR) model and random forest (RF) regression model are 0.0039 and 0.605, respectively. For dense area, the accuracy of the random forest model is better than the linear regression model, while for sparse area, the accuracy of the linear regression model is better than the random forest model. The results indicated that the proposed method has great potential in fine-scale population mapping. Therefore, it is advised that the zonal modeling strategy should be the primary choice for solving regional differences in the population distribution mapping research. © 2020 by the authors. Licensee MDPI, Basel, Switzerland.",Guangzhou; Point of interest; Population mapping; Random forest; Zonal model,"Zhao G., Yang M.",10.3390/ijgi9110654
231,,2020,Conference Paper,A NOVEL SELF-TAUGHT LEARNING FRAMEWORK USING SPATIAL PYRAMID MATCHING for SCENE CLASSIFICATION,"Remote sensing earth observation images have a wide range of applications in areas like urban planning, agriculture, environment monitoring, etc. While the industrial world benefits from availability of high resolution earth observation images since recent years, interpreting such images has become more challenging than ever. Among many machine learning based methods that have worked out successfully in remote sensing scene classification, spatial pyramid matching using sparse coding (ScSPM) is a classical model that has achieved promising classification accuracy on many benchmark data sets. ScSPM is a three-stage algorithm, composed of dictionary learning, sparse representation and classification. It is generally believed that in the dictionary learning stage, although unsupervised, one should use the same data set as classification stage to get good results. However, recent studies in transfer learning suggest that it might be a better strategy to train the dictionary on a larger data set different from the one to classify. In our work, we propose an algorithm that combines ScSPM with self-taught learning, a transfer learning framework that trains a dictionary on an unlabeled data set and uses it for multiple classification tasks. In the experiments, we learn the dictionary on Caltech-101 data set, and classify two remote sensing scene image data sets: UC Merced LandUse data set and Changping data set. Experimental results show that the classification accuracy of proposed method is compatible to that of ScSPM. Our work thus provides a new way to reduce resource cost in learning a remote sensing scene image classifier. © 2020 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives.",High Resolution Imagery; Remote Sensing; Scene Classification; Self-taught Learning; Spatial Pyramid Matching,"Yang Y., Zhu D., Ren F., Cheng C.",10.5194/isprs-archives-XLIII-B2-2020-725-2020
236,12.0,2020,Article,A robust segmentation framework for closely packed buildings from airborne LiDAR point clouds,"Urban villages (UVs) are commonly found in many Asian cities. These villages contain many closely packed buildings constructed decades ago without proper urban planning. There is a need for those buildings to be identified and put into statistics. In this paper, we present a segmentation framework that invokes multiple machine learning techniques and point cloud/image processing algorithms to segment individual closely packed buildings from large urban scenes. The presented framework consists of two major segmentation processes. The framework first filters out the non-ground objects from the point cloud, then it classified them by using the Random Forest classifier to isolate buildings from the entire scene. After that, the building point clouds will be segmented based on several building attribute analysis methods. This is followed by using the Random Sample Consensus (RANSAC) plane filtering method to expand the space between two closely packed buildings, so that the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) clustering technique can be used to more accurately segment each individual building from the closely packed building areas. Two airborne Light Detection and Ranging (LiDAR) datasets collected in two different cities with some typical closely packed buildings were used to verify the proposed framework. The results show that the framework can effectively identify the closely packed buildings with unified structures from large airborne LiDAR datasets. The overall segmentation accuracy reaches 84% for the two datasets. The proposed framework can serve as a basis for analysis and segmentation of closely packed buildings with a more complicated structure. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.",,"Wang X., Chan T.O., Liu K., Pan J., Luo M., Li W., Wei C.",10.1080/01431161.2020.1727053
252,22.0,2020,Article,Comparative assessment of machine learning methods for urban vegetation mapping using multitemporal Sentinel-1 imagery,"Mapping of green vegetation in urban areas using remote sensing techniques can be used as a tool for integrated spatial planning to deal with urban challenges. In this context, multitemporal (MT) synthetic aperture radar (SAR) data have not been equally investigated, as compared to optical satellite data. This research compared various machine learning methods using single-date and MT Sentinel-1 (S1) imagery. The research was focused on vegetation mapping in urban areas across Europe. Urban vegetation was classified using six classifiers-random forests (RF), support vector machine (SVM), extreme gradient boosting (XGB), multi-layer perceptron (MLP), AdaBoost. M1 (AB), and extreme learning machine (ELM). Whereas, SVM showed the best performance in the single-date image analysis, the MLP classifier yielded the highest overall accuracy in the MT classification scenario. Mean overall accuracy (OA) values for all machine learning methods increased from 57% to 77% with speckle filtering. Using MT SAR data, i.e., three and five S1 imagery, an additional increase in the OA of 8.59% and 13.66% occurred, respectively. Additionally, using three and five S1 imagery for classification, the F1 measure for forest and low vegetation land-cover class exceeded 90%. This research allowed us to confirm the possibility of MT C-band SAR imagery for urban vegetation mapping. © 2020 by the authors.",Land-cover classification; Multitemporal; Sentinel-1; Speckle filtering; Synthetic aperture radar (SAR); Urban vegetation,"Gašparović M., Dobrinić D.",10.3390/rs12121952
260,16.0,2020,Letter,Regional mapping of essential urban land use categories in China: A segmentation-based approach,"Understanding distributions of urban land use is of great importance for urban planning, decision support, and resource allocation. The first mapping results of essential urban land use categories (EULUC) in China for 2018 have been recently released. However, such kind of national maps may not sufficiently meet the growing demand for regional analysis. To address this shortcoming, here we proposed a segmentation-based framework named EULUC-seg to improve the mapping results of EULUC at the city scale. An object-based segmentation approach was first applied to generate the basic mapping units within urban parcels. Multiple features derived from high-resolution remotely sensed and social sensing data were updated and then recalculated within each unit. Random forest was adopted as the machine learning algorithm for classifying urban land use into five Level I classes and twelve Level II classes. Finally, an accuracy assessment was carried out based on a collection of manually interpreted samples. Results showed that our derived map achieved an overall accuracy of 87.58% for Level I, and 73.53% for Level II. The accurate and refined map of EULUC-seg is expected to better support various applications in the future. © 2020, by the authors.",Machine learning; Ningbo; Segmentation; Urban land use,"Tu Y., Chen B., Zhang T., Xu B.",10.3390/rs12071058
282,2.0,2020,Conference Paper,Land cover classification based on machine learning using UAV multi-spectral images,"Land cover classification using UAV multi-spectral images is of great significance in precision agriculture, urban planning, land use and other fields. However, traditional remote sensing image classification methods cannot meet the classification accuracy requirements of UAV multi-spectral images. This paper aims to propose an object-based machine learning classification method to improve the land over classification accuracy of UAV multi-spectral images. The experimental area is a standard test field located in the Jilin Province of China. The experimental data was captured by a UAV equipped with a multi-spectral camera which includes four bands from 550 nm to 790 nm. First, the original images were preprocessed and the spectral curves of land cover were analyzed, thus four kinds of land cover with large differences were selected as categories. Then pixel-based, boosting-based and object-based machine learning methods were used for classification. The object-based classification method could make full use of the spatial and spectral information, and eliminate the noise problem caused by the high resolution of the UAV image to a certain extent. Finally, accuracy analysis using the verification image showed that the RF-O method achieved the highest classification accuracy of 92.2419%, and the kappa coefficient was 0.8904. All results indicate that the object-based machine learning classification method proposed in this paper is more suitable for the research of land cover classification, comparing with the traditional remote sensing image classification methods, and performs well on the land cover classification of UAV multi-spectral images. © 2020 SPIE.",Image classification; Machine learning; UAV remote sensing,"Pan L., Gu L., Ren R., Yang S.",10.1117/12.2566128
283,,2020,Conference Paper,"Remote sensing and GIS approach for environmental green areas planning using Landsat satellite imagery, Dubai-UAE","Over the last decade, Dubai emirate witnessed a vast, rapidly growing population, that doubled since 2008. Nowadays, Dubai considers as the most populated emirate within the United Arab Emirates (UAE). With such an increasing population and new urban developments, sustainable urban planning procedures play an essential role in Dubai's environmental quality such as air quality, and pollution. Therefore, this study will utilize the Remote Sensing and Geographic Information system (GIS) to investigate Dubai's environmental quality by addressing and locating green areas and pollution percentages within each district. The study methodology is divided into three steps. First, Landsat Satellite medium spatial resolution and multi-spectral imagery will be used as an input for segmentation and object-based analysis. Considering the spectral and spatial signatures for green areas machine learning techniques will be adopted to select the most significant features to classify and extract green areas. Second, using environmental relational indices, green areas percentages will be quantitatively compared to Sentinel air quality data, such as NO2 and SO2, as well as the population density maps. Finally, GIS techniques will be used to create Dubai Environmental Critical Map (DECM), to locate districts with limited green areas and high pollution to improve environmental standards. The study results can be used as a measure for the municipality policymakers to ensure sustainable urban development for a healthy living. © 2020 SPIE",Dubai Environmental Vulnerability Map (DEVM); Geographic Information system (GIS); Remote Sensing,"Aldogom D., Mansoori S.A., AlMaazmi A., Nazzal T.",10.1117/12.2573904
285,70.0,2020,Article,Google Earth Engine Cloud Computing Platform for Remote Sensing Big Data Applications: A Comprehensive Review,"Remote sensing (RS) systems have been collecting massive volumes of datasets for decades, managing and analyzing of which are not practical using common software packages and desktop computing resources. In this regard, Google has developed a cloud computing platform, called Google Earth Engine (GEE), to effectively address the challenges of big data analysis. In particular, this platform facilitates processing big geo data over large areas and monitoring the environment for long periods of time. Although this platform was launched in 2010 and has proved its high potential for different applications, it has not been fully investigated and utilized for RS applications until recent years. Therefore, this study aims to comprehensively explore different aspects of the GEE platform, including its datasets, functions, advantages/limitations, and various applications. For this purpose, 450 journal articles published in 150 journals between January 2010 and May 2020 were studied. It was observed that Landsat and Sentinel datasets were extensively utilized by GEE users. Moreover, supervised machine learning algorithms, such as Random Forest, were more widely applied to image classification tasks. GEE has also been employed in a broad range of applications, such as Land Cover/land Use classification, hydrology, urban planning, natural disaster, climate analyses, and image processing. It was generally observed that the number of GEE publications have significantly increased during the past few years, and it is expected that GEE will be utilized by more users from different fields to resolve their big data processing challenges. © 2008-2012 IEEE.",Big data; cloud computing; Google Earth Engine (GEE); remote sensing (RS),"Amani M., Ghorbanian A., Ahmadi S.A., Kakooei M., Moghimi A., Mirmazloumi S.M., Moghaddam S.H.A., Mahdavi S., Ghahremanloo M., Parsian S., Wu Q., Brisco B.",10.1109/JSTARS.2020.3021052
289,4.0,2020,Article,A Self-Supervised Learning Framework for Road Centerline Extraction from High-Resolution Remote Sensing Images,"Road extraction from the high-resolution remote sensing image is significant for the land planning, vehicle navigation, etc. The existing road extraction methods normally need many preprocessing and subsequent optimization steps. Therefore, an automatic road centerline extraction method based on the self-supervised learning framework for high-resolution remote sensing image is proposed. This proposed method does not need to manually select training samples and other optimization steps, such as the nonroad area removing. First, the positive sample selection method combining the spectral and shape features is proposed to extract the road sample. Then, the one-class classifier framework is introduced and the random forest positive unlabeled learning classifier is constructed to get the posterior probability of the pixel belonging to road. The shape feature and the posterior probability are combined to form the final road network in the object-oriented way. Finally, the road centerline is obtained through the tensor voting algorithm. In order to verify the effectiveness of the proposed algorithm, high-resolution remote sensing images and benchmark datasets are used to do experiments. The indexes of the completeness ratio, the correctness ratio, and the detection quality are used for the quantitative accuracy evaluation. Compared with the supervised, the unsupervised, and the one-class classification road extraction algorithms, this proposed algorithm achieves high accuracy and efficiency. For the deep learning method comparison, the deep learning method performs well in most cases especially in the complex urban area. However, the deep learning method needs a large number of samples and a long training time, and our self-supervised learning framework does not need the training samples. © 2008-2012 IEEE.",High-resolution remote sensing image; one-class classifier; road centerline; road extraction; self-supervised learning,"Guo Q., Wang Z.",10.1109/JSTARS.2020.3014242
312,11.0,2019,Conference Paper,Mapping Urban Trees Within Cadastral Parcels Using an Object-Based Convolutional Neural Network,"Urban trees offer significant benefits for improving the sustainability and liveability of cities, but its monitoring is a major challenge for urban planners. Remote-sensing based technologies can effectively detect, monitor and quantify urban tree coverage as an alternative to field-based measurements. Automatic extraction of urban land cover features with high accuracy is a challenging task and it demands artificial intelligence workflows for efficiency and thematic quality. In this context, the objective of this research is to map urban tree coverage per cadastral parcel of Sandy Bay, Hobart from very high-resolution aerial orthophoto and LiDAR data using an Object Based Convolution Neural Network (CNN) approach. Instead of manual preparation of a large number of required training samples, automatically classified Object based image analysis (OBIA) output is used as an input samples to train CNN method. Also, CNN output is further refined and segmented using OBIA to assess the accuracy. The result shows 93.2% overall accuracy for refined CNN classification. Similarly, the overlay of improved CNN output with cadastral parcel layer shows that 21.5% of the study area is covered by trees. This research demonstrates that the accuracy of image classification can be improved by using a combination of OBIA and CNN methods. Such a combined method can be used where manual preparation of training samples for CNN is not preferred. Also, our results indicate that the technique can be implemented to calculate parcel level statistics for urban tree coverage that provides meaningful metrics to guide urban planning and land management practices. © 2019 Authors.",Cadastral Parcel; Convolutional Neural Network; GEOBIA; Machine Learning; Urban Trees,"Timilsina S., Sharma S.K., Aryal J.",10.5194/isprs-annals-IV-5-W2-111-2019
327,12.0,2019,Conference Paper,Aerial point cloud classification with deep learning and machine learning algorithms,"With recent advances in technology, 3D point clouds are getting more and more frequently requested and used, not only for visualization needs but also e.g. by public administrations for urban planning and management. 3D point clouds are also a very frequent source for generating 3D city models which became recently more available for many applications, such as urban development plans, energy evaluation, navigation, visibility analysis and numerous other GIS studies. While the main data sources remained the same (namely aerial photogrammetry and LiDAR), the way these city models are generated have been evolving towards automation with different approaches. As most of these approaches are based on point clouds with proper semantic classes, our aim is to classify aerial point clouds into meaningful semantic classes, e.g. ground level objects (GLO, including roads and pavements), vegetation, buildings' facades and buildings' roofs. In this study we tested and evaluated various machine learning algorithms for classification, including three deep learning algorithms and one machine learning algorithm. In the experiments, several hand-crafted geometric features depending on the dataset are used and, unconventionally, these geometric features are used also for deep learning. © 2019 E. Özdemir et al.",Classification; Deep learning; Geometric features; Machine learning; Point cloud; Urban areas,"Özdemir E., Remondino F., Golkar A.",10.5194/isprs-archives-XLII-4-W18-843-2019
328,33.0,2019,Article,Simulation of urban expansion via integrating artificial neural network with Markov chain–cellular automata,"Accurate simulations and predictions of urban expansion are critical to manage urbanization and explicitly address the spatiotemporal trends and distributions of urban expansion. Cellular Automata integrated Markov Chain (CA-MC) is one of the most frequently used models for this purpose. However, the urban suitability index (USI) map produced from the conventional CA-MC is either affected by human bias or cannot accurately reflect the possible nonlinear relations between driving factors and urban expansion. To overcome these limitations, a machine learning model (Artificial Neural Network, ANN) was integrated with CA-MC instead of the commonly used Analytical Hierarchy Process (AHP) and Logistic Regression (LR) CA-MC models. The ANN was optimized to create the USI map and then integrated with CA-MC to spatially allocate urban expansion cells. The validated results of kappa and fuzzy kappa simulation indicate that ANN-CA-MC outperformed other variously coupled CA-MC modelling approaches. Based on the ANN-CA-MC model, the urban area in South Auckland is predicted to expand to 1340.55 ha in 2026 at the expense of non-urban areas, mostly grassland and open-bare land. Most of the future expansion will take place within the planned new urban growth zone. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",artificial neural network; cellular automata; kappa simulation; Markov chain; South Auckland; Urban expansion,"Xu T., Gao J., Coco G.",10.1080/13658816.2019.1600701
333,2.0,2019,Conference Paper,Machine Learning for Strategic Urban Planning,"Data mining is an important part of strategic planning for the development of modern urban settlement with capacities to accommodate population explosion. Developing countries are fast becoming urbanized giving the developments and opportunities that are lacking in rural areas. Data regarding urban development such as satellite image need to be analysed to ascertain the possibilities for further development or opening up of new settlements. This work presents a binary sub-pixel and feature based method of classification to detect water bodies and vegetation in earth observatory images. In this work, the images were subjected data pre-processing, feature extraction, and analysed the data using machine learning classification method to detection regions that support urban expansion or development of new settlement. The proposed method achieved 88.93% accuracy and 0.06% RMSE. © 2019 IEEE.",feature engineering; Haar-cascade; machine learning; object detection; Satellite image,"Odaudu S.N., Umoh I.J., Mu'Azu M.B., Adedokun E.A.",10.1109/NigeriaComputConf45974.2019.8949665
378,21.0,2019,Article,Multiscale road centerlines extraction from high-resolution aerial imagery,"Accurate road extraction from high-resolution aerial imagery has many applications such as urban planning and vehicle navigation system. The common road extraction methods are based on classification algorithm, which needs to design robust handcrafted features for road. However, designing such features is difficult. For the road centerlines extraction problem, the existing algorithms have some limitations, such as spurs, time consuming. To address the above issues to some extent, we introduce the feature learning based on deep learning to extract robust features automatically, and present a method to extract road centerlines based on multiscale Gabor filters and multiple directional non-maximum suppression. The proposed algorithm consists of the following four steps. Firstly, the aerial imagery is classified by a pixel-wise classifier based on convolutional neural network (CNN). Specifically, CNN is used to learn features from raw data automatically, especially the structural features. Then, edge-preserving filtering is conducted on the resulting classification map, with the original imagery serving as the guidance image. It is exploited to preserve the edges and the details of the road. After that, we do some post-processing based on shape features to extract more reliable roads. Finally, multiscale Gabor filters and multiple directional non-maximum suppression are integrated to get a complete and accurate road network. Experimental results show that the proposed method can achieve comparable or higher quantitative results, as well as more satisfactory visual performance. © 2018 Elsevier B.V.",Centerlines extraction; Convolutional neural network (CNN); Edge-preserving filtering; Multiscale Gabor filters,"Liu R., Miao Q., Song J., Quan Y., Li Y., Xu P., Dai J.",10.1016/j.neucom.2018.10.036
393,3.0,2019,Conference Paper,Multi-scale correlation-based feature selection and random forest classification for LULC mapping from the integration of SAR and optical Sentinel images,"Reliable and accurate land use/land cover (LULC) map is a crucial data source for the understanding of coupled human-environment systems, monitoring changes, timely low-cost planning, and management of natural resources. Improvements in sensor technologies and machine learning capabilities have shifted the attention of remote sensing community to data complementarity through fusion of multi-sensor data for accurate feature extraction and mapping. Amalgamation of optical and synthetic aperture radar (SAR) images has shown promising advantages in enhancing the accuracy of extracting LULC as such method allows exploitation of information in sensors. This study investigated the potential of using freely available multisource Sentinel images to extract LULC maps in semi-arid environments through multi-scale geographic object-based image analysis (GEOBIA). A multi-scale classification framework that integrates GEOBIA, correlation-based feature selection (CFS), and random forest (RF)-supervised classification was adopted to extract LULC from assimilation of Sentinel multi-sensor products. First, Sentinel-1 and-2 images were pre-processed. Second, optimum multi-scale segmentation levels were selected using F-score segmentation quality measures. Third, 70 features of various spectral indices and derivatives and geometrical features from optical data and multiple ratios and textural features from dual-polarization SAR images were computed, and a CFS based on wrapper approach was used to select the most significant features at multi-scale levels. Finally, a single and multi-scale RF classifier was used to extract LULC classes using the most relevant features extracted from Sentinel SAR and optical images. Results of multi-scale image segmentation optimization showed that scale parameter (SP) values of 40, 60, and 150 were optimal for extraction of LULC classes. Results of feature selection showed that 22, 24, and 27 features were selected at scale SP values of 40, 60, and 150, respectively. Half of the features were common among the three scales. Single RF classification yielded overall accuracy (OA) values of 92.10%, 93%, and 91% and kappa coefficients of 0.901, 0.912, and 0.89 at scale values of 150, 60, and 40, respectively. Multiscale RF classification from scale values of 150 and 60 produced better LULC classification with OA 96.06% and kappa coefficient of 0.95 compared with other scale SP values. The integrated approach demonstrated an effective and promising method for high-quality LULC extraction from coupling optical and SAR images. Overall, multi-sensor Sentinel images along with the adopted approach feature a remarkable potential for improving LULC extraction and can effectively be used to update geographic information system layers for various applications. © 2019 SPIE.",data fusion; image segmentation optimization; LULC; object-based classification; Optical sensors; SAR,"Al-Ruzouq R., Shanableh A., Gibril M.B., Kalantar B.",10.1117/12.2533123
405,30.0,2019,Article,Support Vector Machine accuracy assessment for extracting green urban areas in towns,"The most commonly used model for analyzing satellite imagery is the Support Vector Machine (SVM). Since there are a large number of possible variables for use in SVM, this paper will provide a combination of parameters that fit best for extracting green urban areas from Copernicus mission satellite images. This paper aims to provide a combination of parameters to extract green urban areas with the highest degree of accuracy, in order to speed up urban planning and ultimately improve town environments. Two different towns in Croatia were investigated, and the results provide an optimal combination of parameters for green urban areas extraction with an overall kappa index of 0.87 and 0.89, which demonstrates a very high classification accuracy. © 2019 by the authors.",Green urban areas extraction; Kernels; Machine learning; Satellite images; Support vector machine,"Kranjčić N., Medak D., Župan R., Rezo M.",10.3390/rs11060655
427,17.0,2018,Article,Progressively Expanded Neural Network (PEN Net) for hyperspectral image classification: A new neural network paradigm for remote sensing image analysis,"Hyperspectral image (HSI) has been used for a wide range of applications including forestry, urban planning, and precision agriculture. In recent years, machine learning based algorithms, such as support vector machines, decision trees, ensemble learning, and their variations have shown promising results in HSI analysis. Such methodologies, nevertheless, can lead to insufficient information abstraction in interpreting hyperspectral pixels. In this paper, we propose a novel neural network based classification algorithm, named Progressively Expanded Neural Network (PEN Net), that can effectively interpret hyperspectral pixels in nonlinear feature spaces and then determine their categories. Furthermore, a spectral-spatial HSI classification framework is also introduced to test the generality and robustness of the PEN Net. Experimental results on four standard hyperspectral datasets illustrate that: (1) PEN Net classifier yields better accuracy and competitive processing speed in HSI classification tasks compared to the state-of-the-art methods; (2) Multi-hidden layer based PEN Net generally provides better performance than single hidden layer one; (3) Combination of spectral and spatial features in the PEN Net classifier can significantly improve the classification accuracy by 6–15% compared to the spectral only based HSI classification. This study implies that the proposed neural network architecture opens a new window for future research and the potential for remote sensing image analysis. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)",Classification; Hyperspectral image (HSI); Machine learning; Neural network; Remote sensing,"Sidike P., Asari V.K., Sagan V.",10.1016/j.isprsjprs.2018.09.007
437,12.0,2018,Article,Ultra-Light aircraft-based hyperspectral and colour-infrared imaging to identify deciduous tree species in an urban environment,"One may consider the application of remote sensing as a trade-off between the imaging platforms, sensors, and data gathering and processing techniques. This study addresses the potential of hyperspectral imaging using ultra-light aircraft for vegetation species mapping in an urban environment, exploring both the engineering and scientific aspects related to imaging platform design and image classification methods. An imaging system based on simultaneous use of Rikola frame format hyperspectral and Nikon D800E adopted colour infrared cameras installed onboard a Bekas X32 manned ultra-light aircraft is introduced. Two test imaging flight missions were conducted in July of 2015 and September of 2016 over a 4000 ha area in Kaunas City, Lithuania. Sixteen and 64 spectral bands in 2015 and 2016, respectively, in a spectral range of 500-900 nm were recorded with colour infrared images. Three research questions were explored assessing the identification of six deciduous tree species: (1) Pre-treatment of spectral features for classification, (2) testing five conventional machine learning classifiers, and (3) fusion of hyperspectral and colour infrared images. Classification performance was assessed by applying leave-one-out cross-validation at the individual crown level and using as a reference at least 100 field inventoried trees for each species. The best-performing classification algorithm-multilayer perceptron, using all spectral properties extracted from the hyperspectral images-resulted in a moderate classification accuracy. The overall classification accuracy was 63%, Cohen's Kappa was 0.54, and the species-specific classification accuracies were in the range of 51-72%. Hyperspectral images resulted in significantly better tree species classification ability than the colour infrared images and simultaneous use of spectral properties extracted from hyperspectral and colour infrared images improved slightly the accuracy over the 2015 image. Even though classifications using hyperspectral data cubes of 64 bands resulted in relatively larger accuracies than with 16 bands, classification error matrices were not statistically different. Alternative imaging platforms (like an unmanned aerial vehicle and a Cessna 172 aircraft) and settings of the flights were discussed using simulated imaging projects assuming the same study area and field of application. Ultra-light aircraft-based hyperspectral and colour-infrared imaging was considered to be a technically and economically sound solution for urban green space inventories to facilitate tree mapping, characterization, and monitoring. © 2018 by the authors.",Classification; Colour infrared; Hyperspectral; Ultra-light aircraft; Urban trees,"Mozgeris G., Juodkiene V., Jonikavičius D., Straigyte L., Gadal S., Ouerghemmi W.",10.3390/rs10101668
443,18.0,2018,Article,Road centerline extraction from very-high-resolution aerial image and LiDAR data based on road connectivity,"The road networks provide key information for a broad range of applications such as urban planning, urban management, and navigation. The fast-developing technology of remote sensing that acquires high-resolution observational data of the land surface offers opportunities for automatic extraction of road networks. However, the road networks extracted from remote sensing images are likely affected by shadows and trees, making the road map irregular and inaccurate. This research aims to improve the extraction of road centerlines using both very-high-resolution (VHR) aerial images and light detection and ranging (LiDAR) by accounting for road connectivity. The proposed method first applies the fractal net evolution approach (FNEA) to segment remote sensing images into image objects and then classifies image objects using the machine learning classifier, random forest. A post-processing approach based on the minimum area bounding rectangle (MABR) is proposed and a structure feature index is adopted to obtain the complete road networks. Finally, a multistep approach, that is, morphology thinning, Harris corner detection, and least square fitting (MHL) approach, is designed to accurately extract the road centerlines from the complex road networks. The proposed method is applied to three datasets, including the New York dataset obtained from the object identification dataset, the Vaihingen dataset obtained from the International Society for Photogrammetry and Remote Sensing (ISPRS) 2D semantic labelling benchmark and Guangzhou dataset. Compared with two state-of-the-art methods, the proposed method can obtain the highest completeness, correctness, and quality for the three datasets. The experiment results show that the proposed method is an efficient solution for extracting road centerlines in complex scenes from VHR aerial images and light detection and ranging (LiDAR) data. © 2018 by the authors.",LiDAR data; Object recognition; Road centerline; Road connectivity; Very-high-resolution image,"Zhang Z., Zhang X., Sun Y., Zhang P.",10.3390/rs10081284
444,7.0,2018,Article,Context-Based Filtering of Noisy Labels for Automatic Basemap Updating from UAV Data,"Unmanned aerial vehicles (UAVs) have the potential to obtain high-resolution aerial imagery at frequent intervals, making them a valuable tool for urban planners who require up-to-date basemaps. Supervised classification methods can be exploited to translate the UAV data into such basemaps. However, these methods require labeled training samples, the collection of which may be complex and time consuming. Existing spatial datasets can be exploited to provide the training labels, but these often contain errors due to differences in the date or resolution of the dataset from which these outdated labels were obtained. In this paper, we propose an approach for updating basemaps using global and local contextual cues to automatically remove unreliable samples from the training set, and thereby, improve the classification accuracy. Using UAV datasets over Kigali, Rwanda, and Dar es Salaam, Tanzania, we demonstrate how the amount of mislabeled training samples can be reduced by 44.1% and 35.5%, respectively, leading to a classification accuracy of 92.1% in Kigali and 91.3% in Dar es Salaam. To achieve the same accuracy in Dar es Salaam, between 50000 and 60000 manually labeled image segments would be needed. This demonstrates that the proposed approach of using outdated spatial data to provide labels and iteratively removing unreliable samples is a viable method for obtaining high classification accuracies while reducing the costly step of acquiring labeled training samples. © 2008-2012 IEEE.",Basemap updating; image classification; informal settlements; label noise; random forests; unmanned aerial vehicles (UAVs); urban planning,"Gevaert C.M., Persello C., Elberink S.O., Vosselman G., Sliuzas R.",10.1109/JSTARS.2017.2762905
452,1.0,2018,Conference Paper,Application of machine learning in urban greenery land cover extraction,"Urban greenery is a critical part of the modern city and the greenery coverage information is essential for land resource management, environmental monitoring and urban planning. It is a challenging work to extract the urban greenery information from remote sensing image as the trees and grassland are mixed with city built-ups. In this paper, we propose a new automatic pixel-based greenery extraction method using multispectral remote sensing images. The method includes three main steps. First, a small part of the images is manually interpreted to provide prior knowledge. Secondly, a five-layer neural network is trained and optimised with the manual extraction results, which are divided to serve as training samples, verification samples and testing samples. Lastly, the well-trained neural network will be applied to the unlabelled data to perform the greenery extraction. The GF-2 and GJ-1 high resolution multispectral remote sensing images were used to extract greenery coverage information in the built-up areas of city X. It shows a favourable performance in the 619 square kilometers areas. Also, when comparing with the traditional NDVI method, the proposed method gives a more accurate delineation of the greenery region. Due to the advantage of low computational load and high accuracy, it has a great potential for large area greenery auto extraction, which saves a lot of manpower and resources. © Authors 2018. CC BY 4.0 License.",Auto extraction; Greenery land cover; Machine learning; Multispectral image; Neural network,"Qiao X., Li L.L., Li D., Gan Y.L., Hou A.Y.",10.5194/isprs-archives-XLII-3-1409-2018
476,61.0,2018,Article,Exploring the optimal integration levels between SAR and optical data for better urban land cover mapping in the Pearl River Delta,"Integrating synthetic aperture radar (SAR) and optical data to improve urban land cover classification has been identified as a promising approach. However, which integration level is the most suitable remains unclear but important to many researchers and engineers. This study aimed to compare different integration levels for providing a scientific reference for a wide range of studies using optical and SAR data. SAR data from TerraSAR-X and ENVISAT ASAR in both WSM and IMP modes were used to be combined with optical data at pixel level, feature level and decision levels using four typical machine learning methods. The experimental results indicated that: 1) feature level that used both the original images and extracted features achieved a significant improvement of up to 10% compared to that using optical data alone; 2) different levels of fusion required different suitable methods depending on the data distribution and data resolution. For instance, support vector machine was the most stable at both the feature and decision levels, while random forest was suitable at the pixel level but not suitable at the decision level. 3) By examining the distribution of SAR features, some features (e.g., homogeneity) exhibited a close-to-normal distribution, explaining the improvement from the maximum likelihood method at the feature and decision levels. This indicated the benefits of using texture features from SAR data when being combined with optical data for land cover classification. Additionally, the research also shown that combining optical and SAR data does not guarantee improvement compared with using single data source for urban land cover classification, depending on the selection of appropriate fusion levels and fusion methods. © 2017 Elsevier B.V.",Fusion level; Fusion strategies; Optical and SAR fusion; Urban land cover,"Zhang H., Xu R.",10.1016/j.jag.2017.08.013
478,5.0,2017,Conference Paper,3D shape descriptor for objects recognition,"3D point cloud classification is an important task in applications for many areas such as robotics, urban planning and augmented reality. 3D sensors measure a high amount of points in the 3D scene objects' surface at a high collect rate, so robust techniques are needed to process all input data and also deal with some imprecision. A common solution for these tasks is the use of robust features extraction techniques to gather representative scene information at the lowest computational cost possible. This paper presents a new approach for object recognition in 3D scenes, using a novel 3D shape descriptor which is used as input for a supervised machine learning method. Proposed robust 3D feature is invariant to translation and scale and provides a very simplified object representation for pattern recognition input. Experiments were performed using an Artificial Neural Network to recognize six different object shapes, and obtained results showed that the proposed method is a promising approach for object recognition in 3D scenes. © 2017 IEEE.",3D Feature Extraction; Object Classification; Pattern Recognition,"Sales D.O., Amaro J., Osório F.S.",10.1109/SBR-LARS-R.2017.8215285
479,7.0,2017,Conference Paper,Deep highway unit network for land cover type classification with GF-3 SAR imagery,"The fully polarized synthetic aperture radar (SAR) is an advanced earth observation system with day and night imaging capability, which can obtain rich information of terrain and has a wide range of applications in environmental protection, urban planning and resource investigation. As the first selfdeveloped C-band multi-polarized SAR image, the acquisition of massive data and operational operation of Chinese SAR remote sensing has entered the era of big data. Under the era of remote sensing large data, however, SAR image interpretation is a great challenge for scientific applications. At present, big data-based intelligent methods such as computer vision technology have achieved great success. Deep learning such as deep highway unit networks has revolutionized the computer vision area. However, due to the characteristics of SAR microwave band imaging and phase coherence processing, SAR images are very different from ordinary optical images in terms of band, projection direction, data composition and so on. Therefore, deep learning can not be directly used for quad-pol SAR image classification. In this paper, deep learning is applied to land cover type classification with GF-3 quad-pol SAR imagery. A deep highway unit network is employed to automatically extract a hierarchic feature representation from the data, based on which the land cover type classification can be conducted. Our classification model is trained on limited training data from forest resource inventory and planning data, and tested on a Radarsat-2 quad-pol images, which is the image of the same area acquired at different times. We also employ the machine learning such as SVM, Random Forest on the same samples for comparison. The deep highway unit network trained by the GF-3 images, which can reduce speckle, fully excavate the regularity of SAR images in time and space. © 2017 IEEE.",Deep highway unit networks; Deep learning; GaoFen-3; Land cover type classification,"Guo Y., Chen E., Guo Y., Li Z., Li C., Xu K.",10.1109/BIGSARDATA.2017.8124926
496,4.0,2017,Article,Urban areas extraction from multi sensor data based on machine learning and data fusion,"Accurate urban areas information is important for a variety of applications, especially city planning and natural disaster prediction and management. In recent years, extraction of urban structures from remotely sensed images has been extensively explored. The key advantages of this imaging modality are reduction of surveying expense and time. It also elevates restrictions on ground surveys. Thus far, much research typically extracts these structures from very high resolution satellite imagery, which are unfortunately of relatively poor spectral resolution, resulting in good precision yet moderate accuracy. Therefore, this paper investigates extraction of buildings from middle and high resolution satellite images by using spectral indices (Normalized Difference Building Index: NDBI, Normalized Difference Vegetation Index: NDVI, Soil Adjustment Vegetation Index: SAVI, Modified Normalized Difference Index: MNDWI, and Global Environment Monitoring Index: GEMI) by means of various Machine Learning methods (Artificial Neural Network: ANN, K-Nearest Neighbor: KNN, and Support Vector Machine: SVM) and Data Fusion (i.e., Majority Voting). Herein empirical results suggested that suitable learning methods for urban areas extraction are in preferring order Data Fusion, SVM, KNN, and ANN. Their accuracies were 85.46, 84.86, 84.66, and 84.91%, respectively. © 2017, Pleiades Publishing, Ltd.",data fusion; machine learning; spectral indices; urban areas extraction,"Puttinaovarat S., Horkaew P.",10.1134/S1054661816040131
507,,2017,Conference Paper,A support vector machine approach on object based image analysis for feature extraction from high resolution images,"Satellite images are the most important available data sources for generation and updating of available maps. They have highly improved in terms of spatial, spectral and temporal resolutions and by the sheer volume of collected images, the necessity of simplification of automation in feature extraction. Road data play a key role in urban planning, traffic management, military applications, and vehicle navigation as well as for decision making in numerous applications. The faster updation of road infrastructure is a need because the technology has brought map in the hands of people in the form of mobile phones and tablets. Road detection is one of the major issues of the road infrastructure extraction. Its accuracy depends on the type of methodology used. An attempt is made here to analyse the first order, the co-occurrence texture features and image transforms useful for discriminating roads from other features specially the buildings. The identified dataset forms high dimension feature space and the Support Vector Machine is a theoretically superior machine learning methodology with great results in classification of high dimensional datasets. In the past, SVMs have been tested and evaluated only as pixel-based image classifiers. Moving from pixel-based techniques towards object-based representation, the dimensions of remote sensing imagery feature space increases significantly. An SVM approach for classification was followed, based on primitive image objects produces by a multi-resolution segmentation algorithm. The SVM procedure produced the final object classification results which were compared to the Nearest Neighbor classifier results and were found to give better results in OBIA domain. © 2017 ACRS. All rights reserved.",Feature Extraction; Grey-Level Co-occurrence textures; Object Based Image Analysis (OBIA); Support Vector Machine (SVM),"Kumar M., Srivastav S.K., Garg P.K.",
528,34.0,2016,Article,A supervoxel-based spectro-spatial approach for 3D urban point cloud labelling,"ABSTRACT: Three-dimensional (3D) point cloud labelling of airborne lidar (light detection and ranging) data has promising applications in urban city modelling. Automatic and efficient methods for semantic labelling of airborne urban point cloud data with multiple classes still remains a challenge. We propose a novel 3D object-based classification framework for labelling urban lidar point cloud using a computer vision technique, supervoxels. The supervoxel approach is promising for representing dense lidar point cloud in a compact manner for 3D segmentation and for improving the computational efficiency. Initially, supervoxels are generated by over-segmenting the coloured point cloud using the voxel-based cloud connectivity algorithm in the geometric space. The local connectivity established between supervoxels has been used to produce meaningful and realistic objects (segments). The segments are classified by different machine learning techniques based on several spectral and geometric features extracted from the segments. All the points within a labelled segment are assigned the same segment label. Furthermore, the effect of different feature vectors and varying point density on the classification accuracy has been studied. Results indicate an accurate labelling of points in realistic 3D space conforming to the boundaries of objects. An overall classification accuracy of 90% is achieved by the proposed method. The labelled 3D points can be used directly for the reconstruction of buildings and other man-made objects. © 2016 Informa UK Limited, trading as Taylor & Francis Group.",,"Ramiya A.M., Nidamanuri R.R., Ramakrishnan K.",10.1080/01431161.2016.1211348
531,29.0,2016,Article,Land Classification Using Remotely Sensed Data: Going Multilabel,"Obtaining an up-to-date high-resolution description of land cover is a challenging task due to the high cost and labor-intensive process of human annotation through field studies. This work introduces a radically novel approach for achieving this goal by exploiting the proliferation of remote sensing satellite imagery, allowing for the up-to-date generation of global-scale land cover maps. We propose the application of multilabel classification, a powerful framework in machine learning, for inferring the complex relationships between the acquired satellite images and the spectral profiles of different types of surface materials. Introducing a drastically different approach compared to unsupervised spectral unmixing, we employ contemporary ground-collected data from the European Environment Agency to generate the label set and multispectral images from the MODIS sensor to generate the spectral features, under a supervised classification framework. To validate the merits of our approach, we present results using several state-of-the-art multilabel learning classifiers and evaluate their predictive performance with respect to the number of annotated training examples, as well as their capability to exploit examples from neighboring regions or different time instances. We also demonstrate the application of our method on hyperspectral data from the Hyperion sensor for the urban land cover estimation of New York City. Experimental results suggest that the proposed framework can achieve excellent prediction accuracy, even from a limited number of diverse training examples, surpassing state-of-the-art spectral unmixing methods. © 2016 IEEE.",CORINE; data processing; land cover; MODIS; pattern classification; remote sensing; satellite applications; time series; unmixing,"Karalas K., Tsagkatakis G., Zervakis M., Tsakalides P.",10.1109/TGRS.2016.2520203
552,58.0,2015,Article,Combining Pixel-and Object-Based Machine Learning for Identification of Water-Body Types from Urban High-Resolution Remote-Sensing Imagery,"Water is one of the vital components for the ecological environment, which plays an important role in human survival and socioeconomic development. Water resources in urban areas are gradually decreasing due to the rapid urbanization, especially in developing countries. Therefore, the precise extraction and automatic identification of water bodies are of great significance and urgently required for urban planning. It should be noted that although some studies have been reported regarding the water-area extraction, to our knowledge, few papers concern the identification of urban water types (e.g., rivers, lakes, canals, and ponds). In this paper, a novel two-level machine-learning framework is proposed for identifying the water types from urban high-resolution remote-sensing images. The framework consists of two interpretation levels: 1) water bodies are extracted at the pixel level, where the water/shadow/vegetation indexes are considered and 2) water types are further identified at the object level, where a set of geometrical and textural features are used. Both levels employ machine learning for the image interpretation. The proposed framework is validated using the GeoEye-1 and WorldView-2 images, over two mega cities in China, i.e., Wuhan and Shenzhen, respectively. The experimental results show that the proposed method achieved satisfactory accuracies for both water extraction [95.4% (Shenzhen), 96.2% (Wuhan)], and water type classification [94.1% (Shenzhen), 95.9% (Wuhan)] in complex urban areas. © 2015 IEEE.",High resolution; machine learning; object-oriented; water detection; water extraction; water index,"Huang X., Xie C., Fang X., Zhang L.",10.1109/JSTARS.2015.2420713
557,1.0,2015,Conference Paper,Comparison of different machine learning classifiers for building extraction in LiDAR-derived datasets,"Building extraction in remotely sensed imagery is an important problem that needs solving. It can be used to aid in urban planning, hazard assessments and disaster risk management among others. Light Detection and Ranging or LiDAR, is one of the most powerful remote sensing technologies nowadays. Many studies have used the fusion of LiDAR data and multispectral images in detecting buildings. This study seeks to maximize the power of LiDAR imagery to be able to classify buildings without the aid of multispectral imagery. This work follows the Object Based Image Analysis (OBIA) approach. Instead of the traditional pixel-based classification methods, pixels are segmented into logical groups called objects. From these objects, features for building extraction are calculated. These features are: the number of returns, difference of returns, and the mean and standard deviation of positive surface openness. These objects are then classified using different machine learning classifiers such as Support Vector Machines, K-Nearest Neighbors, Naïve Bayes Classifier, Decision Trees, and Random Forests. A comparative assessment was done on the performance of these different machine learning classifiers. The classifiers performed similarly with the Random Forest Classifier slightly outperforming the others.",Feature extraction; Object based image analysis,"Escamos I.M.H., Roberto A.R.C., Abucay E.R., Inciong G.K.L., Queliste M.D., Hermocilla J.A.C.",
559,64.0,2015,Article,Urban land use and land cover classification using remotely sensed sar data through deep belief networks,"Land use and land cover (LULC) mapping in urban areas is one of the core applications in remote sensing, and it plays an important role in modern urban planning and management. Deep learning is springing up in the field of machine learning recently. By mimicking the hierarchical structure of the human brain, deep learning can gradually extract features from lower level to higher level. The Deep Belief Networks (DBN) model is a widely investigated and deployed deep learning architecture. It combines the advantages of unsupervised and supervised learning and can archive good classification performance. This study proposes a classification approach based on the DBN model for detailed urban mapping using polarimetric synthetic aperture radar (PolSAR) data. Through the DBN model, effective contextual mapping features can be automatically extracted from the PolSAR data to improve the classification performance. Two-date high-resolution RADARSAT-2 PolSAR data over the Great Toronto Area were used for evaluation. Comparisons with the support vector machine (SVM), conventional neural networks (NN), and stochastic Expectation-Maximization (SEM) were conducted to assess the potential of the DBN-based classification approach. Experimental results show that the DBN-based method outperforms three other approaches and produces homogenous mapping results with preserved shape details. © 2015 Qi Lv et al.",,"Lv Q., Dou Y., Niu X., Xu J., Xu J., Xia F.",10.1155/2015/538063
563,11.0,2014,Article,Ensemble methods for binary classifications of airborne LIDAR data,"This paper presents a framework that is aimed at improving the performance of two existing ensemble methods (namely, AdaBoost and Bagging) for airborne light detection and ranging (LIDAR) classification. LIDAR is one of the fastest growing technologies to support a multitude of civil engineering applications, such as transportation, urban planning, flood control, and city 3D reconstruction. For the above applications, LIDAR data need to be classified into binary classes (i.e., terrain and nonterrain) or multiple classes (e.g., ground, vegetation, and buildings). The proposed framework is designed to enhance the generalization performance of binary classification approach by minimizing type II errors. The authors developed and tested the framework on different LIDAR data sets representing geographic sites in Germany and the United States. The results showed that the proposed ensemble framework performed better compared to the existing methods. In addition, the AdaBoost method outperformed the Bagging method on all the terrain types. However, the framework has some limitations in terms of dealing with rough terrain and discontinuous surfaces. © 2014 American Society of Civil Engineers.",Computing; Ensemble method; LIDAR; Machine learning; Remote sensing,"Nourzad S.H.H., Pradhan A.",10.1061/(ASCE)CP.1943-5487.0000276
592,28.0,2007,Conference Paper,Conditional random field for 3D point clouds with adaptive data reduction,"We proposed using Conditional Random Fields with adaptive data reduction for the classification of 3D point clouds acquired from a Riegl Terrestrial laser scanner. The training and inference of the acquired large outdoor urban data can be time consuming. We approach the problem by computing an adaptive support region for each data point using 3D scale theory. For training and inference of the discriminative Conditional Random Fields, smaller set of data samples that contains relevant information within the support region is selected instead of using all point cloud data. We tested the algorithm on synthetically generated data and urban point clouds data acquired from the laser scanner. The computed support region is also used in feature extraction for urban point clouds data. The results showed improvement in the training and inference rate while maintaining comparable classification accuracy. © 2007 IEEE.",Classifications; Conditional random fields; LIDAR data; Machine learning; Scale theory,"Lim E.H., Suter D.",10.1109/CW.2007.24
