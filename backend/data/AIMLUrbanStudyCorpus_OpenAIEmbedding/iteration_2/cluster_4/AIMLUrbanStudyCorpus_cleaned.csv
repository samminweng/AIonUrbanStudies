Cluster,DocId,Cited by,Title,Author Keywords,Abstract,Year,Source title,Authors,DOI,Document Type,x,y
4,8,,Automatic Target Detection from Satellite Imagery Using Machine Learning,Deep learning; Faster RCNN; Satellite images; SIMRDWN; SSD; YOLO,"Object detection is a vital step in satellite imagery-based computer vision applications such as precision agriculture, urban planning and defense applications. In satellite imagery, object detection is a very complicated task due to various reasons including low pixel resolution of objects and detection of small objects in the large scale (a single satellite image taken by Digital Globe com-prises over 240 million pixels) satellite images. Object detection in satellite images has many challenges such as class variations, multiple objects pose, high variance in object size, illumination and a dense background. This study aims to compare the performance of existing deep learning algorithms for object detection in satellite imagery. We created the dataset of satellite imagery to perform object detection using convolutional neural network-based frameworks such as faster RCNN (faster region-based convolutional neural network), YOLO (you only look once), SSD (single-shot detector) and SIMRDWN (satellite imagery multiscale rapid detection with windowed networks). In addition to that, we also performed an analysis of these approaches in terms of accuracy and speed using the developed dataset of satellite imagery. The results showed that SIMRDWN has an accuracy of 97% on high-resolution images, while Faster RCNN has an accuracy of 95.31% on the standard resolution (1000 × 600). YOLOv3 has an accuracy of 94.20% on standard resolution (416 416) while on the other hand SSD has an accuracy of 84.61% on standard resolution (300 × 300). When it comes to speed and efficiency, YOLO is the obvious leader. In real-time surveillance, SIMRDWN fails. When YOLO takes 170 to 190 milliseconds to perform a task, SIMRDWN takes 5 to 103 milliseconds. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",2022,Sensors,"Tahir A., Munawar H.S., Akram J., Adil M., Ali S., Kouzani A.Z., Pervez Mahmud M.A.",10.3390/s22031147,Article,9.5853223801,6.8900203705
4,9,,Comparison of DEM Super-Resolution Methods Based on Interpolation and Neural Networks,DEM; Neural network; Super-resolution process; Terrain features,"High-resolution digital elevation models (DEMs) play a critical role in geospatial databases, which can be applied to many terrain-related studies such as facility siting, hydrological analysis, and urban design. However, due to the limitation of precision of equipment, there are big gaps to collect high-resolution DEM data. A practical idea is to recover high-resolution DEMs from easily obtained low-resolution DEMs, and this process is termed DEM super-resolution (SR). However, traditional DEM SR methods (e.g., bicubic interpolation) tend to over-smooth high-frequency regions on account of the operation of averaging local variations. With the recent development of machine learning, image SR methods have made great progress. Nevertheless, due to the complexity of terrain characters (e.g., peak and valley) and the huge difference between elevation field and image RGB (Red, Green, and Blue) value field, there are few works that apply image SR methods to the task of DEM SR. Therefore, this paper investigates the question of whether the state-of-the-art image SR methods are appropriate for DEM SR. More specifically, the traditional interpolation method and three excellent SR methods based on neural networks are chosen for comparison. Experimental results suggest that SRGAN (Super-Resolution with Generative Adversarial Network) presents the best performance on accuracy evaluation over a series of DEM SR experiments. © 2022 by the authors. Licensee MDPI, Basel, Switzerland.",2022,Sensors,"Zhang Y., Yu W.",10.3390/s22030745,Article,9.4421443939,6.8250417709
4,16,,SNLRUX++ for Building Extraction from High-Resolution Remote Sensing Images,Building extraction; convolution neural network; deep learning; high-resolution image; remote sensing,"Building extraction plays an important role in high-resolution remote sensing image processing, which can be used as the basis for urban planning and demographic analysis. In recent years, many powerful general semantic segmentation models have emerged, but these models often perform poorly when transferred to remote sensing images because of the characteristics of remote sensing images. To this end, we propose a new deep learning network called Selective Nonlocal ResUNeXt++ (SNLRUX++) for building extraction. First, the cascaded multiscale feature fusion is proposed to transform the high-performance image classification network ResNeXt into the segmentation network ResUNeXt++. Second, selective nonlocal operation is designed to establish long-range dependencies while avoiding introducing excessive noise and computational effort. Finally, multiscale prediction is applied as deep supervision to accelerate training and convergence, and improves prediction performance of objects at different scales. The experimental results on two different remote sensing image datasets show the effectiveness and generalization ability of the proposed method. © 2008-2012 IEEE.",2022,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"Lei Y., Yu J., Chan S., Wu W., Liu X.",10.1109/JSTARS.2021.3135705,Article,9.4253940582,6.8759431839000005
4,23,,DSA-Net: A novel deeply supervised attention-guided network for building change detection in high-resolution remote sensing images,Building change detection; CLA-Con-SAM; Deep learning; Deep supervision; DSA-Net,"Building change detection (BCD) plays a crucial role in urban planning and development and has received extensive attention. However, existing deep learning-based change detection methods suffer from limited accuracy, mainly due to the information loss and inadequate capability in feature extraction. To overcome these shortcomings, we propose a novel deeply supervised attention-guided network (DSA-Net) for BCD tasks in high-resolution images. In the DSA-Net, we innovatively introduce a spatial attention mechanism-guided cross-layer addition and skip-connection (CLA-Con-SAM) module to aggregate multi-level contextual information, weaken the heterogeneity between raw image features and difference features, and direct the network's attention to changed regions. We also introduce an atrous spatial pyramid pooling (ASPP) module to extract multi-scale features. To further improve detection performance, we implement a new deep supervision module to enhance the ability of middle layers to extract more distinctive features. We conduct quantitative and qualitative experiments on the two publicly available datasets, i.e., the LEVIR-CD and the WHU Building datasets. Compared with the competing methods, the proposed DSA-Net achieves the best performance in all evaluation metrics. The efficiency analysis reveals that the proposed DSA-Net achieves a great balance between BCD performance and complexity/efficiency, with faster convergence and higher robustness. © 2021 The Authors",2021,International Journal of Applied Earth Observation and Geoinformation,"Ding Q., Shao Z., Huang X., Altan O.",10.1016/j.jag.2021.102591,Article,9.4062509537,6.8711285591
4,37,2.0,GAN-Based Semisupervised Scene Classification of Remote Sensing Image,Gating unit; generative adversarial nets (GANs); remote sensing image (RSI); self-attention gating (SAG) module; semisupervised image classification,"With the advent of a large number of remote sensing images (RSIs), scene classification of RSI is widely applied to many fields such as urban planning, natural disaster detection, and environmental monitoring. Compared with the natural image field, the lack of labeled RSI is a bottleneck of supervised scene classification methods based on deep learning. Meanwhile, unsupervised scene classification is difficult to meet actual needs. To this end, we propose a novel semisupervised scene classification method for RSI using generative adversarial nets (GANs), in which a gating unit, a self-attention gating (SAG) module, and a pretrained Inception V3 branch are introduced into discriminative network to enhance the feature representation capability for facilitating semisupervised classification. To be specific, the gating unit aims to learn the weights of each feature map and capture the dependence relationship between features. The SAG module aims to capture a long-range dependence for adaptively focusing on important scene regions. The Inception V3 branch aims to extract the high-level semantic representation of input images and further enhance the discriminant ability by gating unit and SAG module. Furthermore, a new optimization term is incorporated into the generator loss to indirectly drive discriminator to correctly classify scene images. To verify the effectiveness of the proposed method, extensive experimental results on UC Merced and EuroSAT data sets demonstrate that the method surpasses most of the state-of-the-art semisupervised image classification methods significantly, especially when only few samples are tagged. © 2004-2012 IEEE.",2021,IEEE Geoscience and Remote Sensing Letters,"Guo D., Xia Y., Luo X.",10.1109/LGRS.2020.3014108,Article,9.4263458252,6.8585152626
4,46,,GeoAI in terrain analysis: Enabling multi-source deep learning and data fusion for natural feature detection,Data enrichment; Deep Learning; GeoAI; Multi-source data fusion; Object detection,"In this paper we report on a new GeoAI research method which enables deep machine learning from multi-source geospatial data for natural feature detection. In particular, a multi-source, deep learning-based object detection pipeline was developed. This pipeline introduces three new features: First, strategies of both data-level fusion (i.e., channel expansion on convolutional neural networks) and feature-level fusion were integrated into the object detection model to allow simultaneous machine learning from multi-source data, including remote sensing imagery and Digital Elevation Model (DEM) data. Second, a new data fusion strategy was developed to blend DEM data and its derivatives to create a new, fused data source with enriched information content and image features. The model has also enabled deep learning by combining both the proposed data fusion and feature-level fusion strategies to yield a much-improved detection result. Third, two different sets of data augmentation techniques were applied to the multi-source training data to further improve the model performance. A series of experiments were conducted to verify the effectiveness of the proposed strategies in multi-source deep learning. © 2021 Elsevier Ltd",2021,"Computers, Environment and Urban Systems","Wang S., Li W.",10.1016/j.compenvurbsys.2021.101715,Article,9.53403759,6.8278551102
4,66,,Multi-class strategies for joint building footprint and road detection in remote sensing,Binary semantic segmentation; Building detection; Convolutional neural networks; Deep learning; Multi-class semantic segmentation; Multi-task semantic segmentation; Remote sensing; Road detection; Sentinel-1; Sentinel-2,"Building footprints and road networks are important inputs for a great deal of services. For instance, building maps are useful for urban planning, whereas road maps are essential for disaster response services. Traditionally, building and road maps are manually generated by remote sensing experts or land surveying, occasionally assisted by semi-automatic tools. In the last decade, deep learning-based approaches have demonstrated their capabilities to extract these elements automatically and accurately from remote sensing imagery. The building footprint and road network detection problem can be considered a multi-class semantic segmentation task, that is, a single model performs a pixel-wise classification on multiple classes, optimizing the overall performance. However, depending on the spatial resolution of the imagery used, both classes may coexist within the same pixel, drastically reducing their separability. In this regard, binary decomposition techniques, which have been widely studied in the machine learning literature, are proved useful for addressing multiclass problems. Accordingly, the multi-class problem can be split into multiple binary semantic segmentation sub-problems, specializing different models for each class. Nevertheless, in these cases, an aggregation step is required to obtain the final output labels. Additionally, other novel approaches, such as multi-task learning, may come in handy to further increase the performance of the binary semantic segmentation models. Since there is no certainty as to which strategy should be carried out to accurately tackle a multi-class remote sensing semantic segmentation problem, this paper performs an in-depth study to shed light on the issue. For this purpose, open-access Sentinel-1 and Sentinel-2 imagery (at 10 m) are considered for extracting buildings and roads, making use of the well-known U-Net convolutional neural network. It is worth stressing that building and road classes may coexist within the same pixel when working at such a low spatial resolution, setting a challenging problem scheme. Accordingly, a robust experimental study is developed to assess the benefits of the decomposition strategies and their combination with a multi-task learning scheme. The obtained results demonstrate that decomposing the considered multi-class remote sensing semantic segmentation problem into multiple binary ones using a One-vs-All binary decomposition technique leads to better results than the standard direct multi-class approach. Additionally, the benefits of using a multi-task learning scheme for pushing the performance of binary segmentation models are also shown. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",2021,Applied Sciences (Switzerland),"Ayala C., Aranda C., Galar M.",10.3390/app11188340,Article,9.4151000977,6.8969120979
4,72,,A novel approach for change detection analysis of land cover from multispectral FCC optical image using machine learning,Change Detection; Land Use Land Classification; Machine Learning; Multispectral; Remote Sensing,"Land covers refers to the physical land types such as vegetation, water, urban area, roads, and many more according to the geographical region. With the rapid change in land-use patterns, the land covers are varying drastically which requires immediate attention to have an eye at the impact of the land use planning and environmental changes is on the right track, or it needs to be modified. Hence utilizing the advancements in remote sensing technology for analyzing Land Use Land Cover (L ULC) classification maps using satellite images of the geographical region plays an important role in analyzing the present scenario of land covers. This paper proposes a novel approach for change detection analysis using the classification maps generated using Machine Learning (ML) classification techniques on a particular geographical region surrounding Nirma University, Ahmedabad, India. The highest classification accuracy of 98.48% was achieved using Support Vector Machine (SVM) for Near Infrared (NIR) band False Colour Composite (FCC) image obtained from Sentinel 2. © 2021 IEEE.",2021,"2nd International Conference on Range Technology, ICORT 2021","Patel K., Jain M., Patel M.I., Gajjar R.",10.1109/ICORT52730.2021.9582057,Conference Paper,9.9794282913,6.9321265221
4,73,1.0,Automatic detection of impervious surfaces from remotely sensed data using deep learning,Deep learning; Google Earth Engine; Impervious surfaces; Machine learning; Remote sensing,"The large scale quantification of impervious surfaces provides valuable information for urban planning and socioeconomic development. Remote sensing and GIS techniques provide spatial and temporal information of land surfaces and are widely used for modeling impervious surfaces. Traditionally, these surfaces are predicted by computing statistical indices derived from different bands available in remotely sensed data, such as the Landsat and Sentinel series. More recently, researchers have explored classification and regression techniques to model impervious surfaces. However, these modeling efforts are limited due to lack of labeled data for training and evaluation. This in turn requires significant effort for manual labeling of data and visual interpretation of results. In this paper, we train deep learning neural networks using TensorFlow to predict impervious surfaces from Landsat 8 images. We used OpenStreetMap (OSM), a crowd-sourced map of the world with manually interpreted impervious surfaces such as roads and buildings, to programmatically generate large amounts of training and evaluation data, thus overcoming the need for manual labeling. We conducted extensive experimentation to compare the performance of different deep learning neural network architectures, optimization methods, and the set of features used to train the networks. The four model configurations labeled U-Net_SGD_Bands, U-Net_Adam_Bands, U-Net_Adam_Bands+SI, and VGG-19_Adam_Bands+SI resulted in a root mean squared error (RMSE) of 0.1582, 0.1358, 0.1375, and 0.1582 and an accuracy of 90.87%, 92.28%, 92.46%, and 90.11%, respectively, on the test set. The U-Net_Adam_Bands+SI Model, similar to the others mentioned above, is a deep learning neural network that combines Landsat 8 bands with statistical indices. This model performs the best among all four on statistical accuracy and produces qualitatively sharper and brighter predictions of impervious surfaces as compared to the other models. © 2021 by the authors.",2021,Remote Sensing,"Parekh J.R., Poortinga A., Bhandari B., Mayer T., Saah D., Chishtie F.",10.3390/rs13163166,Article,9.819926261900001,6.8920173645
4,74,,Cloud detection using an ensemble of pixel-based machine learning models incorporating unsupervised classification,Cloud detection; Ensemble approaches; HOT; Landsat 8; Machine learning; NDSI; NDVI; Self organizing maps (SOM); Whitness,"Remote sensing imagery, such as that provided by the United States Geological Survey (USGS) Landsat satellites, has been widely used to study environmental protection, hazard analysis, and urban planning for decades. Clouds are a constant challenge for such imagery and, if not handled correctly, can cause a variety of issues for a wide range of remote sensing analyses. Typically, cloud mask algorithms use the entire image; in this study we present an ensemble of different pixel-based approaches to cloud pixel modeling. Based on four training subsets with a selection of different input features, 12 machine learning models were created. We evaluated these models using the cropped LC8-Biome cloud validation dataset. As a comparison, Fmask was also applied to the cropped scene Biome dataset. One goal of this research is to explore a machine learning modeling approach that uses as small a training data sample as possible but still provides an accurate model. Overall, the model trained on the sample subset (1.3% of the total training samples) that includes unsupervised Self-Organizing Map classification results as an input feature has the best performance. The approach achieves 98.57% overall accuracy, 1.18% cloud omission error, and 0.93% cloud commission error on the 88 cropped test images. By comparison to Fmask 4.0, this model improves the accuracy by 10.12% and reduces the cloud omission error by 6.39%. Furthermore, using an additional eight independent validation images that were not sampled in model training, the model trained on the second largest subset with an additional five features has the highest overall accuracy at 86.35%, with 12.48% cloud omission error and 7.96% cloud commission error. This model’s overall correctness increased by 3.26%, and the cloud omission error decreased by 1.28% compared to Fmask 4.0. The machine learning cloud classification models discussed in this paper could achieve very good performance utilizing only a small portion of the total training pixels available. We showed that a pixel-based cloud classification model, and that as each scene obviously has unique spectral characteristics, and having a small portion of example pixels from each of the sub-regions in a scene can improve the model accuracy significantly. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",2021,Remote Sensing,"Yu X., Lary D.J.",10.3390/rs13163289,Article,9.9029722214,6.9216961861
4,87,,Comparing three machine learning techniques for building extraction from a digital surface model,Automated building extraction; Deep learning (DL); Digital surface model (DSM); Extreme learning machine (ELM); Fully convolutional network (FCN); Histogram of oriented gradients (HOG); Machine learning (ML); Support vector machine (SVM),"Automatic building extraction from high-resolution remotely sensed data is a major area of interest for an extensive range of fields (e.g., urban planning, environmental risk management) but challenging due to urban morphology complexity. Among the different methods proposed, the approaches based on supervised machine learning (ML) achieve the best results. This paper aims to investigate building footprint extraction using only high-resolution raster digital surface model (DSM) data by comparing the performance of three different popular supervised ML models on a benchmark dataset. The first two methods rely on a histogram of oriented gradients (HOG) feature descriptor and a classical ML (support vector machine (SVM)) or a shallow neural network (extreme learning machine (ELM)) classifier, and the third model is a fully convolutional network (FCN) based on deep learning with transfer learning. Used data were obtained from the International Society for Photogrammetry and Remote Sensing (ISPRS) and cover the urban areas of Vaihingen an der Enz, Potsdam, and Toronto. The results indicated that performances of models based on shallow ML (feature extraction and classifier training) are affected by the urban context investigated (F1 scores from 0.49 to 0.81), whereas the FCN-based model proved to be the most robust and best-performing method for building extraction from a high-resolution raster DSM (F1 scores from 0.80 to 0.86). © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",2021,Applied Sciences (Switzerland),"Notarangelo N.M., Mazzariello A., Albano R., Sole A.",10.3390/app11136072,Article,9.5793132782,6.8844351768
4,89,1.0,Building Roof Superstructures Classification from Imbalanced and Low Density Airborne LiDAR Point Cloud,3D classification; imbalanced data; light detection and ranging (LiDAR); Low-density point cloud; roof superstructures,"Light Detection and Ranging (LiDAR), an active remote sensing technology, is becoming an essential tool for geoinformation extraction and urban planning. Airborne Laser Scanning (ALS) point clouds segmentation and accurate classification are challenging and crucial to produce different geo-information products like three-dimensional (3D) city designs. This paper introduces an effective data-driven approach to build roof superstructures classification for airborne LiDAR point clouds with very low density and imbalanced classes, covering an urban area. Notably, it focuses on building roof superstructures (especially dormers and chimneys) and mitigating nonplanar objects' problems. Also, the imbalanced class problem of LiDAR data, to the best of our knowledge, is not yet addressed in the literature; it is considered in this study. The major advantage of the proposed approach is using only raw data without assumptions on the distribution underlying data. The main methodological novelties of this work are summarized in the following key elements. (i) At first, an adapted connected component analysis for 3D points cloud is proposed. (ii) Twelve geometry-based features are extracted for each component. (iii) A Support Vector Machine (SVM)-driven procedure is applied to classify the 3D components. (iv) Furthermore, a new component size-based sampling (CSBS) method is proposed to treat the imbalanced data problem and has been compared with several existing resampling strategies. In this study, components are classified into five classes: shed and gable dormers, chimneys, ground, and others. The results of this investigation show the satisfying classification performance of the proposed approach. Results also showed that the proposed approach outperformed machine learning methods, including SVM, Random Forest, Decision Tree, and Adaboost. © 2001-2012 IEEE.",2021,IEEE Sensors Journal,"Aissou B.E., Aissa A.B., Dairi A., Harrou F., Wichmann A., Kada M.",10.1109/JSEN.2021.3073535,Article,9.4246015549,6.8989453316
4,92,,"Assessment of combining convolutional neural networks and object based image analysis to land cover classification using sentinel 2 satellite imagery (Tenes Region, Algeria)",Convolutional neural networks (CNN); Land cover; Machine learning; Object based image analysis (OBIA); Sentinel-2; Ténès,"Land cover maps can provide valuable information for various applications, such as territorial monitoring, environmental protection, urban planning and climate change prevention. In this purpose, remote sensing based on image classification approaches undergoing a high revolution can be dedicated to land cover mapping tasks. Similarly, deep learning models are considerably applied in remote sensing applications; which can automatically learn features from large amounts of data. Prevalently, the Convolutional Neural Network (CNN), have been increasingly performed in image classification. The aim of this study is to apply a new approach to analyse land cover, and extract its features. Experiments carried out on a coastal town located in north-western Algeria (Ténès region). The study area is chosen because of its importance as a part of the national strategy to combat natural hazards, specifically floods. As well as, a simple CNN model with two hidden layers was constructed, combined with an Object-Based Image Analysis (OBIA). In this regard, a Sentinel-2 image was used, to perform the classification, using spectral index combinations. Furthermore, to compare the performance of the proposed approach, an OBIA based on machines learning algorithms mainly Random Forest (RF) and Support Vector Machine (SVM), was provided. Results of accuracy assessment of classification showed good values in terms of Overall accuracy and Kappa Index, which reach to 93.1% and 0.91, respectively. As a comparison, CNN-OBIA approach outperformed OBIA based on RF algorithm. Therefore, Final land cover maps can be used as a support tool in regional and national decisions. © 2021 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives. All rights reserved.",2021,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","Zaabar N., Niculescu S., Mihoubi M.K.",10.5194/isprs-archives-XLIII-B3-2021-383-2021,Conference Paper,9.8809900284,6.9079732895
4,93,,3D urban change detection with point cloud siamese networks,3D Change Detection; Deep Learning; Kernel Point Convolution; Point Clouds; Siamese Network; Urban Monitoring,"As the majority of the earth population is living in urban environments, cities are continuously evolving and efficient monitoring tools are needed to retrieve and classify their evolution. In this context, analysing changes between two dates is a crucial point. In urban environments, most changes occur along the vertical axis (with new construction or demolition of buildings) and the use of 3D data is therefore mandatory. Among them, LiDAR constitutes a valuable source of information. However, With the difficulty of processing sparse and unordered 3D point clouds, most of existing methods start by rasterizing point clouds (for example to Digital Surface Models) before using more conventional image processing tools. This implies a significant loss of information. Among existing studies dealing directly with point clouds, and to the best of our knowledge, no deep neural network-based method has been explored yet. Thus, in order to fill this gap and to test the ability of deep methods to deal with change detection and characterization of 3D point clouds, we propose a Siamese network with Kernel Point Convolution inspired by Siamese architectures that have already shown their performances on change detection in 2D images and on KPConv network which achieves high-quality results for semantic segmentation of raw 3D point clouds. We show quantitatively and qualitatively that our method outperforms by more than 25% (in terms of average Intersection over Union for classes of change) existing machine learning methods based on hand-crafted features. © 2021 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives. All rights reserved.",2021,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","De Gélis I., Lefèvre S., Corpetti T.",10.5194/isprs-archives-XLIII-B3-2021-879-2021,Conference Paper,9.4039316177,6.8622941971
4,96,2.0,Assessment of deep learning techniques for land use land cover classification in southern new caledonia,Deep learning: XGBoost; Land cover; Land use; Neo-channels; Neural network; New Caledonia; Remote sensing,"Land use (LU) and land cover (LC) are two complementary pieces of cartographic information used for urban planning and environmental monitoring. In the context of New Caledonia, a biodiversity hotspot, the availability of up-to-date LULC maps is essential to monitor the impact of extreme events such as cyclones and human activities on the environment. With the democratization of satellite data and the development of high-performance deep learning techniques, it is possible to create these data automatically. This work aims at determining the best current deep learning configuration (pixel-wise vs. semantic labelling architectures, data augmentation, image prepos-sessing, … ), to perform LULC mapping in a complex, subtropical environment. For this purpose, a specific data set based on SPOT6 satellite data was created and made available for the scientific community as an LULC benchmark in a tropical, complex environment using five representative areas of New Caledonia labelled by a human operator: four used as training sets, and the fifth as a test set. Several architectures were trained and the resulting classification was compared with a state-of-the-art machine learning technique: XGboost. We also assessed the relevance of popular neo-channels derived from the raw observations in the context of deep learning. The deep learning approach showed comparable results to XGboost for LC detection and over-performed it on the LU detection task (61.45% vs. 51.56% of overall accuracy). Finally, adding LC classification output of the dedicated deep learning architecture to the raw channels input significantly improved the overall accuracy of the deep learning LU classification task (63.61% of overall accuracy). All the data used in this study are available on line for the remote sensing community and for assessing other LULC detection techniques. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",2021,Remote Sensing,"Rousset G., Despinoy M., Schindler K., Mangeas M.",10.3390/rs13122257,Article,9.8515024185,6.9103755951
4,106,,"Machine learning approach to extract building footprint from high-resolution images: The case study of Makkah, Saudi Arabia",extract building footprint; high-resolution images; machine learning; Makkah,"Extracting and identifying building boundaries from high-resolution images have been a hot topic in the field of remote sensing for years. Various methods including geometric, radiometric, object based and edge detection were previously deliberated and implemented in different studies in the context of building extraction. Nevertheless, the reliability of extraction process is mainly subject to user intervention. The current study proposes a new automatic morphology-based approach for extracting buildings using high-resolution satellite images of Al-Hudaybiyah region in the city of Makkah as a case study. The proposed technique integrates the support vector machine for extracting buildings that have bright and dark roofs. The appropriateness of this method has been examined by means of various indicators for example completeness, correctness and quality. Preliminary findings will illustrate the precision and accuracy of the used machine learning algorithm. Research results can provide a generic indicator to assist the planning authorities in achieving better urban planning processes taking into account all potential environmental, social and urban demands and requirements. © 2021 The Author(s) 2021. Published by Oxford University Press.",2021,International Journal of Low-Carbon Technologies,"Faisal K., Imam A., Majrashi A., Hegazy I.",10.1093/ijlct/ctaa099,Article,9.5063152313,6.8975768089
4,118,1.0,Machine Learning Methods for Road Edge Detection on Fused Airborne Hyperspectral and LIDAR Data,data fusion; Hyperspectral; LiDAR; machine learning; remote sensing; road edge detection,"In the last decades, remote sensing sensors, such as hyperspectral systems or LiDAR scanners, have been used for urban mapping. However, an analysis in the urban environment is very complex in applications, e.g., road detection, city management, and urban planning. One of the important urban features is the detection of the road edges. In this study, an approach on multisensory hyperspectral and LiDAR data fusion (HL-Fusion) is introduced for road edge detection using different machine learning algorithms, such as Support Vector Machines, Random Forests, and Convolutional Neural Networks. The first results show that the Random Forest algorithm outperformed in the experiments on the study area at Oslo's surroundings in Norway. This study opens a window for further investigation on machine learning algorithms and a better understanding of HL-Fusion capabilities. © 2021 IEEE.",2021,"Workshop on Hyperspectral Image and Signal Processing, Evolution in Remote Sensing","Senchuri R., Kuras A., Burud I.",10.1109/WHISPERS52202.2021.9484007,Conference Paper,9.4069461823,6.8894996643
4,128,1.0,"A study on vehicle detection through aerial images: Various challenges, issues and applications",Aerial Images; Machine Learning; Security; Vehicle,"Nowadays vehicle detection and counting at the border of countries, as well as states/cities, has become popular through aerial images because of security concerns. It will play a vital role to reduce the various crimes i.e. (children kidnapping, drug/alcohol smuggling, traffic misconduct, weapons smuggling, sexual misconduct and mission of country-related crime, etc.) at the border of the cities as well as countries. Vehicle detection and counting have various other applications like traffic management, parking allotment, tracking the rescue vehicle in hill areas, digital watermarking, vehicle tracking at the toll plaza and urban planning, etc. However, vehicle detection and counting task are very challenging and difficult because of the complex background, the small size of the vehicle, other similar visual appearance objects, distance, etc. Till now, traditional methodology introduced several robust algorithms which has limitations while extracting the features from aerial images. Recently, deep learning-based algorithms introduced and the outcomes of these algorithms are robust for such kind of applications in the area of computer vision. But accuracy of these algorithms is not optimized in aerial images because the deep learning algorithm required a huge amount of data to train the machine and the size of the object in aerial images is also too small. All these factors affecting the efficiency of the real-time device. This paper provides a brief description of traditional algorithms as well as machine learning and deep learning concepts to identifying the object through aerial images. The study has shown the comprehensive analysis of benchmark datasets and their parameters and corresponding challenges used by researchers and scientists in the area of object detection/tracking through aerial images. © 2021 IEEE.",2021,"Proceedings - IEEE 2021 International Conference on Computing, Communication, and Intelligent Systems, ICCCIS 2021","Kumar S., Rajan E.G., Rani S.",10.1109/ICCCIS51004.2021.9397116,Conference Paper,9.2946386337,6.8558034897
4,129,3.0,Fusion of airborne lidar point clouds and aerial images for heterogeneous land-use urban mapping,Bootstrap aggregation; K-fold cross-validation; LiDAR classification; LiDAR-aerial geo-registration; LiDAR-aerial integration; Maximum likelihood; Neural networks; Supervised machine learning; Support vector machines; Urban land-use,"The World Health Organization has reported that the number of worldwide urban residents is expected to reach 70% of the total world population by 2050. In the face of challenges brought about by the demographic transition, there is an urgent need to improve the accuracy of urban land-use mappings to more efficiently inform about urban planning processes. Decision-makers rely on accurate urban mappings to properly assess current plans and to develop new ones. This study investigates the effects of including conventional spectral signatures acquired by different sensors on the classification of airborne LiDAR (Light Detection and Ranging) point clouds using multiple feature spaces. The proposed method applied three machine learning algorithms—ML (Maximum Likelihood), SVM (Support Vector Machines), and MLP (Multilayer Perceptron Neural Network)—to classify LiDAR point clouds of a residential urban area after being geo-registered to aerial photos. The overall classification accuracy passed 97%, with height as the only geometric feature in the classifying space. Misclassifications occurred among different classes due to independent acquisition of aerial and LiDAR data as well as shadow and orthorectification problems from aerial images. Nevertheless, the outcomes are promising as they surpassed those achieved with large geometric feature spaces and are encouraging since the approach is computationally reasonable and integrates radiometric properties from affordable sensors. © 2021 by the authors. Licensee MDPI, Basel, Switzerland.",2021,Remote Sensing,"Megahed Y., Shaker A., Yan W.Y.",10.3390/rs13040814,Article,9.5106458664,6.9043588638
4,137,2.0,Use of deep learning models in street-level images to classify one-story unreinforced masonry buildings based on roof diaphragms,Convolutional neural networks; Deep learning; Diaphragm; Risk assessment; Seismic risk; Unreinforced masonry; Urban planning,"In this paper, we explore the potential of convolutional neural networks to classify street-level imagery of one-story unreinforced masonry buildings (MURs) according to the flexibility of the roof diaphragm (rigid or flexible). This information is critical for vulnerability studies, disaster risk assessments, disaster management strategies, etc., and is of great relevance in cities where unreinforced masonry is the most common building typology or where the majority of the population resides in such buildings. Our contribution could be useful for local governments of cities in developing countries seeking to significantly reduce the number of deaths caused by disasters. Our research results indicate that VGG19 is the convolutional neural network architecture with the best performance, with an accuracy of 0.80, a precision of 0.88, and a recall of 0.84. The results are encouraging and could be used to reduce the amount of resources (both human and economic) for the development of detailed exposure models for unreinforced masonry buildings. © 2020 Elsevier Ltd",2021,Building and Environment,"Rueda-Plata D., González D., Acevedo A.B., Duque J.C., Ramos-Pollán R.",10.1016/j.buildenv.2020.107517,Article,9.4925069809,6.8512134552
4,141,10.0,Deep learning-based multi-feature semantic segmentation in building extraction from images of UAV photogrammetry,,"Building information is an essential part of geographic information system (GIS) applications in urban planning and management. However, it changes rapidly with economic growth. Unmanned aerial vehicles (UAV)-based photogrammetry works well in this situation with its advantages of quick and high-resolution data updating. In this paper, in order to improve building extraction accuracy in complex areas where buildings are characterized by various patterns, complex structures, and unique styles, we present a framework which applies deep learning (DL) semantic segmentation to UAV images with digital surface model (DSM) and visible-band difference vegetation index (VDVI). The results show that extraction accuracy improves. The combination of red, green, blue (RGB) and VDVI bands (RGBVI) can effectively distinguish the building area and vegetation. The application of RGB with DSM bands (RGBD) helps separate buildings from ground objects. The combination of RGB, DSM, and VDVI bands (RGBDVI) can identify small buildings which are usually not high and covered partly by tree branches. The proposed method is further applied to an open standard dataset to evaluate its robustness and results indicate an increased overall accuracy from RGB only (93%) to RGBD (97%). © 2020 Informa UK Limited, trading as Taylor & Francis Group.",2021,International Journal of Remote Sensing,"Boonpook W., Tan Y., Xu B.",10.1080/01431161.2020.1788742,Article,9.4015388489,6.8872241974
4,144,,Transfer Learning Models for Land Cover and Land Use Classification in Remote Sensing Image,,"Land Cover or Land Use (LCLU) classification is an important, challenging problem in remote sensing (RS) images. RS image classification is a recent technology used to extract hidden information from remotely sensed images in the observed earth environment. This classification is essential for sustainable development in agricultural decisions and urban planning using deep learning (DL) methods. DL gets more attention for accuracy and performance improvements in large datasets. This paper is aimed to apply one of the DL methods called transfer learning (TL). TL is the recent research problem in machine learning and DL approaches for image classification. DL consumes much time for training when starting from scratch. This problem could be overcome in the TL modeling technique, which uses pre-trained models to build deep TL models efficiently. We applied the TL model using bottleneck feature extraction from the pre-trained models: InceptionV3, Resnet50V2, and VGG19 to LCLU classification in the UC Merced dataset. With these experiments, the TL model has been built the outdate performance of 92.46, 94.38, and 99.64 in Resnet50V2, InceptionV3, and VGG19, respectively. © 2021 The Author(s). Published with license by Taylor & Francis Group, LLC.",2021,Applied Artificial Intelligence,"Alem A., Kumar S.",10.1080/08839514.2021.2014192,Article,9.7649936676,6.9060354233000005
4,153,,Assessment of approaches for the extraction of building footprints from pléiades images,Backpropagation; Ensemble classifiers; Image classification; Machine learning; Maximum likelihood; Random forest; Support vector machines,"The Marina area represents an official new gateway of entry to Egypt and the development of infrastructure is proceeding rapidly in this region. The objective of this research is to obtain building data by means of automated extraction from Pléiades satellite images. This is due to the need for efficient mapping and updating of geodatabases for urban planning and touristic development. It compares the performance of random forest algorithm to other classifiers like maximum likelihood, support vector machines, and backpropagation neural networks over the well-organized buildings which appeared in the satellite images. Images were subsequently classified into two classes: buildings and non-buildings. In addition, basic morphological operations such as opening and closing were used to enhance the smoothness and connectedness of the classified imagery. The overall accuracy for random forest, maximum likelihood, support vector machines, and backpropagation were 97%, 95%, 93% and 92% respectively. It was found that random forest was the best option, followed by maximum likelihood, while the least effective was the backpropagation neural network. The completeness and correctness of the detected buildings were evaluated. Experiments confirmed that the four classification methods can effectively and accurately detect 100% of buildings from very high-resolution images. It is encouraged to use machine learning algorithms for object detection and extraction from very high-resolution images. © 2021 Author.",2021,Geomatics and Environmental Engineering,"Taha L.G.E.-D., Ibrahim R.E.",10.7494/geom.2021.15.4.101,Article,9.601146698,6.9170780182
4,161,3.0,Scale-Robust Deep-Supervision Network for Mapping Building Footprints from High-Resolution Remote Sensing Images,Building footprint extraction; convolutional neural network; deep learning; remote sensing image,"Building footprint information is one of the key factors for sustainable urban planning and environmental monitoring. Mapping building footprints from remote sensing images is an important and challenging task in the earth observation field. Over the years, convolutional neural networks have shown outstanding improvements in the building extraction field due to their ability to automatically extract hierarchical features and make building predictions. However, as buildings are various in different sizes, scenes, and roofing materials, it is hard to precisely depict buildings of varied sizes, especially in large areas (e.g., nationwide). To tackle these limitations, we propose a novel deep-supervision convolutional neural network (denoted as DS-Net) for extracting building footprints from high-resolution remote sensing images. In the proposed network, we applied deep supervision with an extra lightweight encoder, which enables the network to learn representative building features of different scales. Furthermore, a scale attention module is designed to aggregate multiscale features and generate the final building prediction. Experiments on two publicly available building datasets, including the WHU Building Dataset and the Massachusetts Building Dataset, show the effectiveness of the proposed method. With only a 0.22-M increment of parameters compared with U-Net, the proposed DS-Net achieved an IoU of 90.4% on the WHU Building Dataset and 73.8% on the Massachusetts Dataset. DS-Net also outperforms the state-of-the-art building extraction methods on the two datasets, indicating the effectiveness of the proposed deep supervision and scale attention. © 2008-2012 IEEE.",2021,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"Guo H., Su X., Tang S., Du B., Zhang L.",10.1109/JSTARS.2021.3109237,Article,9.4401359558,6.8785638809
4,162,2.0,Urban tree species classification using UAV-based multi-sensor data fusion and machine learning,Random Forest; Remote Sensing; Support Vector Machine; Tree Species Detection,"Urban tree species classification is a challenging task due to spectral and spatial diversity within an urban environment. Unmanned aerial vehicle (UAV) platforms and small-sensor technology are rapidly evolving, presenting the opportunity for a comprehensive multi-sensor remote sensing approach for urban tree classification. The objectives of this paper were to develop a multi-sensor data fusion technique for urban tree species classification with limited training samples. To that end, UAV-based multispectral, hyperspectral, LiDAR, and thermal infrared imagery was collected over an urban study area to test the classification of 96 individual trees from seven species using a data fusion approach. Two supervised machine learning classifiers, Random Forest (RF) and Support Vector Machine (SVM), were investigated for their capacity to incorporate highly dimensional and diverse datasets from multiple sensors. When using hyperspectral-derived spectral features with RF, the fusion of all features extracted from all sensor types (spectral, LiDAR, thermal) achieved the highest overall classification accuracy (OA) of 83.3% and kappa of 0.80. Despite multispectral reflectance bands alone producing significantly lower OA of 55.2% compared to 70.2% with minimum noise fraction (MNF) transformed hyperspectral reflectance bands, the full dataset combination (spectral, LiDAR, thermal) with multispectral-derived spectral features achieved an OA of 81.3% and kappa of 0.77 using RF. Comparison of the features extracted from individual sensors for each species highlight the ability for each sensor to identify distinguishable characteristics between species to aid classification. The results demonstrate the potential for a high-resolution multi-sensor data fusion approach for classifying individual trees by species in a complex urban environment under limited sampling requirements. © 2021 Informa UK Limited, trading as Taylor & Francis Group.",2021,GIScience and Remote Sensing,"Hartling S., Sagan V., Maimaitijiang M.",10.1080/15481603.2021.1974275,Article,9.6959848404,6.8890390396
4,165,,Using 3-D Convolution and Multimodal Architecture for Earthquake Damage Detection Based on Satellite Imagery and Digital Urban Data,3-D convolution; earthquake damage detection; multimodal learning; satellite imagery; spatiotemporal data,"When a large earthquake occurs, it is quite important to quickly figure out the damage distribution of housing structures for disaster prevention measures. Currently, the information is confirmed manually by local public organizations, which takes a lot of time. Therefore, a method is required for gathering the information more swiftly and objectively. In this work, a novel method for detecting damage to single buildings from a set of multitemporal satellite images is developed by applying a recent machine learning approach. The damage detection system is designed as a deep learning model that uses multimodal data, consisting of optical satellite images and structural attributes. The proposed method achieved over 90% detection accuracy on damaged housing in the affected area of 2016 Kumamoto earthquake, Japan from satellite images taken by Pleiades as well as digital urban data. © 2008-2012 IEEE.",2021,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"Miyamoto T., Yamamoto Y.",10.1109/JSTARS.2021.3102701,Article,9.5726804733,6.8733839989
4,169,3.0,Satellite derived bathymetry using deep learning,Deep learning; Earth observation; Machine learning; Regression; Satellite-derived bathymetry,"Coastal development and urban planning are facing different issues including natural disasters and extreme storm events. The ability to track and forecast the evolution of the physical characteristics of coastal areas over time is an important factor in coastal development, risk mitigation and overall coastal zone management. Traditional bathymetry measurements are obtained using echo-sounding techniques which are considered expensive and not always possible due to various complexities. Remote sensing tools such as satellite imagery can be used to estimate bathymetry using incident wave signatures and inversion models such as physical models of waves. In this work, we present two novel approaches to bathymetry estimation using deep learning and we compare the two proposed methods in terms of accuracy, computational costs, and applicability to real data. We show that deep learning is capable of accurately estimating ocean depth in a variety of simulated cases which offers a new approach for bathymetry estimation and a novel application for deep learning. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.",2021,Machine Learning,"Al Najar M., Thoumyre G., Bergsma E.W.J., Almar R., Benshila R., Wilson D.G.",10.1007/s10994-021-05977-w,Article,9.6683778763,6.8569645882
4,183,4.0,UVid-Net: Enhanced Semantic Segmentation of UAV Aerial Videos by Embedding Temporal Information,Deep learning; semantic segmentation; transfer learning; U-Net; unmanned aerial vehicle (UAV) video,"Semantic segmentation of aerial videos has been extensively used for decision making in monitoring environmental changes, urban planning, and disaster management. The reliability of these decision support systems is dependent on the accuracy of the video semantic segmentation algorithms. The existing CNN-based video semantic segmentation methods have enhanced the image semantic segmentation methods by incorporating an additional module such as LSTM or optical flow for computing temporal dynamics of the video which is a computational overhead. The proposed research work modifies the CNN architecture by incorporating temporal information to improve the efficiency of video semantic segmentation. In this work, an enhanced encoder-decoder based CNN architecture (UVid-Net) is proposed for unmanned aerial vehicle (UAV) video semantic segmentation. The encoder of the proposed architecture embeds temporal information for temporally consistent labeling. The decoder is enhanced by introducing the feature-refiner module, which aids in accurate localization of the class labels. The proposed UVid-Net architecture for UAV video semantic segmentation is quantitatively evaluated on extended ManipalUAVid dataset. The performance metric mean Intersection over Union of 0.79 has been observed which is significantly greater than the other state-of-the-art algorithms. Further, the proposed work produced promising results even for the pretrained model of UVid-Net on urban street scene by fine tuning the final layer on UAV aerial videos. © 2008-2012 IEEE.",2021,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"Girisha S., Verma U., Manohara Pai M.M., Pai R.M.",10.1109/JSTARS.2021.3069909,Article,9.360909462,6.8668956757
4,188,11.0,Attention-Gate-Based Encoder-Decoder Network for Automatical Building Extraction,Attention gate (AG); building extraction; deep learning; fully convolutional networks (FCNs); semantic segmentation,"Rapidly developing remote sensing technology provides massive data for urban planning, mapping, and disaster management. As a carrier of human productive activities, buildings are essential to both urban dynamic monitoring and suburban construction inspection. Fully-convolutional-network-based methods have provided a paradigm for automatically extracting buildings from high-resolution imagery. However, high intraclass variance and complexity are two problems in building extraction. It is hard to identify different scales of buildings by using a single receptive field. For this purpose, in this article, we use the stable encoder- decoder architecture, combined with a grid-based attention gate and atrous spatial pyramid pooling module, to capture and restore features progressively and effectively. A modified ResNet50 encoder is also applied to extract features. The proposed method could learn gated features and distinguish buildings from complex surroundings such as trees. We evaluate our model on two building datasets, WHU aerial building dataset and our DB UAV rural building dataset. Experiments show that our model outperforms other five most recent models. The results also exhibit great potential for extracting buildings with different scales and validate the effectiveness of deep learning in practical scenarios. © 2008-2012 IEEE.",2021,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"Deng W., Shi Q., Li J.",10.1109/JSTARS.2021.3058097,Article,9.3810186386,6.882748127
4,226,4.0,Precise object detection using adversarially augmented local/global feature fusion,Data augmentation; Geospatial object detection; High spatial resolution (HSR) remote sensing imagery; Local/global feature fusion; Super resolution generative adversarial network,"Object detection, which aims at recognizing or locating the objects of interest in remote sensing imagery with high spatial resolutions (HSR), plays a significant role in many real-world scenarios, e.g., environment monitoring, urban planning, civil infrastructure construction, disaster rescuing, and geographic image retrieval. As a long-lasting challenging problem in both machine learning and geoinformatics communities, many approaches have been proposed to tackle it. However, previous methods always overlook the abundant information embedded in the HSR remote sensing images. The effectiveness of these methods, e.g., accuracy of detection, is therefore limited to some extent. To overcome the mentioned challenge, in this paper, we propose a novel two-phase deep framework, dubbed GLGOD-Net, to effectively detect meaningful objects in HSR images. GLGOD-Net firstly attempts to learn the enhanced deep representations from super-resolution image data. Fully utilizing the augmented image representations, GLGOD-Net then learns the fused representations into which both local and global latent features are implanted. Such fused representations learned by GLGOD-Net can be used to precisely detect different objects in remote sensing images. The proposed framework has been extensively tested on a real-world HSR image dataset for object detection and has been compared with several strong baselines. The remarkable experimental results validate the effectiveness of GLGOD-Net. The success of GLGOD-Net not only advances the cutting-edge of image data analytics, but also promotes the corresponding applicability of deep learning in remote sensing imagery. © 2020",2020,Engineering Applications of Artificial Intelligence,"Han X., He T., Ong Y.-S., Zhong Y.",10.1016/j.engappai.2020.103710,Article,9.4129552841,6.839597702
4,231,,A NOVEL SELF-TAUGHT LEARNING FRAMEWORK USING SPATIAL PYRAMID MATCHING for SCENE CLASSIFICATION,High Resolution Imagery; Remote Sensing; Scene Classification; Self-taught Learning; Spatial Pyramid Matching,"Remote sensing earth observation images have a wide range of applications in areas like urban planning, agriculture, environment monitoring, etc. While the industrial world benefits from availability of high resolution earth observation images since recent years, interpreting such images has become more challenging than ever. Among many machine learning based methods that have worked out successfully in remote sensing scene classification, spatial pyramid matching using sparse coding (ScSPM) is a classical model that has achieved promising classification accuracy on many benchmark data sets. ScSPM is a three-stage algorithm, composed of dictionary learning, sparse representation and classification. It is generally believed that in the dictionary learning stage, although unsupervised, one should use the same data set as classification stage to get good results. However, recent studies in transfer learning suggest that it might be a better strategy to train the dictionary on a larger data set different from the one to classify. In our work, we propose an algorithm that combines ScSPM with self-taught learning, a transfer learning framework that trains a dictionary on an unlabeled data set and uses it for multiple classification tasks. In the experiments, we learn the dictionary on Caltech-101 data set, and classify two remote sensing scene image data sets: UC Merced LandUse data set and Changping data set. Experimental results show that the classification accuracy of proposed method is compatible to that of ScSPM. Our work thus provides a new way to reduce resource cost in learning a remote sensing scene image classifier. © 2020 International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives.",2020,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","Yang Y., Zhu D., Ren F., Cheng C.",10.5194/isprs-archives-XLIII-B2-2020-725-2020,Conference Paper,9.6744613647,6.8901157379
4,234,6.0,A generalized multi-task learning approach to stereo DSM filtering in urban areas,3D city models; Deep learning; Multi-task learning; Roof type segmentation; Stereo DSM filtering,"City models and height maps of urban areas serve as a valuable data source for numerous applications, such as disaster management or city planning. While this information is not globally available, it can be substituted by digital surface models (DSMs), automatically produced from inexpensive satellite imagery. However, stereo DSMs often suffer from noise and blur. Furthermore, they are heavily distorted by vegetation, which is of lesser relevance for most applications. Such basic models can be filtered by convolutional neural networks (CNNs), trained on labels derived from digital elevation models (DEMs) and 3D city models, in order to obtain a refined DSM. We propose a modular multi-task learning concept that consolidates existing approaches into a generalized framework. Our encoder-decoder models with shared encoders and multiple task-specific decoders leverage roof type classification as a secondary task and multiple objectives including a conditional adversarial term. The contributing single-objective losses are automatically weighted in the final multi-task loss function based on learned uncertainty estimates. We evaluated the performance of specific instances of this family of network architectures. Our method consistently outperforms the state of the art on common data, both quantitatively and qualitatively, and generalizes well to a new dataset of an independent study area. © 2020 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)",2020,ISPRS Journal of Photogrammetry and Remote Sensing,"Liebel L., Bittner K., Körner M.",10.1016/j.isprsjprs.2020.03.005,Article,9.5313920975,6.8471393585
4,236,12.0,A robust segmentation framework for closely packed buildings from airborne LiDAR point clouds,,"Urban villages (UVs) are commonly found in many Asian cities. These villages contain many closely packed buildings constructed decades ago without proper urban planning. There is a need for those buildings to be identified and put into statistics. In this paper, we present a segmentation framework that invokes multiple machine learning techniques and point cloud/image processing algorithms to segment individual closely packed buildings from large urban scenes. The presented framework consists of two major segmentation processes. The framework first filters out the non-ground objects from the point cloud, then it classified them by using the Random Forest classifier to isolate buildings from the entire scene. After that, the building point clouds will be segmented based on several building attribute analysis methods. This is followed by using the Random Sample Consensus (RANSAC) plane filtering method to expand the space between two closely packed buildings, so that the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) clustering technique can be used to more accurately segment each individual building from the closely packed building areas. Two airborne Light Detection and Ranging (LiDAR) datasets collected in two different cities with some typical closely packed buildings were used to verify the proposed framework. The results show that the framework can effectively identify the closely packed buildings with unified structures from large airborne LiDAR datasets. The overall segmentation accuracy reaches 84% for the two datasets. The proposed framework can serve as a basis for analysis and segmentation of closely packed buildings with a more complicated structure. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.",2020,International Journal of Remote Sensing,"Wang X., Chan T.O., Liu K., Pan J., Luo M., Li W., Wei C.",10.1080/01431161.2020.1727053,Article,9.3932104111,6.8810739517
4,237,,Object detection of aerial image using mask-region convolutional neural network (mask R-CNN),,"The most fundamental task in remote sensing data processing and analysis is object detection. It plays an important role in classification and very useful for various applications such as forestry, urban planning, agriculture, land use and land cover mapping, etc. However, it has many challenges to find an appropriate method due to many variations in the appearance of the object in image. The object may have occlusion, illumination, viewpoint variation, shadow, etc. Many object detection method has been researched and developed. Recently, the development of various machine learning-based methods for object detection has been increasing. Among of them are methods based on artificial neural network, deep learning and its derivatives. In this research, object detection method of aerial image by using mask-region convolutional neural network (mask-R CNN) is developed. The result shows that this method gives a significant accuracy by increasing the image training and epoch time. © 2020 IOP Publishing Ltd. All rights reserved.",2020,IOP Conference Series: Earth and Environmental Science,"Musyarofah, Schmidt V., Kada M.",10.1088/1755-1315/500/1/012090,Conference Paper,9.5111713409,6.8623781204
4,252,22.0,Comparative assessment of machine learning methods for urban vegetation mapping using multitemporal Sentinel-1 imagery,Land-cover classification; Multitemporal; Sentinel-1; Speckle filtering; Synthetic aperture radar (SAR); Urban vegetation,"Mapping of green vegetation in urban areas using remote sensing techniques can be used as a tool for integrated spatial planning to deal with urban challenges. In this context, multitemporal (MT) synthetic aperture radar (SAR) data have not been equally investigated, as compared to optical satellite data. This research compared various machine learning methods using single-date and MT Sentinel-1 (S1) imagery. The research was focused on vegetation mapping in urban areas across Europe. Urban vegetation was classified using six classifiers-random forests (RF), support vector machine (SVM), extreme gradient boosting (XGB), multi-layer perceptron (MLP), AdaBoost. M1 (AB), and extreme learning machine (ELM). Whereas, SVM showed the best performance in the single-date image analysis, the MLP classifier yielded the highest overall accuracy in the MT classification scenario. Mean overall accuracy (OA) values for all machine learning methods increased from 57% to 77% with speckle filtering. Using MT SAR data, i.e., three and five S1 imagery, an additional increase in the OA of 8.59% and 13.66% occurred, respectively. Additionally, using three and five S1 imagery for classification, the F1 measure for forest and low vegetation land-cover class exceeded 90%. This research allowed us to confirm the possibility of MT C-band SAR imagery for urban vegetation mapping. © 2020 by the authors.",2020,Remote Sensing,"Gašparović M., Dobrinić D.",10.3390/rs12121952,Article,9.7939682007,6.8886890411
4,259,10.0,A locally-constrained YOLO framework for detecting small and densely-distributed building footprints,Building detection; deep learning; locally constrained; remote sensing; YOLO,"Building footprints are among the most predominant features in urban areas, and provide valuable information for urban planning, solar energy suitability analysis, etc. We aim to automatically and rapidly identify building footprints by leveraging deep learning techniques and the increased availability of remote sensing datasets at high spatial resolution. The task is computationally challenging due to the use of large training datasets and large number of parameters. In related work, You-Only-Look-Once (YOLO) is a state-of-the-art deep learning framework for object detection. However, YOLO is limited in its capacity to identify small objects that appear in groups, which is the case for building footprints. We propose a LOcally-COnstrained (LOCO) You-Only-Look-Once framework to detect small and densely-distributed building footprints. LOCO is a variant of YOLO. Its layer architecture is determined by the spatial characteristics of building footprints and it uses a constrained regression modeling to improve the robustness of building size predictions. We also present an invariant augmentation based voting scheme to further improve the precision in the prediction phase. Experiments show that LOCO can greatly improve the solution quality of building detection compared to related work. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",2020,International Journal of Geographical Information Science,"Xie Y., Cai J., Bhojwani R., Shekhar S., Knight J.",10.1080/13658816.2019.1624761,Article,9.4030780792,6.8743925095
4,282,2.0,Land cover classification based on machine learning using UAV multi-spectral images,Image classification; Machine learning; UAV remote sensing,"Land cover classification using UAV multi-spectral images is of great significance in precision agriculture, urban planning, land use and other fields. However, traditional remote sensing image classification methods cannot meet the classification accuracy requirements of UAV multi-spectral images. This paper aims to propose an object-based machine learning classification method to improve the land over classification accuracy of UAV multi-spectral images. The experimental area is a standard test field located in the Jilin Province of China. The experimental data was captured by a UAV equipped with a multi-spectral camera which includes four bands from 550 nm to 790 nm. First, the original images were preprocessed and the spectral curves of land cover were analyzed, thus four kinds of land cover with large differences were selected as categories. Then pixel-based, boosting-based and object-based machine learning methods were used for classification. The object-based classification method could make full use of the spatial and spectral information, and eliminate the noise problem caused by the high resolution of the UAV image to a certain extent. Finally, accuracy analysis using the verification image showed that the RF-O method achieved the highest classification accuracy of 92.2419%, and the kappa coefficient was 0.8904. All results indicate that the object-based machine learning classification method proposed in this paper is more suitable for the research of land cover classification, comparing with the traditional remote sensing image classification methods, and performs well on the land cover classification of UAV multi-spectral images. © 2020 SPIE.",2020,Proceedings of SPIE - The International Society for Optical Engineering,"Pan L., Gu L., Ren R., Yang S.",10.1117/12.2566128,Conference Paper,9.8150110245,6.887403965
4,289,4.0,A Self-Supervised Learning Framework for Road Centerline Extraction from High-Resolution Remote Sensing Images,High-resolution remote sensing image; one-class classifier; road centerline; road extraction; self-supervised learning,"Road extraction from the high-resolution remote sensing image is significant for the land planning, vehicle navigation, etc. The existing road extraction methods normally need many preprocessing and subsequent optimization steps. Therefore, an automatic road centerline extraction method based on the self-supervised learning framework for high-resolution remote sensing image is proposed. This proposed method does not need to manually select training samples and other optimization steps, such as the nonroad area removing. First, the positive sample selection method combining the spectral and shape features is proposed to extract the road sample. Then, the one-class classifier framework is introduced and the random forest positive unlabeled learning classifier is constructed to get the posterior probability of the pixel belonging to road. The shape feature and the posterior probability are combined to form the final road network in the object-oriented way. Finally, the road centerline is obtained through the tensor voting algorithm. In order to verify the effectiveness of the proposed algorithm, high-resolution remote sensing images and benchmark datasets are used to do experiments. The indexes of the completeness ratio, the correctness ratio, and the detection quality are used for the quantitative accuracy evaluation. Compared with the supervised, the unsupervised, and the one-class classification road extraction algorithms, this proposed algorithm achieves high accuracy and efficiency. For the deep learning method comparison, the deep learning method performs well in most cases especially in the complex urban area. However, the deep learning method needs a large number of samples and a long training time, and our self-supervised learning framework does not need the training samples. © 2008-2012 IEEE.",2020,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"Guo Q., Wang Z.",10.1109/JSTARS.2020.3014242,Article,9.3599033356,6.8489737511
4,301,1.0,Application of satellite image segmentation for urban planning optimization,Aerial image segmentation; Building detection; Machine learning,"This article presents research results of a convolution neural network for building detection on high-resolution aerial images of Planet database. Jaccard index was used for analysis of the quality of machine learning algorithm. This index of similarity compares results of algorithms with real masks. The masks were sliced on smaller parts together with images before training of developed model. The convolution neural network was launched on NVIDIA DGX-1 supercomputer, which was provided by AI-center of P.G Demidov Yaroslavl State University. The problem of building detection on satellite images can be put into practice for urban planning, building control, search of the best locations for outlets etc. © WCSE 2019. All rights reserved.",2020,"Proceedings of 2019 the 9th International Workshop on Computer Science and Engineering, WCSE 2019","Khryashchev V., Ivanovsky L., Ostrovskaya A., Semenov A.",,Conference Paper,9.5504169464,6.8779840469
4,312,11.0,Mapping Urban Trees Within Cadastral Parcels Using an Object-Based Convolutional Neural Network,Cadastral Parcel; Convolutional Neural Network; GEOBIA; Machine Learning; Urban Trees,"Urban trees offer significant benefits for improving the sustainability and liveability of cities, but its monitoring is a major challenge for urban planners. Remote-sensing based technologies can effectively detect, monitor and quantify urban tree coverage as an alternative to field-based measurements. Automatic extraction of urban land cover features with high accuracy is a challenging task and it demands artificial intelligence workflows for efficiency and thematic quality. In this context, the objective of this research is to map urban tree coverage per cadastral parcel of Sandy Bay, Hobart from very high-resolution aerial orthophoto and LiDAR data using an Object Based Convolution Neural Network (CNN) approach. Instead of manual preparation of a large number of required training samples, automatically classified Object based image analysis (OBIA) output is used as an input samples to train CNN method. Also, CNN output is further refined and segmented using OBIA to assess the accuracy. The result shows 93.2% overall accuracy for refined CNN classification. Similarly, the overlay of improved CNN output with cadastral parcel layer shows that 21.5% of the study area is covered by trees. This research demonstrates that the accuracy of image classification can be improved by using a combination of OBIA and CNN methods. Such a combined method can be used where manual preparation of training samples for CNN is not preferred. Also, our results indicate that the technique can be implemented to calculate parcel level statistics for urban tree coverage that provides meaningful metrics to guide urban planning and land management practices. © 2019 Authors.",2019,"ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences","Timilsina S., Sharma S.K., Aryal J.",10.5194/isprs-annals-IV-5-W2-111-2019,Conference Paper,9.7369623184,6.8774089813
4,314,,Urban Intelligent Navigator for Drone Using Convolutional Neural Network (CNN),Convolution neural networks; Faster R-CNN; Object detection; object recognition,"It is still a difficult and challenging task for a drone to maneuver autonomously at low altitude in the urban environments. This is due to the complexity of the urban environment and its unpredictability. Many researches have been carried out in the past decades until recent time, to find a way to solve this problem using powerful sensors such as laser rangefinder sensor, RGB-D camera, stereo vision system, LIDAR and computer vision methods. This paper is aimed to present an urban intelligent navigator for drone using CNN (convolutional neural network). The application of computer vision (object detection) is cheap and has low power consumption compared to other kinds of vision systems. The machine learning allows a drone to detect and recognize all the objects and obstacles on the roads, which can block drone's way. One thousand images were captured of six different street objects (tree, lamp, bump sign, free-smoking sign, no-horn sign, and roof-wall). Those images were used as a dataset to create a machine learning using Faster R-CNN (region convolutional neural network) method. Three machine-learning models were created using different parameters for each model. The controlled parameters are the initial learning rate and the batch-size. Only the third model could successfully detect and recognize all the objects at a specified location showing 98% accuracy. © 2019 IEEE.",2019,"2019 International Conference on Smart Applications, Communications and Networking, SmartNets 2019","Moteir I.G.M.I., Ismail K., Zawawi F.M., Azhar M.M.M.",10.1109/SmartNets48225.2019.9069781,Conference Paper,9.446516037,6.8283109665
4,320,17.0,Evaluation and comparison of eight machine learning models in land use/land cover mapping using Landsat 8 OLI: a case study of the northern region of Iran,Image classification; LULCC; Machine learning; R statistical packages; WEKA,"Land use land cover change mapping has been used for monitoring environmental changes as an essential factor to study on the earth’s surface land cover in the field of climate change phenomena such as floods and droughts. Remote sensing images have been suggested to present inexpensive and fine-scale data offering multi-temporal coverage. This tool is useful in the field of environmental monitoring, land-cover mapping, and urban planning. This study aims to evaluate eight machine learning algorithms for image classification implemented in WEKA and R programming language. Firstly, Landsat 8 OLI/TIRS Level-2 images based on eight machine learning techniques including Random Forest, Decision Table, DTNB, J48, Lazy IBK, Multilayer Perceptron, Non-Nested Generalized Exemplars (NN ge), and Simple Logistic are classified. Then, obtained results are compared in term of Overall Accuracy (OA), Mean Absolute Error (MAE), and Root Mean Squared Error (RMSE) for land use land cover mapping. Among the eight machine learning algorithms used for image classification based on the training and test dataset, NN ge classifier is ranked first with values of 100, 0, and 0 for Overall Accuracy, Mean Absolute Error and Root Mean Squared Error respectively. All machine learning algorithms had an Overall Accuracy of more than 99% for the training dataset. On the other hand, for the test dataset, J48 and DTNB algorithms had the worst performance with values of 88.1188 and 76.9802 respectively for the Overall Accuracy. © 2019, Springer Nature Switzerland AG.",2019,SN Applied Sciences,Jamali A.,10.1007/s42452-019-1527-8,Article,9.9402103424,6.8822231293
4,327,12.0,Aerial point cloud classification with deep learning and machine learning algorithms,Classification; Deep learning; Geometric features; Machine learning; Point cloud; Urban areas,"With recent advances in technology, 3D point clouds are getting more and more frequently requested and used, not only for visualization needs but also e.g. by public administrations for urban planning and management. 3D point clouds are also a very frequent source for generating 3D city models which became recently more available for many applications, such as urban development plans, energy evaluation, navigation, visibility analysis and numerous other GIS studies. While the main data sources remained the same (namely aerial photogrammetry and LiDAR), the way these city models are generated have been evolving towards automation with different approaches. As most of these approaches are based on point clouds with proper semantic classes, our aim is to classify aerial point clouds into meaningful semantic classes, e.g. ground level objects (GLO, including roads and pavements), vegetation, buildings' facades and buildings' roofs. In this study we tested and evaluated various machine learning algorithms for classification, including three deep learning algorithms and one machine learning algorithm. In the experiments, several hand-crafted geometric features depending on the dataset are used and, unconventionally, these geometric features are used also for deep learning. © 2019 E. Özdemir et al.",2019,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","Özdemir E., Remondino F., Golkar A.",10.5194/isprs-archives-XLII-4-W18-843-2019,Conference Paper,9.4392633438,6.8743515015
4,330,,THE POTENTIAL of BUILDING DETECTION from SAR and LIDAR USING DEEP LEARNING,building extraction; deep learning; feature extraction; lidar; orthophoto; synthetic aperture radar (SAR),"The introduction of airborne Synthetic Aperture Radar (SAR) approach has successfully addressed several challenges for mapping and surveying applications Unlike other conventional sensors, airborne SAR mapping approach offers practicality and significant cost savings for the nation minimizing the need for ground control points on the ground in addition to providing high-resolution, day-and-night, cloud coverage and weather independent images, which in turn provides faster turnaround times for creation of large area geospatial data. Up-to-date building map is necessary to guide the decision making in many fields to understand the urban dynamics such as in disaster management, population estimation, planning and many other applications. Whilst mapping and surveying work using airborne SAR have started to capture many interest among surveyors, professionals and practitioners abroad, Malaysia however is still lacking behind in term of the knowledge and the usage of this technology together with Deep Learning, Machine Learning approach especially in building extraction for topographic mapping and urban planning and development. Deep learning is a subset of the machine learning algorithm. Recently, Deep Learning has been proposed to solve traditional artificial intelligent problems. In order to develop a sustainable national geospatial infrastructure for years to come, the integration between airborne SAR and other sensors as such LIDAR is therefore essential in Malaysia and in high demand for urban planning and management. Thus, this paper reviews current techniques and future trends of multi-sources Remote Sensing for building extraction. © 2019 Z. Nordin et al.",2019,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","Nordin Z., Shafri H.Z.M., Abdullah A.F., Hashim S.J.",10.5194/isprs-archives-XLII-4-W16-489-2019,Conference Paper,9.535159111,6.8928360939
4,331,,EVALUATION of ADVANCED DATA MINING ALGORITHMS in LAND USE/LAND COVER MAPPING,data mining; image classification; land use land cover; lulc; machine learning,"For environmental monitoring, land-cover mapping, and urban planning, remote sensing is an effective method. In this paper, firstly, for land use land cover mapping, Landsat 8 OLI image classification based on six advanced mathematical algorithms of machine learning including Random Forest, Decision Table, DTNB, Multilayer Perceptron, Non-Nested Generalized Exemplars (NN ge) and Simple Logistic is used. Then, results are compared in the terms of Overall Accuracy (OA), Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) for land use land cover (LULC) mapping. Based on the training and test datasets, Simple Logistic had the best performance in terms of OA, MAE and RMSE values of 99.9293, 0.0006 and 0.016 for training dataset and values of 99.9467, 0.0005 and 0.0153 for the test dataset. © 2019 A. Jamali.",2019,"International Archives of the Photogrammetry, Remote Sensing and Spatial Information Sciences - ISPRS Archives","Jamali A., Abdul Rahman A.",10.5194/isprs-archives-XLII-4-W16-283-2019,Conference Paper,9.8709211349,6.9004993439
4,378,21.0,Multiscale road centerlines extraction from high-resolution aerial imagery,Centerlines extraction; Convolutional neural network (CNN); Edge-preserving filtering; Multiscale Gabor filters,"Accurate road extraction from high-resolution aerial imagery has many applications such as urban planning and vehicle navigation system. The common road extraction methods are based on classification algorithm, which needs to design robust handcrafted features for road. However, designing such features is difficult. For the road centerlines extraction problem, the existing algorithms have some limitations, such as spurs, time consuming. To address the above issues to some extent, we introduce the feature learning based on deep learning to extract robust features automatically, and present a method to extract road centerlines based on multiscale Gabor filters and multiple directional non-maximum suppression. The proposed algorithm consists of the following four steps. Firstly, the aerial imagery is classified by a pixel-wise classifier based on convolutional neural network (CNN). Specifically, CNN is used to learn features from raw data automatically, especially the structural features. Then, edge-preserving filtering is conducted on the resulting classification map, with the original imagery serving as the guidance image. It is exploited to preserve the edges and the details of the road. After that, we do some post-processing based on shape features to extract more reliable roads. Finally, multiscale Gabor filters and multiple directional non-maximum suppression are integrated to get a complete and accurate road network. Experimental results show that the proposed method can achieve comparable or higher quantitative results, as well as more satisfactory visual performance. © 2018 Elsevier B.V.",2019,Neurocomputing,"Liu R., Miao Q., Song J., Quan Y., Li Y., Xu P., Dai J.",10.1016/j.neucom.2018.10.036,Article,9.3044404984,6.8250827789
4,389,13.0,Self-supervised feature learning for semantic segmentation of overhead imagery,,"Overhead imageries play a crucial role in many applications such as urban planning, crop yield forecasting, mapping, and policy making. Semantic segmentation could enable automatic, efficient, and large-scale understanding of overhead imageries for these applications. However, semantic segmentation of overhead imageries is a challenging task, primarily due to the large domain gap from existing research in ground imageries, unavailability of large-scale dataset with pixel-level annotations, and inherent complexity in the task. Readily available vast amount of unlabeled overhead imageries share more common structures and patterns compared to the ground imageries, therefore, its large-scale analysis could benefit from unsupervised feature learning techniques. In this work, we study various self-supervised feature learning techniques for semantic segmentation of overhead imageries. We choose image semantic inpainting as a self-supervised task [36] for our experiments due to its proximity to the semantic segmentation task. We (i) show that existing approaches are inefficient for semantic segmentation, (ii) propose architectural changes towards self-supervised learning for semantic segmentation, (iii) propose an adversarial training scheme for self-supervised learning by increasing the pretext task's difficulty gradually and show that it leads to learning better features, and (iv) propose a unified approach for overhead scene parsing, road network extraction, and land cover estimation. Our approach improves over training from scratch by more than 10% and ImageNet pre-trained network by more than 5% mIOU. © 2018. The copyright of this document resides with its authors.",2019,"British Machine Vision Conference 2018, BMVC 2018","Singh S., Batra A., Pang G., Torresani L., Basu S., Paluri M., Jawahar C.V.",,Conference Paper,9.4005746841,6.844974041
4,393,3.0,Multi-scale correlation-based feature selection and random forest classification for LULC mapping from the integration of SAR and optical Sentinel images,data fusion; image segmentation optimization; LULC; object-based classification; Optical sensors; SAR,"Reliable and accurate land use/land cover (LULC) map is a crucial data source for the understanding of coupled human-environment systems, monitoring changes, timely low-cost planning, and management of natural resources. Improvements in sensor technologies and machine learning capabilities have shifted the attention of remote sensing community to data complementarity through fusion of multi-sensor data for accurate feature extraction and mapping. Amalgamation of optical and synthetic aperture radar (SAR) images has shown promising advantages in enhancing the accuracy of extracting LULC as such method allows exploitation of information in sensors. This study investigated the potential of using freely available multisource Sentinel images to extract LULC maps in semi-arid environments through multi-scale geographic object-based image analysis (GEOBIA). A multi-scale classification framework that integrates GEOBIA, correlation-based feature selection (CFS), and random forest (RF)-supervised classification was adopted to extract LULC from assimilation of Sentinel multi-sensor products. First, Sentinel-1 and-2 images were pre-processed. Second, optimum multi-scale segmentation levels were selected using F-score segmentation quality measures. Third, 70 features of various spectral indices and derivatives and geometrical features from optical data and multiple ratios and textural features from dual-polarization SAR images were computed, and a CFS based on wrapper approach was used to select the most significant features at multi-scale levels. Finally, a single and multi-scale RF classifier was used to extract LULC classes using the most relevant features extracted from Sentinel SAR and optical images. Results of multi-scale image segmentation optimization showed that scale parameter (SP) values of 40, 60, and 150 were optimal for extraction of LULC classes. Results of feature selection showed that 22, 24, and 27 features were selected at scale SP values of 40, 60, and 150, respectively. Half of the features were common among the three scales. Single RF classification yielded overall accuracy (OA) values of 92.10%, 93%, and 91% and kappa coefficients of 0.901, 0.912, and 0.89 at scale values of 150, 60, and 40, respectively. Multiscale RF classification from scale values of 150 and 60 produced better LULC classification with OA 96.06% and kappa coefficient of 0.95 compared with other scale SP values. The integrated approach demonstrated an effective and promising method for high-quality LULC extraction from coupling optical and SAR images. Overall, multi-sensor Sentinel images along with the adopted approach feature a remarkable potential for improving LULC extraction and can effectively be used to update geographic information system layers for various applications. © 2019 SPIE.",2019,Proceedings of SPIE - The International Society for Optical Engineering,"Al-Ruzouq R., Shanableh A., Gibril M.B., Kalantar B.",10.1117/12.2533123,Conference Paper,9.8021450043,6.8825950623
4,398,3.0,A big remote sensing data analysis using deep learning framework,Big Data; Deep Learning; Feature extraction; Multi-label Classification; Remote Sensing; Support Vector Machines,"Spaceborne and airborne sensors deliver a huge number of Earth Observation Data every day. In this context, we can easily observe the whole earth from its different sides. Therefore, this big data is important in remote sensing and could be exploited in several domains requiring image classification, natural hazard monitoring, global climate change, agriculture, urban planning. Over the last five years, Convolutional Neural Networks (CNN) emerged as the most successful technique for the image classification task, as well as a number of other computer vision tasks. However, to train millions of parameters in CNN one requires a huge amount of annotated data. This requirement leads to a significant challenge if the available training data is limited for a target task at hand. To address this challenge, in the recent literature, researchers proposed various ways to apply a technique called Transfer Learning to transfer the knowledge gained by training CNNs parameters on some large annotated dataset to the target task with limited availability of training data. Most of our work in this paper was dedicated to proposing a hybrid classification of remote sensing images. This architecture combines Spark RDD image coding to consider image's local regions, pre-trained VGGNET-16 and UNET for image segmentation and SVM (Support Vector Machines) from spark Machine Learning to achieve labeling task. © Multi Conference on Computer Science and Information Systems, MCCSIS 2019. All rights reserved.",2019,"Multi Conference on Computer Science and Information Systems, MCCSIS 2019 - Proceedings of the International Conferences on Big Data Analytics, Data Mining and Computational Intelligence 2019 and Theory and Practice in Modern Computing 2019","Balti H., Chebbi I., Mellouli N., Farah I.R., Lamolle M.",10.33965/bigdaci2019_201907l015,Conference Paper,9.6228475571,6.8840970993
4,418,34.0,Building extraction from LiDAR data applying deep convolutional neural networks,Building classification; convolutional neural networks (CNNs); Light Detection and Ranging (LiDAR); machine learning; point cloud,"Deep learning paradigm has been shown to be a very efficient classification framework for many application scenarios, including the analysis of Light Detection and Ranging (LiDAR) data for building detection. In fact, deep learning acts as a set of mathematical transformations, encoding the raw input data into appropriate forms of representations that maximize the classification performance. However, it is clear that mathematical computations alone, even highly nonlinear, are not adequate to model the physical properties of a problem, distinguishing, for example, the building structures from vegetation. In this letter, we address this difficulty by augmenting the raw LiDAR data with features coming from a physical interpretation of the information. Then, we exploit a deep learning paradigm based on a convolutional neural network model to find out the best input representations suitable for the classification. As test sites, three complex urban study areas with various kinds of building structures through the LiDAR data set of Vaihingen, Germany were selected. Our method has been evaluated in the context of 'ISPRS Test Project on Urban Classification and 3-D Building Reconstruction.' Comparisons with traditional methods, such as artificial neural networks and support vector machine-based classifiers, indicate the outperformance of the proposed approach in terms of robustness and efficiency. © 2004-2012 IEEE.",2019,IEEE Geoscience and Remote Sensing Letters,"Maltezos E., Doulamis A., Doulamis N., Ioannidis C.",10.1109/LGRS.2018.2867736,Article,9.5435352325,6.9027547836
4,427,17.0,Progressively Expanded Neural Network (PEN Net) for hyperspectral image classification: A new neural network paradigm for remote sensing image analysis,Classification; Hyperspectral image (HSI); Machine learning; Neural network; Remote sensing,"Hyperspectral image (HSI) has been used for a wide range of applications including forestry, urban planning, and precision agriculture. In recent years, machine learning based algorithms, such as support vector machines, decision trees, ensemble learning, and their variations have shown promising results in HSI analysis. Such methodologies, nevertheless, can lead to insufficient information abstraction in interpreting hyperspectral pixels. In this paper, we propose a novel neural network based classification algorithm, named Progressively Expanded Neural Network (PEN Net), that can effectively interpret hyperspectral pixels in nonlinear feature spaces and then determine their categories. Furthermore, a spectral-spatial HSI classification framework is also introduced to test the generality and robustness of the PEN Net. Experimental results on four standard hyperspectral datasets illustrate that: (1) PEN Net classifier yields better accuracy and competitive processing speed in HSI classification tasks compared to the state-of-the-art methods; (2) Multi-hidden layer based PEN Net generally provides better performance than single hidden layer one; (3) Combination of spectral and spatial features in the PEN Net classifier can significantly improve the classification accuracy by 6–15% compared to the spectral only based HSI classification. This study implies that the proposed neural network architecture opens a new window for future research and the potential for remote sensing image analysis. © 2018 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)",2018,ISPRS Journal of Photogrammetry and Remote Sensing,"Sidike P., Asari V.K., Sagan V.",10.1016/j.isprsjprs.2018.09.007,Article,9.5083456039,6.8591499329
4,437,12.0,Ultra-Light aircraft-based hyperspectral and colour-infrared imaging to identify deciduous tree species in an urban environment,Classification; Colour infrared; Hyperspectral; Ultra-light aircraft; Urban trees,"One may consider the application of remote sensing as a trade-off between the imaging platforms, sensors, and data gathering and processing techniques. This study addresses the potential of hyperspectral imaging using ultra-light aircraft for vegetation species mapping in an urban environment, exploring both the engineering and scientific aspects related to imaging platform design and image classification methods. An imaging system based on simultaneous use of Rikola frame format hyperspectral and Nikon D800E adopted colour infrared cameras installed onboard a Bekas X32 manned ultra-light aircraft is introduced. Two test imaging flight missions were conducted in July of 2015 and September of 2016 over a 4000 ha area in Kaunas City, Lithuania. Sixteen and 64 spectral bands in 2015 and 2016, respectively, in a spectral range of 500-900 nm were recorded with colour infrared images. Three research questions were explored assessing the identification of six deciduous tree species: (1) Pre-treatment of spectral features for classification, (2) testing five conventional machine learning classifiers, and (3) fusion of hyperspectral and colour infrared images. Classification performance was assessed by applying leave-one-out cross-validation at the individual crown level and using as a reference at least 100 field inventoried trees for each species. The best-performing classification algorithm-multilayer perceptron, using all spectral properties extracted from the hyperspectral images-resulted in a moderate classification accuracy. The overall classification accuracy was 63%, Cohen's Kappa was 0.54, and the species-specific classification accuracies were in the range of 51-72%. Hyperspectral images resulted in significantly better tree species classification ability than the colour infrared images and simultaneous use of spectral properties extracted from hyperspectral and colour infrared images improved slightly the accuracy over the 2015 image. Even though classifications using hyperspectral data cubes of 64 bands resulted in relatively larger accuracies than with 16 bands, classification error matrices were not statistically different. Alternative imaging platforms (like an unmanned aerial vehicle and a Cessna 172 aircraft) and settings of the flights were discussed using simulated imaging projects assuming the same study area and field of application. Ultra-light aircraft-based hyperspectral and colour-infrared imaging was considered to be a technically and economically sound solution for urban green space inventories to facilitate tree mapping, characterization, and monitoring. © 2018 by the authors.",2018,Remote Sensing,"Mozgeris G., Juodkiene V., Jonikavičius D., Straigyte L., Gadal S., Ouerghemmi W.",10.3390/rs10101668,Article,9.7139120102,6.8773212433
4,443,18.0,Road centerline extraction from very-high-resolution aerial image and LiDAR data based on road connectivity,LiDAR data; Object recognition; Road centerline; Road connectivity; Very-high-resolution image,"The road networks provide key information for a broad range of applications such as urban planning, urban management, and navigation. The fast-developing technology of remote sensing that acquires high-resolution observational data of the land surface offers opportunities for automatic extraction of road networks. However, the road networks extracted from remote sensing images are likely affected by shadows and trees, making the road map irregular and inaccurate. This research aims to improve the extraction of road centerlines using both very-high-resolution (VHR) aerial images and light detection and ranging (LiDAR) by accounting for road connectivity. The proposed method first applies the fractal net evolution approach (FNEA) to segment remote sensing images into image objects and then classifies image objects using the machine learning classifier, random forest. A post-processing approach based on the minimum area bounding rectangle (MABR) is proposed and a structure feature index is adopted to obtain the complete road networks. Finally, a multistep approach, that is, morphology thinning, Harris corner detection, and least square fitting (MHL) approach, is designed to accurately extract the road centerlines from the complex road networks. The proposed method is applied to three datasets, including the New York dataset obtained from the object identification dataset, the Vaihingen dataset obtained from the International Society for Photogrammetry and Remote Sensing (ISPRS) 2D semantic labelling benchmark and Guangzhou dataset. Compared with two state-of-the-art methods, the proposed method can obtain the highest completeness, correctness, and quality for the three datasets. The experiment results show that the proposed method is an efficient solution for extracting road centerlines in complex scenes from VHR aerial images and light detection and ranging (LiDAR) data. © 2018 by the authors.",2018,Remote Sensing,"Zhang Z., Zhang X., Sun Y., Zhang P.",10.3390/rs10081284,Article,9.3894233704,6.8451924324
4,444,7.0,Context-Based Filtering of Noisy Labels for Automatic Basemap Updating from UAV Data,Basemap updating; image classification; informal settlements; label noise; random forests; unmanned aerial vehicles (UAVs); urban planning,"Unmanned aerial vehicles (UAVs) have the potential to obtain high-resolution aerial imagery at frequent intervals, making them a valuable tool for urban planners who require up-to-date basemaps. Supervised classification methods can be exploited to translate the UAV data into such basemaps. However, these methods require labeled training samples, the collection of which may be complex and time consuming. Existing spatial datasets can be exploited to provide the training labels, but these often contain errors due to differences in the date or resolution of the dataset from which these outdated labels were obtained. In this paper, we propose an approach for updating basemaps using global and local contextual cues to automatically remove unreliable samples from the training set, and thereby, improve the classification accuracy. Using UAV datasets over Kigali, Rwanda, and Dar es Salaam, Tanzania, we demonstrate how the amount of mislabeled training samples can be reduced by 44.1% and 35.5%, respectively, leading to a classification accuracy of 92.1% in Kigali and 91.3% in Dar es Salaam. To achieve the same accuracy in Dar es Salaam, between 50000 and 60000 manually labeled image segments would be needed. This demonstrates that the proposed approach of using outdated spatial data to provide labels and iteratively removing unreliable samples is a viable method for obtaining high classification accuracies while reducing the costly step of acquiring labeled training samples. © 2008-2012 IEEE.",2018,IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing,"Gevaert C.M., Persello C., Elberink S.O., Vosselman G., Sliuzas R.",10.1109/JSTARS.2017.2762905,Article,9.6894254684,6.9090723991
4,460,12.0,Object-based detection of vehicles using combined optical and elevation data,Cluster analysis; Data fusion; Elevation data; Feature extraction; High-resolution; Object-based classification; Random forest; Vehicle detection,"The detection of vehicles is an important and challenging topic that is relevant for many applications. In this work, we present a workflow that utilizes optical and elevation data to detect vehicles in remotely sensed urban data. This workflow consists of three consecutive stages: candidate identification, classification, and single vehicle extraction. Unlike in most previous approaches, fusion of both data sources is strongly pursued at all stages. While the first stage utilizes the fact that most man-made objects are rectangular in shape, the second and third stages employ machine learning techniques combined with specific features. The stages are designed to handle multiple sensor input, which results in a significant improvement. A detailed evaluation shows the benefits of our workflow, which includes hand-tailored features; even in comparison with classification approaches based on Convolutional Neural Networks, which are state of the art in computer vision, we could obtain a comparable or superior performance (F1 score of 0.96–0.94). © 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS)",2018,ISPRS Journal of Photogrammetry and Remote Sensing,"Schilling H., Bulatov D., Middelmann W.",10.1016/j.isprsjprs.2017.11.023,Article,9.3467168808,6.8480095863
4,462,458.0,Spectral-Spatial Residual Network for Hyperspectral Image Classification: A 3-D Deep Learning Framework,3-D deep learning; hyperspectral image classification; spectral-spatial feature extraction; spectral-spatial residual network (SSRN),"In this paper, we designed an end-to-end spectral-spatial residual network (SSRN) that takes raw 3-D cubes as input data without feature engineering for hyperspectral image classification. In this network, the spectral and spatial residual blocks consecutively learn discriminative features from abundant spectral signatures and spatial contexts in hyperspectral imagery (HSI). The proposed SSRN is a supervised deep learning framework that alleviates the declining-accuracy phenomenon of other deep learning models. Specifically, the residual blocks connect every other 3-D convolutional layer through identity mapping, which facilitates the backpropagation of gradients. Furthermore, we impose batch normalization on every convolutional layer to regularize the learning process and improve the classification performance of trained models. Quantitative and qualitative results demonstrate that the SSRN achieved the state-of-the-art HSI classification accuracy in agricultural, rural-urban, and urban data sets: Indian Pines, Kennedy Space Center, and University of Pavia. © 1980-2012 IEEE.",2018,IEEE Transactions on Geoscience and Remote Sensing,"Zhong Z., Li J., Luo Z., Chapman M.",10.1109/TGRS.2017.2755542,Article,9.5603132248,6.8504638672
4,463,1.0,Building detection from orthophotos using binary feature classification,Building detection; Classifier; Descriptor; Local feature; Machine learning,"Building detection in orthophotos is crucial for various applications, such as urban planning and real-estate management. In order to realize accurate and fast building detection, a non-interactive approach based on binary feature classification is brought forward in this paper. The proposed approach includes two major stages, i.e., building area detection and building contours extraction. In the first stage, a sequence of intersections is obtained by superpixel segmentation in the subsampled orthophoto, and then building area is reserved roughly according to the classification of intersections. In the second stage, the sequence of intersections is updated by superpixel segmentation in the building area from original orthophoto, and then building contours is extracted in accordance with the classification of intersections likewise. The local feature of the intersections is descripted employing our extremely compact binary descriptor, and is classified using binary bag-of-features. Experiments show that benefiting from binary description and making full use of texture details and color channels, the proposed descriptor is not only computationally frugal, but also accurate. Experiments are also conducted on orthophotos with different roof colors, textures, shapes, sizes and orientations, and demonstrate that the proposed approach are capable of achieving desirable results. © 2017, Springer Science+Business Media, LLC.",2018,Multimedia Tools and Applications,"Hu Y., Hu X., Li P., Ding Y.",10.1007/s11042-017-5093-z,Article,9.396900177,6.8638391495
4,469,2.0,Understanding Historical Cityscapes from Aerial Imagery Through Machine Learning,Building detection; Machine learning; Point cloud; Shadow compensation; Shadow detection,"Understanding cityscapes using remote sensing data has been an active research field for more than two decades. Meanwhile, machine learning provides generalization capabilities compared to hierarchical and rule-based methods. This paper evaluates several machine learning algorithms in order to fuse shadow detection and shadow compensation methods for building detection using high resolution aerial imagery. Three complex and real-life urban study areas were used as test datasets with various: (i) kinds of buildings structures of special architecture, (ii) pixel resolutions and, (iii) types of data. Objective evaluation metrics have been used for assessing the compared algorithms such recall, precision and F1-score as well as rates of completeness, correctness and quality. For both approaches, i.e., shadow detection and building detection, the computational complexity of each machine learning algorithm was examined. The results indicate that deep learning schemes, such a Convolutional Neural Network (CNN), provides the best classification performance in terms of shadow detection and building detection. © 2018, Springer Nature Switzerland AG.",2018,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),"Maltezos E., Protopapadakis E., Doulamis N., Doulamis A., Ioannidis C.",10.1007/978-3-030-01762-0_17,Conference Paper,9.5178565979,6.8609166145
4,473,,Automatic semantic segmentation for change detection in remote sensing images,Change detection; Deep learning; Multispectral; Remote sensing,"Change detection (CD) mainly focuses on the extraction of change information from multispectral remote sensing images of the same geographical location for environmental monitoring, natural disaster evaluation, urban studies, and deforestation monitoring. While capturing the Landsat imagery, there may occur data missing issues such as occlusion of cloud, camera sensor, and aperture artifacts. The existing machine learning approaches do not provide significant results. This paper proposes a DeepLab Dilated convolutional neural network (DL-DCNN) for semantic segmentation with the goal to occur the change map for earth observation applications. Experimental results reveal that the accuracy of the proposed change detection results provides improved results as compared with the existing algorithms and maps the semantic objects within the predefined class as change or no change. © Springer Nature Singapore Pte Ltd. 2018.",2018,Advances in Intelligent Systems and Computing,"Kulkarni T., Venugopal N.",10.1007/978-981-10-8569-7_34,Conference Paper,9.6479387283,6.8642406464
4,476,61.0,Exploring the optimal integration levels between SAR and optical data for better urban land cover mapping in the Pearl River Delta,Fusion level; Fusion strategies; Optical and SAR fusion; Urban land cover,"Integrating synthetic aperture radar (SAR) and optical data to improve urban land cover classification has been identified as a promising approach. However, which integration level is the most suitable remains unclear but important to many researchers and engineers. This study aimed to compare different integration levels for providing a scientific reference for a wide range of studies using optical and SAR data. SAR data from TerraSAR-X and ENVISAT ASAR in both WSM and IMP modes were used to be combined with optical data at pixel level, feature level and decision levels using four typical machine learning methods. The experimental results indicated that: 1) feature level that used both the original images and extracted features achieved a significant improvement of up to 10% compared to that using optical data alone; 2) different levels of fusion required different suitable methods depending on the data distribution and data resolution. For instance, support vector machine was the most stable at both the feature and decision levels, while random forest was suitable at the pixel level but not suitable at the decision level. 3) By examining the distribution of SAR features, some features (e.g., homogeneity) exhibited a close-to-normal distribution, explaining the improvement from the maximum likelihood method at the feature and decision levels. This indicated the benefits of using texture features from SAR data when being combined with optical data for land cover classification. Additionally, the research also shown that combining optical and SAR data does not guarantee improvement compared with using single data source for urban land cover classification, depending on the selection of appropriate fusion levels and fusion methods. © 2017 Elsevier B.V.",2018,International Journal of Applied Earth Observation and Geoinformation,"Zhang H., Xu R.",10.1016/j.jag.2017.08.013,Article,9.782479286200001,6.8480024338
4,478,5.0,3D shape descriptor for objects recognition,3D Feature Extraction; Object Classification; Pattern Recognition,"3D point cloud classification is an important task in applications for many areas such as robotics, urban planning and augmented reality. 3D sensors measure a high amount of points in the 3D scene objects' surface at a high collect rate, so robust techniques are needed to process all input data and also deal with some imprecision. A common solution for these tasks is the use of robust features extraction techniques to gather representative scene information at the lowest computational cost possible. This paper presents a new approach for object recognition in 3D scenes, using a novel 3D shape descriptor which is used as input for a supervised machine learning method. Proposed robust 3D feature is invariant to translation and scale and provides a very simplified object representation for pattern recognition input. Experiments were performed using an Artificial Neural Network to recognize six different object shapes, and obtained results showed that the proposed method is a promising approach for object recognition in 3D scenes. © 2017 IEEE.",2017,"Proceedings - 2017 LARS 14th Latin American Robotics Symposium and 2017 5th SBR Brazilian Symposium on Robotics, LARS-SBR 2017 - Part of the Robotics Conference 2017","Sales D.O., Amaro J., Osório F.S.",10.1109/SBR-LARS-R.2017.8215285,Conference Paper,9.3507709503,6.8474411964
4,479,7.0,Deep highway unit network for land cover type classification with GF-3 SAR imagery,Deep highway unit networks; Deep learning; GaoFen-3; Land cover type classification,"The fully polarized synthetic aperture radar (SAR) is an advanced earth observation system with day and night imaging capability, which can obtain rich information of terrain and has a wide range of applications in environmental protection, urban planning and resource investigation. As the first selfdeveloped C-band multi-polarized SAR image, the acquisition of massive data and operational operation of Chinese SAR remote sensing has entered the era of big data. Under the era of remote sensing large data, however, SAR image interpretation is a great challenge for scientific applications. At present, big data-based intelligent methods such as computer vision technology have achieved great success. Deep learning such as deep highway unit networks has revolutionized the computer vision area. However, due to the characteristics of SAR microwave band imaging and phase coherence processing, SAR images are very different from ordinary optical images in terms of band, projection direction, data composition and so on. Therefore, deep learning can not be directly used for quad-pol SAR image classification. In this paper, deep learning is applied to land cover type classification with GF-3 quad-pol SAR imagery. A deep highway unit network is employed to automatically extract a hierarchic feature representation from the data, based on which the land cover type classification can be conducted. Our classification model is trained on limited training data from forest resource inventory and planning data, and tested on a Radarsat-2 quad-pol images, which is the image of the same area acquired at different times. We also employ the machine learning such as SVM, Random Forest on the same samples for comparison. The deep highway unit network trained by the GF-3 images, which can reduce speckle, fully excavate the regularity of SAR images in time and space. © 2017 IEEE.",2017,"Proceedings of 2017 SAR in Big Data Era: Models, Methods and Applications, BIGSARDATA 2017","Guo Y., Chen E., Guo Y., Li Z., Li C., Xu K.",10.1109/BIGSARDATA.2017.8124926,Conference Paper,9.65974617,6.853372097
4,507,,A support vector machine approach on object based image analysis for feature extraction from high resolution images,Feature Extraction; Grey-Level Co-occurrence textures; Object Based Image Analysis (OBIA); Support Vector Machine (SVM),"Satellite images are the most important available data sources for generation and updating of available maps. They have highly improved in terms of spatial, spectral and temporal resolutions and by the sheer volume of collected images, the necessity of simplification of automation in feature extraction. Road data play a key role in urban planning, traffic management, military applications, and vehicle navigation as well as for decision making in numerous applications. The faster updation of road infrastructure is a need because the technology has brought map in the hands of people in the form of mobile phones and tablets. Road detection is one of the major issues of the road infrastructure extraction. Its accuracy depends on the type of methodology used. An attempt is made here to analyse the first order, the co-occurrence texture features and image transforms useful for discriminating roads from other features specially the buildings. The identified dataset forms high dimension feature space and the Support Vector Machine is a theoretically superior machine learning methodology with great results in classification of high dimensional datasets. In the past, SVMs have been tested and evaluated only as pixel-based image classifiers. Moving from pixel-based techniques towards object-based representation, the dimensions of remote sensing imagery feature space increases significantly. An SVM approach for classification was followed, based on primitive image objects produces by a multi-resolution segmentation algorithm. The SVM procedure produced the final object classification results which were compared to the Nearest Neighbor classifier results and were found to give better results in OBIA domain. © 2017 ACRS. All rights reserved.",2017,"38th Asian Conference on Remote Sensing - Space Applications: Touching Human Lives, ACRS 2017","Kumar M., Srivastav S.K., Garg P.K.",,Conference Paper,9.3601331711,6.855509758
4,525,34.0,Building detection from orthophotos using a machine learning approach: An empirical study on image segmentation and descriptors,Automatic building detection and delineation; Classifier; Image descriptors; Image segmentation; Orthophotos; Supervised learning,"Building detection from aerial images has many applications in fields like urban planning, real-estate management, and disaster relief. In the last two decades, a large variety of methods on automatic building detection have been proposed in the remote sensing literature. Many of these approaches make use of local features to classify each pixel or segment to an object label, therefore involving an extra step to fuse pixelwise decisions. This paper presents a generic framework that exploits recent advances in image segmentation and region descriptors extraction for the automatic and accurate detection of buildings on aerial orthophotos. The proposed solution is supervised in the sense that appearances of buildings are learnt from examples. For the first time in the context of building detection, we use the matrix covariance descriptor, which proves to be very informative and compact. Moreover, we introduce a principled evaluation that allows selecting the best pair segmentation algorithm-region descriptor for the task of building detection. Finally, we provide a performance evaluation at pixel level using different classifiers. This evaluation is conducted over 200 buildings using different segmentation algorithms and descriptors. The performance analysis quantifies the quality of both the image segmentation and the descriptor used. The proposed approach presents several advantages in terms of scalability, suitability and simplicity with respect to the existing methods. Furthermore, the proposed scheme (detection chain and evaluation) can be deployed for detecting multiple object categories that are present in images and can be used by intelligent systems requiring scene perception and parsing such as intelligent unmanned aerial vehicle navigation and automatic 3D city modeling. © 2016 Elsevier Ltd. All rights reserved.",2016,Expert Systems with Applications,"Dornaika F., Moujahid A., El Merabet Y., Ruichek Y.",10.1016/j.eswa.2016.03.024,Article,9.4164514542,6.8852286339
4,528,34.0,A supervoxel-based spectro-spatial approach for 3D urban point cloud labelling,,"ABSTRACT: Three-dimensional (3D) point cloud labelling of airborne lidar (light detection and ranging) data has promising applications in urban city modelling. Automatic and efficient methods for semantic labelling of airborne urban point cloud data with multiple classes still remains a challenge. We propose a novel 3D object-based classification framework for labelling urban lidar point cloud using a computer vision technique, supervoxels. The supervoxel approach is promising for representing dense lidar point cloud in a compact manner for 3D segmentation and for improving the computational efficiency. Initially, supervoxels are generated by over-segmenting the coloured point cloud using the voxel-based cloud connectivity algorithm in the geometric space. The local connectivity established between supervoxels has been used to produce meaningful and realistic objects (segments). The segments are classified by different machine learning techniques based on several spectral and geometric features extracted from the segments. All the points within a labelled segment are assigned the same segment label. Furthermore, the effect of different feature vectors and varying point density on the classification accuracy has been studied. Results indicate an accurate labelling of points in realistic 3D space conforming to the boundaries of objects. An overall classification accuracy of 90% is achieved by the proposed method. The labelled 3D points can be used directly for the reconstruction of buildings and other man-made objects. © 2016 Informa UK Limited, trading as Taylor & Francis Group.",2016,International Journal of Remote Sensing,"Ramiya A.M., Nidamanuri R.R., Ramakrishnan K.",10.1080/01431161.2016.1211348,Article,9.3593730927,6.8685650826
4,531,29.0,Land Classification Using Remotely Sensed Data: Going Multilabel,CORINE; data processing; land cover; MODIS; pattern classification; remote sensing; satellite applications; time series; unmixing,"Obtaining an up-to-date high-resolution description of land cover is a challenging task due to the high cost and labor-intensive process of human annotation through field studies. This work introduces a radically novel approach for achieving this goal by exploiting the proliferation of remote sensing satellite imagery, allowing for the up-to-date generation of global-scale land cover maps. We propose the application of multilabel classification, a powerful framework in machine learning, for inferring the complex relationships between the acquired satellite images and the spectral profiles of different types of surface materials. Introducing a drastically different approach compared to unsupervised spectral unmixing, we employ contemporary ground-collected data from the European Environment Agency to generate the label set and multispectral images from the MODIS sensor to generate the spectral features, under a supervised classification framework. To validate the merits of our approach, we present results using several state-of-the-art multilabel learning classifiers and evaluate their predictive performance with respect to the number of annotated training examples, as well as their capability to exploit examples from neighboring regions or different time instances. We also demonstrate the application of our method on hyperspectral data from the Hyperion sensor for the urban land cover estimation of New York City. Experimental results suggest that the proposed framework can achieve excellent prediction accuracy, even from a limited number of diverse training examples, surpassing state-of-the-art spectral unmixing methods. © 2016 IEEE.",2016,IEEE Transactions on Geoscience and Remote Sensing,"Karalas K., Tsagkatakis G., Zervakis M., Tsakalides P.",10.1109/TGRS.2016.2520203,Article,9.7934703827,6.9039230347
4,547,203.0,Building detection in very high resolution multispectral data with deep learning features,deep convolutional networks; extraction; ImageNet; Machine learning; man made objects,"The automated man-made object detection and building extraction from single satellite images is, still, one of the most challenging tasks for various urban planning and monitoring engineering applications. To this end, in this paper we propose an automated building detection framework from very high resolution remote sensing data based on deep convolutional neural networks. The core of the developed method is based on a supervised classification procedure employing a very large training dataset. An MRF model is then responsible for obtaining the optimal labels regarding the detection of scene buildings. The experimental results and the performed quantitative validation indicate the quite promising potentials of the developed approach. © 2015 IEEE.",2015,International Geoscience and Remote Sensing Symposium (IGARSS),"Vakalopoulou M., Karantzalos K., Komodakis N., Paragios N.",10.1109/IGARSS.2015.7326158,Conference Paper,9.4824390411,6.8705377579
4,548,42.0,Improving Spatial Feature Representation from Aerial Scenes by Using Convolutional Networks,Deep Learning; Feature Learning; High-resolution Images; Image Classification; Machine Learning; Remote Sensing,"The performance of image classification is highly dependent on the quality of extracted features. Concerning high resolution remote image images, encoding the spatial features in an efficient and robust fashion is the key to generating discriminatory models to classify them. Even though many visual descriptors have been proposed or successfully used to encode spatial features of remote sensing images, some applications, using this sort of images, demand more specific description techniques. Deep Learning, an emergent machine learning approach based on neural networks, is capable of learning specific features and classifiers at the same time and adjust at each step, in real time, to better fit the need of each problem. For several task, such image classification, it has achieved very good results, mainly boosted by the feature learning performed which allows the method to extract specific and adaptable visual features depending on the data. In this paper, we propose a novel network capable of learning specific spatial features from remote sensing images, with any pre-processing step or descriptor evaluation, and classify them. Specifically, automatic feature learning task aims at discovering hierarchical structures from the raw data, leading to a more representative information. This task not only poses interesting challenges for existing vision and recognition algorithms, but also brings huge opportunities for urban planning, crop and forest management and climate modelling. The propose convolutional neural network has six layers: three convolutional, two fully-connected and one classifier layer. So, the five first layers are responsible to extract visual features while the last one is responsible to classify the images. We conducted a systematic evaluation of the proposed method using two datasets: (i) the popular aerial image dataset UCMerced Land-use and, (ii) a multispectral high-resolution scenes of the Brazilian Coffee Scenes. The experiments show that the proposed method outperforms state-of-the-art algorithms in terms of overall accuracy. © 2015 IEEE.",2015,Brazilian Symposium of Computer Graphic and Image Processing,"Nogueira K., Miranda W.O., Santos J.A.D.",10.1109/SIBGRAPI.2015.39,Conference Paper,9.5713834763,6.8631043434
4,557,1.0,Comparison of different machine learning classifiers for building extraction in LiDAR-derived datasets,Feature extraction; Object based image analysis,"Building extraction in remotely sensed imagery is an important problem that needs solving. It can be used to aid in urban planning, hazard assessments and disaster risk management among others. Light Detection and Ranging or LiDAR, is one of the most powerful remote sensing technologies nowadays. Many studies have used the fusion of LiDAR data and multispectral images in detecting buildings. This study seeks to maximize the power of LiDAR imagery to be able to classify buildings without the aid of multispectral imagery. This work follows the Object Based Image Analysis (OBIA) approach. Instead of the traditional pixel-based classification methods, pixels are segmented into logical groups called objects. From these objects, features for building extraction are calculated. These features are: the number of returns, difference of returns, and the mean and standard deviation of positive surface openness. These objects are then classified using different machine learning classifiers such as Support Vector Machines, K-Nearest Neighbors, Naïve Bayes Classifier, Decision Trees, and Random Forests. A comparative assessment was done on the performance of these different machine learning classifiers. The classifiers performed similarly with the Random Forest Classifier slightly outperforming the others.",2015,"ACRS 2015 - 36th Asian Conference on Remote Sensing: Fostering Resilient Growth in Asia, Proceedings","Escamos I.M.H., Roberto A.R.C., Abucay E.R., Inciong G.K.L., Queliste M.D., Hermocilla J.A.C.",,Conference Paper,9.5172758102,6.8921265602
4,559,64.0,Urban land use and land cover classification using remotely sensed sar data through deep belief networks,,"Land use and land cover (LULC) mapping in urban areas is one of the core applications in remote sensing, and it plays an important role in modern urban planning and management. Deep learning is springing up in the field of machine learning recently. By mimicking the hierarchical structure of the human brain, deep learning can gradually extract features from lower level to higher level. The Deep Belief Networks (DBN) model is a widely investigated and deployed deep learning architecture. It combines the advantages of unsupervised and supervised learning and can archive good classification performance. This study proposes a classification approach based on the DBN model for detailed urban mapping using polarimetric synthetic aperture radar (PolSAR) data. Through the DBN model, effective contextual mapping features can be automatically extracted from the PolSAR data to improve the classification performance. Two-date high-resolution RADARSAT-2 PolSAR data over the Great Toronto Area were used for evaluation. Comparisons with the support vector machine (SVM), conventional neural networks (NN), and stochastic Expectation-Maximization (SEM) were conducted to assess the potential of the DBN-based classification approach. Experimental results show that the DBN-based method outperforms three other approaches and produces homogenous mapping results with preserved shape details. © 2015 Qi Lv et al.",2015,Journal of Sensors,"Lv Q., Dou Y., Niu X., Xu J., Xu J., Xia F.",10.1155/2015/538063,Article,9.7191877365,6.8482699394
4,563,11.0,Ensemble methods for binary classifications of airborne LIDAR data,Computing; Ensemble method; LIDAR; Machine learning; Remote sensing,"This paper presents a framework that is aimed at improving the performance of two existing ensemble methods (namely, AdaBoost and Bagging) for airborne light detection and ranging (LIDAR) classification. LIDAR is one of the fastest growing technologies to support a multitude of civil engineering applications, such as transportation, urban planning, flood control, and city 3D reconstruction. For the above applications, LIDAR data need to be classified into binary classes (i.e., terrain and nonterrain) or multiple classes (e.g., ground, vegetation, and buildings). The proposed framework is designed to enhance the generalization performance of binary classification approach by minimizing type II errors. The authors developed and tested the framework on different LIDAR data sets representing geographic sites in Germany and the United States. The results showed that the proposed ensemble framework performed better compared to the existing methods. In addition, the AdaBoost method outperformed the Bagging method on all the terrain types. However, the framework has some limitations in terms of dealing with rough terrain and discontinuous surfaces. © 2014 American Society of Civil Engineers.",2014,Journal of Computing in Civil Engineering,"Nourzad S.H.H., Pradhan A.",10.1061/(ASCE)CP.1943-5487.0000276,Article,9.5023145676,6.8692593575
4,567,66.0,Building type classification using spatial and landscape attributes derived from LiDAR remote sensing data,Building classification; Decision trees; LiDAR; Machine learning; Random forest; Support vector machines,"Building information is one of the key elements for a range of urban planning and management practices. In this study, an investigation was performed to classify buildings delineated from light detection and ranging (LiDAR) remote sensing data into three types: single-family houses, multiple-family houses, and non-residential buildings. Four kinds of spatial attributes describing the shape, location, and surrounding environment of buildings were calculated and subsequently employed in the classification. Experiments were performed in suburban and downtown sites in Denver, CO, USA, considering different building components and neighborhood environments. Building type classification results yielded overall accuracy > 70% and Kappa > 0.5 for both sites, demonstrating the feasibility of obtaining building type information from LiDAR data. The shape attributes, such as width, footprint area, and perimeter, were most useful for identifying building types. Environmental landscape attributes surrounding buildings, such as the number of road and parking lot pixels, also contributed to obtaining building type information. Combining shape and environmental landscape attributes was necessary to obtain accurate and consistent classification results. © 2014 Elsevier B.V.",2014,Landscape and Urban Planning,"Lu Z., Im J., Rhee J., Hodgson M.",10.1016/j.landurbplan.2014.07.005,Article,9.5933027267,6.8818087578
4,576,5.0,Accuracy comparison of land cover mapping using the object-oriented image classification with machine learning algorithms,Bagging; Boosting; Classification and regression tree; Ensemble classifier; Random Forest,"Land cover mapping provides basic information for advanced science such as ecological management, biodiversity conservation, forest planning and so on. In remote sensing research, the process of creating an accurate land cover map is an important subject. Recently, there has been growing research interest in the object-oriented image classification techniques. The object-oriented image classification consists of multidimensional features including object features and thus requires multi-dimensional image classification approaches. For example, a linear model such as the maximum likelihood method of pixel-based classification cannot characterize the patterns or relations of multi-dimensional data. In multi-dimensional image classification, data mining and ensemble learning have been shown to increase accuracy and flexibility. This study examined the use of the object-oriented image classification by the multiple machine learning algorithms for land cover mapping. We applied four classifiers: Classification and regression tree (CART), Decision tree with Boosting, Decision tree with Bagging, and Random Forest. The study area was Sado Island in Niigata Prefecture, Japan. Pan-sharpened SPOT/HRG imagery (June 2007) was used and classified into the following eight classes: broad-leaved deciduous forest, Japanese cedar, Japanese red pine, bamboo forest, paddy field, urban area, road, and bare land. We prepared four data sets with the object based features including textural information. The number of features is increased from data set I through IV. As the result, CART was unsuitable for multi-dimensional classification. Random Forest and Decision tree with Boosting showed high classification accuracies. Furthermore, in the data set with the limited features, Decision tree with Boosting was the accurate classifier. Finally, we propose two machine learning algorithms to every datasets. Random Forest is effective in the case of the multi-dimensional image classification such as data set II, III, and IV. Decision tree with Boosting is effective in the case of the image classification with the limited features such as data set I.",2012,"33rd Asian Conference on Remote Sensing 2012, ACRS 2012","Mochizuki S., Murakami T.",,Conference Paper,9.8388500214,6.8665308952
4,592,28.0,Conditional random field for 3D point clouds with adaptive data reduction,Classifications; Conditional random fields; LIDAR data; Machine learning; Scale theory,"We proposed using Conditional Random Fields with adaptive data reduction for the classification of 3D point clouds acquired from a Riegl Terrestrial laser scanner. The training and inference of the acquired large outdoor urban data can be time consuming. We approach the problem by computing an adaptive support region for each data point using 3D scale theory. For training and inference of the discriminative Conditional Random Fields, smaller set of data samples that contains relevant information within the support region is selected instead of using all point cloud data. We tested the algorithm on synthetically generated data and urban point clouds data acquired from the laser scanner. The computed support region is also used in feature extraction for urban point clouds data. The results showed improvement in the training and inference rate while maintaining comparable classification accuracy. © 2007 IEEE.",2007,"Proceedings - 2007 International Conference on Cyberworlds, CW'07","Lim E.H., Suter D.",10.1109/CW.2007.24,Conference Paper,9.4013347626,6.8451828957
